{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3U1O-trRUDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b6c451-d2b3-484c-aeeb-f2b27e5531d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-453324044.py:41: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  report[\"timestamp_utc\"] = datetime.utcnow().isoformat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running CPU benchmark...\n",
            "\n",
            "======================================================================\n",
            "CELL 0 ENVIRONMENT REPORT\n",
            "======================================================================\n",
            "timestamp_utc: 2026-02-15T02:03:14.219522\n",
            "python_version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "machine: x86_64\n",
            "processor: x86_64\n",
            "cpu_count_logical: 8\n",
            "ram_total_gb: 50.99\n",
            "ram_available_gb: 49.269\n",
            "gpu_detected: False\n",
            "gpu_info_raw: No GPU detected\n",
            "torch_version: 2.9.0+cpu\n",
            "torch_cuda_available: False\n",
            "cpu_loop_10M_seconds: 0.7659\n",
            "gpu_matmul_3k_seconds: Skipped\n",
            "\n",
            "JSON written to: /content/shadowmap_cps_bootstrap/cell0_environment_report.json\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 0 — COLAB ENVIRONMENT AUDIT + SYSTEM REPORT\n",
        "# ShadowMap / CPS Universal Engine Bootstrap\n",
        "# ==============================================================================\n",
        "\n",
        "# This cell:\n",
        "# 1) Audits the runtime environment\n",
        "# 2) Checks hardware (CPU / RAM / GPU)\n",
        "# 3) Benchmarks basic compute\n",
        "# 4) Writes a structured JSON report\n",
        "# 5) Prints a clean text report for copy/paste back to ChatGPT\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import platform\n",
        "import subprocess\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Utility Functions\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def safe_run(cmd):\n",
        "    try:\n",
        "        return subprocess.check_output(cmd, shell=True, stderr=subprocess.DEVNULL).decode().strip()\n",
        "    except:\n",
        "        return \"Unavailable\"\n",
        "\n",
        "def bytes_to_gb(x):\n",
        "    return round(x / (1024**3), 3)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Begin Audit\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "report = {}\n",
        "report[\"timestamp_utc\"] = datetime.utcnow().isoformat()\n",
        "report[\"python_version\"] = sys.version\n",
        "report[\"platform\"] = platform.platform()\n",
        "report[\"machine\"] = platform.machine()\n",
        "report[\"processor\"] = platform.processor()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CPU + Memory\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "report[\"cpu_count_logical\"] = multiprocessing.cpu_count()\n",
        "\n",
        "# RAM\n",
        "try:\n",
        "    import psutil\n",
        "    mem = psutil.virtual_memory()\n",
        "    report[\"ram_total_gb\"] = bytes_to_gb(mem.total)\n",
        "    report[\"ram_available_gb\"] = bytes_to_gb(mem.available)\n",
        "except:\n",
        "    report[\"ram_total_gb\"] = \"psutil not available\"\n",
        "    report[\"ram_available_gb\"] = \"psutil not available\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# GPU Check\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "gpu_info = safe_run(\"nvidia-smi\")\n",
        "if gpu_info != \"Unavailable\":\n",
        "    report[\"gpu_detected\"] = True\n",
        "    report[\"gpu_info_raw\"] = gpu_info.split(\"\\n\")[0]\n",
        "else:\n",
        "    report[\"gpu_detected\"] = False\n",
        "    report[\"gpu_info_raw\"] = \"No GPU detected\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# PyTorch Check\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    report[\"torch_version\"] = torch.__version__\n",
        "    report[\"torch_cuda_available\"] = torch.cuda.is_available()\n",
        "    if torch.cuda.is_available():\n",
        "        report[\"torch_device_name\"] = torch.cuda.get_device_name(0)\n",
        "        report[\"torch_cuda_memory_gb\"] = bytes_to_gb(torch.cuda.get_device_properties(0).total_memory)\n",
        "except:\n",
        "    report[\"torch_version\"] = \"Not installed\"\n",
        "    report[\"torch_cuda_available\"] = False\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Basic CPU Benchmark\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "print(\"Running CPU benchmark...\")\n",
        "t0 = time.time()\n",
        "s = 0\n",
        "for i in range(10_000_000):\n",
        "    s += i\n",
        "t_cpu = time.time() - t0\n",
        "report[\"cpu_loop_10M_seconds\"] = round(t_cpu, 4)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Optional GPU Benchmark (if available)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "if report.get(\"torch_cuda_available\", False):\n",
        "    print(\"Running GPU benchmark...\")\n",
        "    device = torch.device(\"cuda\")\n",
        "    a = torch.randn(3000, 3000, device=device)\n",
        "    b = torch.randn(3000, 3000, device=device)\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    c = torch.matmul(a, b)\n",
        "    torch.cuda.synchronize()\n",
        "    t_gpu = time.time() - t0\n",
        "    report[\"gpu_matmul_3k_seconds\"] = round(t_gpu, 4)\n",
        "else:\n",
        "    report[\"gpu_matmul_3k_seconds\"] = \"Skipped\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Write JSON Report\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "output_dir = \"/content/shadowmap_cps_bootstrap\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "json_path = os.path.join(output_dir, \"cell0_environment_report.json\")\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Print Clean Report\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CELL 0 ENVIRONMENT REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for k, v in report.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "print(\"\\nJSON written to:\", json_path)\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 1 — UNIVERSAL CPS ENGINE CORE (GIN-CPS.v1)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Directory Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "os.makedirs(ENGINE_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CPS Core Classes\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class CPS:\n",
        "    def __init__(self, spec: Dict[str, Any]):\n",
        "        self.spec = spec\n",
        "\n",
        "    def check_constraints(self, gamma: Dict[str, Any]) -> bool:\n",
        "        # Placeholder — domain adapters will override logic\n",
        "        return True\n",
        "\n",
        "    def invariant_penalty(self, gamma: Dict[str, Any]) -> float:\n",
        "        # Placeholder invariant system\n",
        "        penalty = 0.0\n",
        "        for inv in self.spec.get(\"I\", {}).get(\"invariants\", []):\n",
        "            threshold = inv.get(\"threshold\", 0)\n",
        "            value = gamma.get(inv[\"name\"], 0)\n",
        "            if value > threshold:\n",
        "                penalty += abs(value - threshold)\n",
        "        return penalty\n",
        "\n",
        "    def objective(self, gamma: Dict[str, Any]) -> float:\n",
        "        total = 0.0\n",
        "        for term in self.spec.get(\"J\", {}).get(\"objective_terms\", []):\n",
        "            weight = term.get(\"weight\", 1.0)\n",
        "            value = gamma.get(term[\"name\"], 0)\n",
        "            total += weight * value\n",
        "        return total\n",
        "\n",
        "    def fitness(self, gamma: Dict[str, Any]) -> float:\n",
        "        if not self.check_constraints(gamma):\n",
        "            return float(\"-inf\")\n",
        "        penalty = self.invariant_penalty(gamma)\n",
        "        obj = self.objective(gamma)\n",
        "        return -(obj + penalty)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Mutation Engine\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def mutate_gamma(gamma: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    new_gamma = gamma.copy()\n",
        "    key = random.choice(list(gamma.keys()))\n",
        "    if isinstance(new_gamma[key], (int, float)):\n",
        "        new_gamma[key] += random.uniform(-1, 1)\n",
        "    return new_gamma\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Evolutionary Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evolve(cps: CPS, population: List[Dict[str, Any]], generations=10, retain=5):\n",
        "    log = []\n",
        "\n",
        "    for g in range(generations):\n",
        "        scored = []\n",
        "\n",
        "        for gamma in population:\n",
        "            fit = cps.fitness(gamma)\n",
        "            scored.append((fit, gamma))\n",
        "\n",
        "        scored.sort(reverse=True, key=lambda x: x[0])\n",
        "        population = [x[1] for x in scored[:retain]]\n",
        "\n",
        "        # Log best\n",
        "        best_fit = scored[0][0]\n",
        "        log.append({\"generation\": g, \"best_fitness\": best_fit})\n",
        "\n",
        "        # Mutate survivors\n",
        "        new_pop = population.copy()\n",
        "        while len(new_pop) < len(scored):\n",
        "            parent = random.choice(population)\n",
        "            new_pop.append(mutate_gamma(parent))\n",
        "\n",
        "        population = new_pop\n",
        "\n",
        "    return population, log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Minimal Generic CPS Spec\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "generic_spec = {\n",
        "    \"cps_id\": \"gin_cps_v1\",\n",
        "    \"I\": {\n",
        "        \"invariants\": [\n",
        "            {\"name\": \"violation_metric\", \"threshold\": 0.0}\n",
        "        ]\n",
        "    },\n",
        "    \"J\": {\n",
        "        \"objective_terms\": [\n",
        "            {\"name\": \"cost_metric\", \"weight\": 1.0}\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "cps = CPS(generic_spec)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Initial Population (Domain Neutral)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "population = [\n",
        "    {\"cost_metric\": random.uniform(0, 10),\n",
        "     \"violation_metric\": random.uniform(0, 2)}\n",
        "    for _ in range(20)\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Run Evolution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "final_pop, evolution_log = evolve(cps, population, generations=15)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Save Run Log\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"evolution_log.json\"), \"w\") as f:\n",
        "    json.dump(evolution_log, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 1 ENGINE INITIALIZATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Run ID:\", run_id)\n",
        "print(\"Best Final Candidate:\", final_pop[0])\n",
        "print(\"Log saved to:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "-A71Ic7lRpiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532aa22b-83a6-4992-9bad-69daef0a1b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 1 ENGINE INITIALIZATION COMPLETE\n",
            "======================================================================\n",
            "Run ID: 20260215T020315\n",
            "Best Final Candidate: {'cost_metric': -4.274041387055367, 'violation_metric': -0.27613559239617946}\n",
            "Log saved to: /content/shadowmap_cps_bootstrap/engine_runs/run_20260215T020315\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 2 — GRAPHICAL INFERENCE SUBSTRATE (GIN Layer v1)\n",
        "# ==============================================================================\n",
        "\n",
        "import random\n",
        "import math\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Build Generic Graph (domain neutral)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def build_graph(n_nodes=30, edge_prob=0.15):\n",
        "    G = nx.erdos_renyi_graph(n_nodes, edge_prob)\n",
        "    for u, v in G.edges():\n",
        "        G[u][v][\"weight\"] = random.uniform(0.5, 2.0)\n",
        "    return G\n",
        "\n",
        "G = build_graph()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Define Constraint Metric (invariant)\n",
        "# Count number of violated edges (example condition)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_violation(G, gamma):\n",
        "    violation = 0\n",
        "    threshold = gamma[\"threshold\"]\n",
        "\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        if data[\"weight\"] > threshold:\n",
        "            violation += 1\n",
        "\n",
        "    return violation\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Define Objective Metric\n",
        "# Minimize total edge weight\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_cost(G, gamma):\n",
        "    total = sum(data[\"weight\"] for _,_,data in G.edges(data=True))\n",
        "    return total * gamma[\"scale\"]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Bind Graph to CPS\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class GraphCPS(CPS):\n",
        "    def __init__(self, spec, graph):\n",
        "        super().__init__(spec)\n",
        "        self.graph = graph\n",
        "\n",
        "    def check_constraints(self, gamma):\n",
        "        return gamma[\"scale\"] > 0\n",
        "\n",
        "    def invariant_penalty(self, gamma):\n",
        "        return compute_violation(self.graph, gamma)\n",
        "\n",
        "    def objective(self, gamma):\n",
        "        return compute_cost(self.graph, gamma)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Graph CPS Spec\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "graph_spec = {\n",
        "    \"cps_id\": \"gin_graph_v1\"\n",
        "}\n",
        "\n",
        "graph_cps = GraphCPS(graph_spec, G)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Initial Population\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "population = [\n",
        "    {\"threshold\": random.uniform(0.5, 2.0),\n",
        "     \"scale\": random.uniform(0.1, 2.0)}\n",
        "    for _ in range(30)\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Run Evolution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "final_pop, evolution_log = evolve(graph_cps, population, generations=20)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Save Run\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"graph_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"evolution_log.json\"), \"w\") as f:\n",
        "    json.dump(evolution_log, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 2 GRAPH INFERENCE RUN COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Run ID:\", run_id)\n",
        "print(\"Best Candidate:\", final_pop[0])\n",
        "print(\"Edge Count:\", G.number_of_edges())\n",
        "print(\"Log saved to:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "REHb548RRp9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f03ad6-09d0-4214-c04e-19a03c1d3d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 2 GRAPH INFERENCE RUN COMPLETE\n",
            "======================================================================\n",
            "Run ID: 20260215T020315\n",
            "Best Candidate: {'threshold': 2.217671678813557, 'scale': 0.018350021302024344}\n",
            "Edge Count: 75\n",
            "Log saved to: /content/shadowmap_cps_bootstrap/engine_runs/graph_run_20260215T020315\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 3 — SURFACE-CODE-STYLE DECODING MODEL (GRAPH → SYNDROME INFERENCE)\n",
        "# ==============================================================================\n",
        "\n",
        "# This cell upgrades the generic graph into a minimal surface-code-like\n",
        "# bipartite Tanner graph:\n",
        "#   - Data qubits (variable nodes)\n",
        "#   - Stabilizers (check nodes)\n",
        "#   - Random noise injection\n",
        "#   - Syndrome generation\n",
        "#   - Simple MWPM-style weight scoring proxy\n",
        "#\n",
        "# We bind this into CPS and evolve decoder parameters.\n",
        "\n",
        "import random\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Build Bipartite Tanner Graph\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def build_surface_like_graph(n_data=25, n_checks=25, conn_prob=0.2):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    data_nodes = [f\"d{i}\" for i in range(n_data)]\n",
        "    check_nodes = [f\"s{i}\" for i in range(n_checks)]\n",
        "\n",
        "    G.add_nodes_from(data_nodes, bipartite=0)\n",
        "    G.add_nodes_from(check_nodes, bipartite=1)\n",
        "\n",
        "    for d in data_nodes:\n",
        "        for s in check_nodes:\n",
        "            if random.random() < conn_prob:\n",
        "                G.add_edge(d, s, weight=random.uniform(0.5, 2.0))\n",
        "\n",
        "    return G, data_nodes, check_nodes\n",
        "\n",
        "G, data_nodes, check_nodes = build_surface_like_graph()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Noise + Syndrome Generation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def inject_noise(data_nodes, p_error):\n",
        "    errors = {d: (random.random() < p_error) for d in data_nodes}\n",
        "    return errors\n",
        "\n",
        "def compute_syndrome(G, errors, check_nodes):\n",
        "    syndrome = {}\n",
        "    for s in check_nodes:\n",
        "        parity = 0\n",
        "        for neighbor in G.neighbors(s):\n",
        "            if errors.get(neighbor, False):\n",
        "                parity ^= 1\n",
        "        syndrome[s] = parity\n",
        "    return syndrome\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Decoder Proxy Cost + Invariant\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def decoding_cost(G, gamma, syndrome):\n",
        "    # proxy for matching weight sum\n",
        "    weight_sum = 0\n",
        "    for s, val in syndrome.items():\n",
        "        if val == 1:\n",
        "            for neighbor in G.neighbors(s):\n",
        "                weight_sum += G[s][neighbor][\"weight\"]\n",
        "    return weight_sum * gamma[\"alpha\"]\n",
        "\n",
        "def logical_failure_metric(syndrome):\n",
        "    # proxy: number of active checks\n",
        "    return sum(syndrome.values())\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Bind to CPS\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class SurfaceCodeCPS(CPS):\n",
        "    def __init__(self, graph, data_nodes, check_nodes):\n",
        "        super().__init__({})\n",
        "        self.graph = graph\n",
        "        self.data_nodes = data_nodes\n",
        "        self.check_nodes = check_nodes\n",
        "\n",
        "    def check_constraints(self, gamma):\n",
        "        return gamma[\"alpha\"] > 0 and 0 <= gamma[\"p_error\"] <= 0.3\n",
        "\n",
        "    def invariant_penalty(self, gamma):\n",
        "        errors = inject_noise(self.data_nodes, gamma[\"p_error\"])\n",
        "        syndrome = compute_syndrome(self.graph, errors, self.check_nodes)\n",
        "        return logical_failure_metric(syndrome)\n",
        "\n",
        "    def objective(self, gamma):\n",
        "        errors = inject_noise(self.data_nodes, gamma[\"p_error\"])\n",
        "        syndrome = compute_syndrome(self.graph, errors, self.check_nodes)\n",
        "        return decoding_cost(self.graph, gamma, syndrome)\n",
        "\n",
        "surface_cps = SurfaceCodeCPS(G, data_nodes, check_nodes)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Initial Decoder Population\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "population = [\n",
        "    {\"alpha\": random.uniform(0.1, 2.0),\n",
        "     \"p_error\": random.uniform(0.01, 0.2)}\n",
        "    for _ in range(40)\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Run Evolution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "final_pop, evolution_log = evolve(surface_cps, population, generations=25)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Save Run\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"surface_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"evolution_log.json\"), \"w\") as f:\n",
        "    json.dump(evolution_log, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 3 SURFACE-CODE DECODING RUN COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Run ID:\", run_id)\n",
        "print(\"Best Decoder Params:\", final_pop[0])\n",
        "print(\"Data Nodes:\", len(data_nodes))\n",
        "print(\"Check Nodes:\", len(check_nodes))\n",
        "print(\"Edges:\", G.number_of_edges())\n",
        "print(\"Log saved to:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "Wiq4DoMbRqgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb41a4d1-dec2-40ff-dcba-056f64b727d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 3 SURFACE-CODE DECODING RUN COMPLETE\n",
            "======================================================================\n",
            "Run ID: 20260215T020315\n",
            "Best Decoder Params: {'alpha': 0.522139325164434, 'p_error': 0.008751676122201936}\n",
            "Data Nodes: 25\n",
            "Check Nodes: 25\n",
            "Edges: 137\n",
            "Log saved to: /content/shadowmap_cps_bootstrap/engine_runs/surface_run_20260215T020315\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 4 — DETERMINISTIC MONTE CARLO EVALUATION (STABLE FITNESS)\n",
        "# ==============================================================================\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Deterministic Noise Trial\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def inject_noise_deterministic(data_nodes, p_error, seed):\n",
        "    rng = random.Random(seed)\n",
        "    return {d: (rng.random() < p_error) for d in data_nodes}\n",
        "\n",
        "def compute_syndrome(G, errors, check_nodes):\n",
        "    syndrome = {}\n",
        "    for s in check_nodes:\n",
        "        parity = 0\n",
        "        for neighbor in G.neighbors(s):\n",
        "            if errors.get(neighbor, False):\n",
        "                parity ^= 1\n",
        "        syndrome[s] = parity\n",
        "    return syndrome\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Improved Decoder Metrics\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def decoding_cost(G, gamma, syndrome):\n",
        "    weight_sum = 0\n",
        "    for s, val in syndrome.items():\n",
        "        if val == 1:\n",
        "            for neighbor in G.neighbors(s):\n",
        "                weight_sum += G[s][neighbor][\"weight\"]\n",
        "    return weight_sum * gamma[\"alpha\"]\n",
        "\n",
        "def logical_failure_metric(syndrome):\n",
        "    # True failure proxy: any odd number of unsatisfied checks\n",
        "    return 1 if sum(syndrome.values()) % 2 != 0 else 0\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# New CPS Class with Monte Carlo Averaging\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class SurfaceCodeMonteCarloCPS:\n",
        "    def __init__(self, graph, data_nodes, check_nodes, trials=25):\n",
        "        self.graph = graph\n",
        "        self.data_nodes = data_nodes\n",
        "        self.check_nodes = check_nodes\n",
        "        self.trials = trials\n",
        "\n",
        "    def check_constraints(self, gamma):\n",
        "        return gamma[\"alpha\"] > 0 and 0 <= gamma[\"p_error\"] <= 0.3\n",
        "\n",
        "    def evaluate(self, gamma):\n",
        "        if not self.check_constraints(gamma):\n",
        "            return float(\"-inf\"), {\"feasible\": False}\n",
        "\n",
        "        total_cost = 0\n",
        "        failures = 0\n",
        "\n",
        "        for t in range(self.trials):\n",
        "            seed = hash((gamma[\"alpha\"], gamma[\"p_error\"], t)) % (2**32)\n",
        "            errors = inject_noise_deterministic(self.data_nodes, gamma[\"p_error\"], seed)\n",
        "            syndrome = compute_syndrome(self.graph, errors, self.check_nodes)\n",
        "\n",
        "            total_cost += decoding_cost(self.graph, gamma, syndrome)\n",
        "            failures += logical_failure_metric(syndrome)\n",
        "\n",
        "        avg_cost = total_cost / self.trials\n",
        "        failure_rate = failures / self.trials\n",
        "\n",
        "        fitness = -(avg_cost + 100 * failure_rate)\n",
        "\n",
        "        return fitness, {\n",
        "            \"feasible\": True,\n",
        "            \"avg_cost\": avg_cost,\n",
        "            \"failure_rate\": failure_rate\n",
        "        }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Instantiate CPS\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "surface_mc = SurfaceCodeMonteCarloCPS(G, data_nodes, check_nodes, trials=40)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Evolution Loop (updated for new CPS)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evolve_mc(cps, population, generations=20, retain=10):\n",
        "    log = []\n",
        "\n",
        "    for g in range(generations):\n",
        "        scored = []\n",
        "\n",
        "        for gamma in population:\n",
        "            fit, metrics = cps.evaluate(gamma)\n",
        "            scored.append((fit, gamma, metrics))\n",
        "\n",
        "        scored.sort(reverse=True, key=lambda x: x[0])\n",
        "        population = [x[1] for x in scored[:retain]]\n",
        "\n",
        "        best_fit, best_gamma, best_metrics = scored[0]\n",
        "\n",
        "        log.append({\n",
        "            \"generation\": g,\n",
        "            \"best_fitness\": best_fit,\n",
        "            \"best_gamma\": best_gamma,\n",
        "            \"best_metrics\": best_metrics\n",
        "        })\n",
        "\n",
        "        # mutate\n",
        "        new_pop = population.copy()\n",
        "        while len(new_pop) < len(scored):\n",
        "            parent = random.choice(population)\n",
        "            child = parent.copy()\n",
        "            child[\"alpha\"] += random.uniform(-0.2, 0.2)\n",
        "            child[\"p_error\"] += random.uniform(-0.01, 0.01)\n",
        "            new_pop.append(child)\n",
        "\n",
        "        population = new_pop\n",
        "\n",
        "    return population, log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Run Monte Carlo Evolution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "population = [\n",
        "    {\"alpha\": random.uniform(0.5, 3.0),\n",
        "     \"p_error\": random.uniform(0.01, 0.15)}\n",
        "    for _ in range(40)\n",
        "]\n",
        "\n",
        "final_pop, evolution_log = evolve_mc(surface_mc, population, generations=30)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Save Run\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"surface_mc_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"evolution_log.json\"), \"w\") as f:\n",
        "    json.dump(evolution_log, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 4 MONTE CARLO DECODER RUN COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Run ID:\", run_id)\n",
        "print(\"Best Decoder Params:\", final_pop[0])\n",
        "print(\"Trials per evaluation:\", surface_mc.trials)\n",
        "print(\"Log saved to:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "6riXH-ncRqsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0cc26a-c388-4760-9d82-623e8cae0a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 4 MONTE CARLO DECODER RUN COMPLETE\n",
            "======================================================================\n",
            "Run ID: 20260215T020315\n",
            "Best Decoder Params: {'alpha': 2.279023839661767, 'p_error': 0.0006201664062661779}\n",
            "Trials per evaluation: 40\n",
            "Log saved to: /content/shadowmap_cps_bootstrap/engine_runs/surface_mc_run_20260215T020315\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 5 — MINIMUM-WEIGHT MATCHING DECODER (MWPM PROXY)\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Precompute shortest paths between check nodes\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "check_graph = G.copy()\n",
        "\n",
        "# Build check-to-check graph via shared data qubits\n",
        "check_adj = nx.Graph()\n",
        "check_adj.add_nodes_from(check_nodes)\n",
        "\n",
        "for d in data_nodes:\n",
        "    neighbors = list(G.neighbors(d))\n",
        "    for i in range(len(neighbors)):\n",
        "        for j in range(i+1, len(neighbors)):\n",
        "            check_adj.add_edge(neighbors[i], neighbors[j], weight=1.0)\n",
        "\n",
        "# Shortest path lengths\n",
        "shortest_paths = dict(nx.all_pairs_shortest_path_length(check_adj))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MWPM Decoder Proxy\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def mwpm_cost(defects):\n",
        "    if len(defects) % 2 != 0:\n",
        "        return float(\"inf\")  # odd number → uncorrectable\n",
        "\n",
        "    # build complete graph between defects\n",
        "    complete = nx.Graph()\n",
        "    for i in range(len(defects)):\n",
        "        for j in range(i+1, len(defects)):\n",
        "            d1 = defects[i]\n",
        "            d2 = defects[j]\n",
        "            dist = shortest_paths.get(d1, {}).get(d2, 100)\n",
        "            complete.add_edge(d1, d2, weight=dist)\n",
        "\n",
        "    matching = nx.algorithms.matching.min_weight_matching(\n",
        "        complete, weight=\"weight\"\n",
        "    )\n",
        "\n",
        "    total = 0\n",
        "    for u, v in matching:\n",
        "        total += complete[u][v][\"weight\"]\n",
        "\n",
        "    return total\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Monte Carlo CPS with MWPM\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class SurfaceCodeMWPMCPS:\n",
        "    def __init__(self, graph, data_nodes, check_nodes, trials=30):\n",
        "        self.graph = graph\n",
        "        self.data_nodes = data_nodes\n",
        "        self.check_nodes = check_nodes\n",
        "        self.trials = trials\n",
        "\n",
        "    def evaluate(self, gamma):\n",
        "        total_cost = 0\n",
        "        failures = 0\n",
        "\n",
        "        for t in range(self.trials):\n",
        "            seed = hash((gamma[\"p_error\"], t)) % (2**32)\n",
        "            rng = random.Random(seed)\n",
        "\n",
        "            errors = {d: (rng.random() < gamma[\"p_error\"]) for d in self.data_nodes}\n",
        "            syndrome = compute_syndrome(self.graph, errors, self.check_nodes)\n",
        "\n",
        "            defects = [s for s, val in syndrome.items() if val == 1]\n",
        "\n",
        "            cost = mwpm_cost(defects)\n",
        "            if cost == float(\"inf\"):\n",
        "                failures += 1\n",
        "            else:\n",
        "                total_cost += cost * gamma[\"alpha\"]\n",
        "\n",
        "        avg_cost = total_cost / self.trials\n",
        "        failure_rate = failures / self.trials\n",
        "\n",
        "        fitness = -(avg_cost + 200 * failure_rate)\n",
        "\n",
        "        return fitness, {\n",
        "            \"avg_cost\": avg_cost,\n",
        "            \"failure_rate\": failure_rate\n",
        "        }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Run MWPM Evolution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "surface_mwpm = SurfaceCodeMWPMCPS(G, data_nodes, check_nodes, trials=40)\n",
        "\n",
        "population = [\n",
        "    {\"alpha\": random.uniform(0.5, 3.0),\n",
        "     \"p_error\": random.uniform(0.01, 0.15)}\n",
        "    for _ in range(40)\n",
        "]\n",
        "\n",
        "final_pop, evolution_log = evolve_mc(surface_mwpm, population, generations=25)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Save Run\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"surface_mwpm_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"evolution_log.json\"), \"w\") as f:\n",
        "    json.dump(evolution_log, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 5 MWPM DECODER RUN COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Run ID:\", run_id)\n",
        "print(\"Best Decoder Params:\", final_pop[0])\n",
        "print(\"Trials:\", surface_mwpm.trials)\n",
        "print(\"Log saved to:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "2bF8ySroRq6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5c44bb-cdea-4725-d1f0-3181796862e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 5 MWPM DECODER RUN COMPLETE\n",
            "======================================================================\n",
            "Run ID: 20260215T020317\n",
            "Best Decoder Params: {'alpha': 2.0232503938483184, 'p_error': 0.0005139987776860387}\n",
            "Trials: 40\n",
            "Log saved to: /content/shadowmap_cps_bootstrap/engine_runs/surface_mwpm_run_20260215T020317\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 6B — PATCH THE BUG + RE-RUN THE LOGICAL FAILURE SWEEP (RUN BELOW CELL 6)\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# --- Patch: ensure boundary nodes exist in error_graph before path queries\n",
        "def detect_logical_error(G, total_error_edges):\n",
        "    error_graph = nx.Graph()\n",
        "    error_graph.add_nodes_from(G.nodes())          # critical fix\n",
        "    error_graph.add_edges_from(total_error_edges)\n",
        "\n",
        "    for l in left_boundary:\n",
        "        for r in right_boundary:\n",
        "            if nx.has_path(error_graph, l, r):\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "# --- Re-run sweep with patched detector\n",
        "results = {}\n",
        "for p in np.linspace(0.01, 0.20, 10):\n",
        "    rate = logical_failure_rate(lattice, p, trials=60)\n",
        "    results[round(float(p), 3)] = rate\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 6B PATCHED LOGICAL ERROR SWEEP COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Code distance:\", d)\n",
        "print(\"Results (p_error -> logical_failure_rate):\")\n",
        "print(results)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "-1iLmD14RrYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d2eec6-2038-490a-d81f-461489fa8679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 6B PATCHED LOGICAL ERROR SWEEP COMPLETE\n",
            "======================================================================\n",
            "Code distance: d24\n",
            "Results (p_error -> logical_failure_rate):\n",
            "{0.01: 0.0, 0.031: 0.0, 0.052: 0.0, 0.073: 0.016666666666666666, 0.094: 0.0, 0.116: 0.0, 0.137: 0.0, 0.158: 0.08333333333333333, 0.179: 0.06666666666666667, 0.2: 0.16666666666666666}\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 7 — DISTANCE SCALING + THRESHOLD-STYLE TABLE (CPU-FRIENDLY)\n",
        "# Run this BELOW Cell 6B\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# We will reuse the functions/objects already defined above:\n",
        "# - build_planar_lattice\n",
        "# - inject_edge_noise\n",
        "# - compute_vertex_syndrome\n",
        "# - apply_matching\n",
        "# - detect_logical_error (patched)\n",
        "# - logical_failure_rate (uses the patched detect_logical_error)\n",
        "\n",
        "# Small sweep for multiple code distances.\n",
        "# Keep trials modest to stay fast on CPU; you can increase later.\n",
        "distances = [5, 7, 9]\n",
        "p_values = [0.05, 0.08, 0.11, 0.14, 0.17, 0.20]\n",
        "trials_per_point = 80\n",
        "\n",
        "def run_distance_sweep(d_val):\n",
        "    global d, lattice, left_boundary, right_boundary\n",
        "    d = d_val\n",
        "    lattice = build_planar_lattice(d)\n",
        "    left_boundary = [n for n in lattice.nodes() if n % d == 0]\n",
        "    right_boundary = [n for n in lattice.nodes() if n % d == d-1]\n",
        "\n",
        "    row = {}\n",
        "    for p in p_values:\n",
        "        row[p] = logical_failure_rate(lattice, p, trials=trials_per_point)\n",
        "    return row\n",
        "\n",
        "results = {}\n",
        "for d_val in distances:\n",
        "    results[d_val] = run_distance_sweep(d_val)\n",
        "\n",
        "# Print a clean table\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 7 DISTANCE SCALING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Trials per point: {trials_per_point}\")\n",
        "print(\"p_values:\", p_values)\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Header\n",
        "header = \"d \\\\ p | \" + \" | \".join([f\"{p:>5.2f}\" for p in p_values])\n",
        "print(header)\n",
        "print(\"-\"*len(header))\n",
        "\n",
        "for d_val in distances:\n",
        "    vals = [results[d_val][p] for p in p_values]\n",
        "    line = f\"{d_val:>4} | \" + \" | \".join([f\"{v:>5.3f}\" for v in vals])\n",
        "    print(line)\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"Raw dict:\", results)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "m_LHJrJeRrmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e16fa0f-994a-4a0b-f044-b18e74c0f11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 7 DISTANCE SCALING RESULTS\n",
            "======================================================================\n",
            "Trials per point: 80\n",
            "p_values: [0.05, 0.08, 0.11, 0.14, 0.17, 0.2]\n",
            "----------------------------------------------------------------------\n",
            "d \\ p |  0.05 |  0.08 |  0.11 |  0.14 |  0.17 |  0.20\n",
            "-----------------------------------------------------\n",
            "   5 | 0.000 | 0.000 | 0.013 | 0.013 | 0.037 | 0.125\n",
            "   7 | 0.000 | 0.000 | 0.000 | 0.013 | 0.037 | 0.150\n",
            "   9 | 0.000 | 0.000 | 0.000 | 0.025 | 0.037 | 0.100\n",
            "----------------------------------------------------------------------\n",
            "Raw dict: {5: {0.05: 0.0, 0.08: 0.0, 0.11: 0.0125, 0.14: 0.0125, 0.17: 0.0375, 0.2: 0.125}, 7: {0.05: 0.0, 0.08: 0.0, 0.11: 0.0, 0.14: 0.0125, 0.17: 0.0375, 0.2: 0.15}, 9: {0.05: 0.0, 0.08: 0.0, 0.11: 0.0, 0.14: 0.025, 0.17: 0.0375, 0.2: 0.1}}\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLj3HOJWU-bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 8 — STATISTICALLY CLEAN SCALING (BINOMIAL CI + MORE TRIALS WHERE IT MATTERS)\n",
        "# Run this BELOW Cell 7\n",
        "# ==============================================================================\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Reuses from above:\n",
        "# - build_planar_lattice\n",
        "# - logical_failure_rate\n",
        "# - detect_logical_error (patched)\n",
        "# - and globals left_boundary/right_boundary pattern\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Binomial Wilson score interval (stable, no external libs)\n",
        "# ----------------------------------------------------------------------\n",
        "def wilson_ci(k, n, z=1.96):\n",
        "    if n == 0:\n",
        "        return (0.0, 0.0)\n",
        "    phat = k / n\n",
        "    denom = 1.0 + (z*z)/n\n",
        "    center = (phat + (z*z)/(2*n)) / denom\n",
        "    half = (z * math.sqrt((phat*(1-phat) + (z*z)/(4*n)) / n)) / denom\n",
        "    lo = max(0.0, center - half)\n",
        "    hi = min(1.0, center + half)\n",
        "    return (lo, hi)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Deterministic sweep runner that also returns counts (k,n)\n",
        "# We re-run trials internally so we can compute confidence intervals.\n",
        "# ----------------------------------------------------------------------\n",
        "def logical_failure_count(G, p_error, trials=200):\n",
        "    failures = 0\n",
        "    for t in range(trials):\n",
        "        seed = hash((p_error, t)) % (2**32)\n",
        "        edge_errors = inject_edge_noise(G, p_error, seed)\n",
        "        syndrome = compute_vertex_syndrome(G, edge_errors)\n",
        "        defects = [n for n, v in syndrome.items() if v == 1]\n",
        "        correction = apply_matching(G, defects)\n",
        "        if correction is None:\n",
        "            failures += 1\n",
        "            continue\n",
        "        total_error_edges = [e for e, err in edge_errors.items() if err] + correction\n",
        "        failures += detect_logical_error(G, total_error_edges)\n",
        "    return failures, trials\n",
        "\n",
        "def run_sweep_with_ci(distances, p_values, trials_map):\n",
        "    out = {}\n",
        "    for d_val in distances:\n",
        "        # rebuild lattice + boundaries for each distance\n",
        "        lattice = build_planar_lattice(d_val)\n",
        "        left_boundary = [n for n in lattice.nodes() if n % d_val == 0]\n",
        "        right_boundary = [n for n in lattice.nodes() if n % d_val == d_val-1]\n",
        "\n",
        "        # bind boundaries into global scope used by detect_logical_error\n",
        "        globals()[\"left_boundary\"] = left_boundary\n",
        "        globals()[\"right_boundary\"] = right_boundary\n",
        "\n",
        "        out[d_val] = {}\n",
        "        for p in p_values:\n",
        "            ntrials = trials_map(p)\n",
        "            k, n = logical_failure_count(lattice, p, trials=ntrials)\n",
        "            rate = k / n\n",
        "            lo, hi = wilson_ci(k, n)\n",
        "            out[d_val][p] = {\"rate\": rate, \"k\": k, \"n\": n, \"ci95\": (lo, hi)}\n",
        "    return out\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Settings: concentrate trials near the apparent transition (~0.14–0.20)\n",
        "# ----------------------------------------------------------------------\n",
        "distances = [5, 7, 9]\n",
        "p_values = [0.08, 0.11, 0.14, 0.17, 0.20, 0.23]\n",
        "\n",
        "def trials_map(p):\n",
        "    # more trials where failure rates start to rise\n",
        "    if p < 0.12:\n",
        "        return 150\n",
        "    if p < 0.18:\n",
        "        return 300\n",
        "    return 500\n",
        "\n",
        "results_ci = run_sweep_with_ci(distances, p_values, trials_map)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Print compact table: rate [lo,hi]\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 8 SCALING WITH 95% BINOMIAL CI (WILSON)\")\n",
        "print(\"=\"*70)\n",
        "print(\"Format: rate [lo, hi]  (k/n)\")\n",
        "print(\"p_values:\", p_values)\n",
        "print(\"-\"*70)\n",
        "\n",
        "header = \"d \\\\ p | \" + \" | \".join([f\"{p:>5.2f}\" for p in p_values])\n",
        "print(header)\n",
        "print(\"-\"*len(header))\n",
        "\n",
        "for d_val in distances:\n",
        "    parts = []\n",
        "    for p in p_values:\n",
        "        r = results_ci[d_val][p][\"rate\"]\n",
        "        lo, hi = results_ci[d_val][p][\"ci95\"]\n",
        "        k = results_ci[d_val][p][\"k\"]\n",
        "        n = results_ci[d_val][p][\"n\"]\n",
        "        parts.append(f\"{r:0.3f}[{lo:0.3f},{hi:0.3f}]({k}/{n})\")\n",
        "    print(f\"{d_val:>4} | \" + \" | \".join(parts))\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"Raw dict:\", results_ci)\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Optional: crude \"crossing\" indicator (which p looks most distance-invariant)\n",
        "# (Not a true threshold estimate, but helps choose next refinement step.)\n",
        "# ----------------------------------------------------------------------\n",
        "def dispersion_at_p(p):\n",
        "    rates = [results_ci[dv][p][\"rate\"] for dv in distances]\n",
        "    mean = sum(rates)/len(rates)\n",
        "    var = sum((x-mean)**2 for x in rates)/len(rates)\n",
        "    return math.sqrt(var)\n",
        "\n",
        "disp = {p: dispersion_at_p(p) for p in p_values}\n",
        "best_p = min(disp, key=disp.get)\n",
        "print(\"Dispersion by p (lower ~ more distance-invariant):\", disp)\n",
        "print(\"Lowest-dispersion p (candidate crossing region):\", best_p)"
      ],
      "metadata": {
        "id": "2FpAGZFCRr-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b29cdd-b97c-4dec-ae2d-c2ac84fff4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 8 SCALING WITH 95% BINOMIAL CI (WILSON)\n",
            "======================================================================\n",
            "Format: rate [lo, hi]  (k/n)\n",
            "p_values: [0.08, 0.11, 0.14, 0.17, 0.2, 0.23]\n",
            "----------------------------------------------------------------------\n",
            "d \\ p |  0.08 |  0.11 |  0.14 |  0.17 |  0.20 |  0.23\n",
            "-----------------------------------------------------\n",
            "   5 | 0.000[0.000,0.025](0/150) | 0.013[0.004,0.047](2/150) | 0.010[0.003,0.029](3/300) | 0.040[0.023,0.069](12/300) | 0.116[0.091,0.147](58/500) | 0.164[0.134,0.199](82/500)\n",
            "   7 | 0.000[0.000,0.025](0/150) | 0.000[0.000,0.025](0/150) | 0.030[0.016,0.056](9/300) | 0.057[0.036,0.089](17/300) | 0.122[0.096,0.154](61/500) | 0.156[0.127,0.190](78/500)\n",
            "   9 | 0.000[0.000,0.025](0/150) | 0.000[0.000,0.025](0/150) | 0.017[0.007,0.038](5/300) | 0.053[0.033,0.085](16/300) | 0.104[0.080,0.134](52/500) | 0.196[0.164,0.233](98/500)\n",
            "----------------------------------------------------------------------\n",
            "Raw dict: {5: {0.08: {'rate': 0.0, 'k': 0, 'n': 150, 'ci95': (0.0, 0.02497113914571871)}, 0.11: {'rate': 0.013333333333333334, 'k': 2, 'n': 150, 'ci95': (0.0036640523882232115, 0.047307856380276334)}, 0.14: {'rate': 0.01, 'k': 3, 'n': 300, 'ci95': (0.0034065567627181124, 0.028984004635062833)}, 0.17: {'rate': 0.04, 'k': 12, 'n': 300, 'ci95': (0.02302709213793877, 0.06860486345997804)}, 0.2: {'rate': 0.116, 'k': 58, 'n': 500, 'ci95': (0.09081324505614743, 0.14704246197558635)}, 0.23: {'rate': 0.164, 'k': 82, 'n': 500, 'ci95': (0.13412841862886243, 0.19899532502390466)}}, 7: {0.08: {'rate': 0.0, 'k': 0, 'n': 150, 'ci95': (0.0, 0.02497113914571871)}, 0.11: {'rate': 0.0, 'k': 0, 'n': 150, 'ci95': (0.0, 0.02497113914571871)}, 0.14: {'rate': 0.03, 'k': 9, 'n': 300, 'ci95': (0.015861673205964575, 0.05602315099190695)}, 0.17: {'rate': 0.056666666666666664, 'k': 17, 'n': 300, 'ci95': (0.03567717591318777, 0.08886666535147117)}, 0.2: {'rate': 0.122, 'k': 61, 'n': 500, 'ci95': (0.0961588543796491, 0.15360535722971383)}, 0.23: {'rate': 0.156, 'k': 78, 'n': 500, 'ci95': (0.12683030337229503, 0.19041543417696646)}}, 9: {0.08: {'rate': 0.0, 'k': 0, 'n': 150, 'ci95': (0.0, 0.02497113914571871)}, 0.11: {'rate': 0.0, 'k': 0, 'n': 150, 'ci95': (0.0, 0.02497113914571871)}, 0.14: {'rate': 0.016666666666666666, 'k': 5, 'n': 300, 'ci95': (0.0071393720427721935, 0.038415943621705605)}, 0.17: {'rate': 0.05333333333333334, 'k': 16, 'n': 300, 'ci95': (0.03309162538404202, 0.08486983874726851)}, 0.2: {'rate': 0.104, 'k': 52, 'n': 500, 'ci95': (0.08019382152134363, 0.13384487635513181)}, 0.23: {'rate': 0.196, 'k': 98, 'n': 500, 'ci95': (0.16357755483279346, 0.23305821323399578)}}}\n",
            "======================================================================\n",
            "Dispersion by p (lower ~ more distance-invariant): {0.08: 0.0, 0.11: 0.00628539361054709, 0.14: 0.00831479419283098, 0.17: 0.0072008229982309544, 0.2: 0.007483314773547884, 0.23: 0.017281975195754296}\n",
            "Lowest-dispersion p (candidate crossing region): 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 9 — THRESHOLD ESTIMATION (ISO-FAILURE CROSSING) + MORE DISTANCES\n",
        "# Run this BELOW Cell 8\n",
        "# ==============================================================================\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Reuses from above:\n",
        "# - build_planar_lattice\n",
        "# - inject_edge_noise\n",
        "# - compute_vertex_syndrome\n",
        "# - apply_matching\n",
        "# - detect_logical_error (patched)\n",
        "# - logical_failure_count\n",
        "# - wilson_ci\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Helper: sweep p for a given distance and return (p, rate, ci, k, n)\n",
        "# ----------------------------------------------------------------------\n",
        "def sweep_distance(d_val, p_grid, trials_per_p):\n",
        "    lattice = build_planar_lattice(d_val)\n",
        "    left_boundary = [n for n in lattice.nodes() if n % d_val == 0]\n",
        "    right_boundary = [n for n in lattice.nodes() if n % d_val == d_val-1]\n",
        "    globals()[\"left_boundary\"] = left_boundary\n",
        "    globals()[\"right_boundary\"] = right_boundary\n",
        "\n",
        "    rows = []\n",
        "    for p in p_grid:\n",
        "        k, n = logical_failure_count(lattice, p, trials=trials_per_p(p))\n",
        "        rate = k / n\n",
        "        lo, hi = wilson_ci(k, n)\n",
        "        rows.append({\"p\": float(p), \"rate\": rate, \"ci95\": (lo, hi), \"k\": k, \"n\": n})\n",
        "    return rows\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Helper: estimate p* where rate crosses a target (linear interp on grid)\n",
        "# ----------------------------------------------------------------------\n",
        "def estimate_p_at_target(rows, target=0.10):\n",
        "    # assumes rows sorted by p\n",
        "    rows = sorted(rows, key=lambda r: r[\"p\"])\n",
        "    for i in range(len(rows) - 1):\n",
        "        r1, r2 = rows[i][\"rate\"], rows[i+1][\"rate\"]\n",
        "        p1, p2 = rows[i][\"p\"], rows[i+1][\"p\"]\n",
        "        if (r1 - target) == 0:\n",
        "            return p1\n",
        "        if (r1 - target) * (r2 - target) < 0:\n",
        "            # crossing\n",
        "            t = (target - r1) / (r2 - r1)\n",
        "            return p1 + t * (p2 - p1)\n",
        "    return None  # no crossing in range\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Settings: focus on the rising region\n",
        "# ----------------------------------------------------------------------\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = np.round(np.linspace(0.14, 0.24, 9), 3)  # 0.14..0.24\n",
        "\n",
        "def trials_per_p(p):\n",
        "    # heavier sampling as p increases\n",
        "    if p < 0.17:\n",
        "        return 400\n",
        "    if p < 0.21:\n",
        "        return 700\n",
        "    return 900\n",
        "\n",
        "target_rate = 0.10\n",
        "\n",
        "all_rows = {}\n",
        "pstars = {}\n",
        "\n",
        "for d_val in distances:\n",
        "    rows = sweep_distance(d_val, p_grid, trials_per_p)\n",
        "    all_rows[d_val] = rows\n",
        "    pstars[d_val] = estimate_p_at_target(rows, target=target_rate)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Print table\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 9 THRESHOLD ESTIMATION VIA ISO-FAILURE CROSSING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Target logical failure rate: {target_rate}\")\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"-\"*70)\n",
        "\n",
        "# header\n",
        "header = \"d \\\\ p | \" + \" | \".join([f\"{p:>5.3f}\" for p in p_grid])\n",
        "print(header)\n",
        "print(\"-\"*len(header))\n",
        "\n",
        "for d_val in distances:\n",
        "    parts = []\n",
        "    for r in all_rows[d_val]:\n",
        "        rate = r[\"rate\"]\n",
        "        lo, hi = r[\"ci95\"]\n",
        "        parts.append(f\"{rate:0.3f}\")\n",
        "    print(f\"{d_val:>4} | \" + \" | \".join([f\"{x:>5}\" for x in parts]))\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"Estimated p* where rate ~= 0.10 (linear interp on grid):\")\n",
        "for d_val in distances:\n",
        "    print(f\"  d={d_val}: p* = {pstars[d_val]}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Also show the CI-aware band at p=0.20 (often near crossing in your data)\n",
        "probe_p = 0.200\n",
        "print(f\"CI check at p={probe_p}:\")\n",
        "for d_val in distances:\n",
        "    row = next(rr for rr in all_rows[d_val] if abs(rr[\"p\"]-probe_p) < 1e-9)\n",
        "    print(f\"  d={d_val}: rate={row['rate']:.3f}  ci95=[{row['ci95'][0]:.3f},{row['ci95'][1]:.3f}]  ({row['k']}/{row['n']})\")\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "5KFKkbYNU4pE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "e2e66673-7117-43b2-8da9-62adbe5520e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 9 THRESHOLD ESTIMATION VIA ISO-FAILURE CROSSING\n",
            "======================================================================\n",
            "Target logical failure rate: 0.1\n",
            "p_grid: [np.float64(0.14), np.float64(0.153), np.float64(0.165), np.float64(0.178), np.float64(0.19), np.float64(0.202), np.float64(0.215), np.float64(0.227), np.float64(0.24)]\n",
            "----------------------------------------------------------------------\n",
            "d \\ p | 0.140 | 0.153 | 0.165 | 0.178 | 0.190 | 0.202 | 0.215 | 0.227 | 0.240\n",
            "-----------------------------------------------------------------------------\n",
            "   5 | 0.018 | 0.045 | 0.048 | 0.081 | 0.119 | 0.123 | 0.152 | 0.207 | 0.216\n",
            "   7 | 0.033 | 0.065 | 0.060 | 0.083 | 0.107 | 0.144 | 0.181 | 0.186 | 0.224\n",
            "   9 | 0.018 | 0.033 | 0.060 | 0.080 | 0.094 | 0.161 | 0.156 | 0.191 | 0.259\n",
            "  11 | 0.013 | 0.030 | 0.052 | 0.080 | 0.117 | 0.137 | 0.181 | 0.222 | 0.274\n",
            "----------------------------------------------------------------------\n",
            "Estimated p* where rate ~= 0.10 (linear interp on grid):\n",
            "  d=5: p* = 0.184\n",
            "  d=7: p* = 0.1864705882352941\n",
            "  d=9: p* = 0.1910212765957447\n",
            "  d=11: p* = 0.18446153846153845\n",
            "----------------------------------------------------------------------\n",
            "CI check at p=0.2:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3133408340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CI check at p={probe_p}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprobe_p\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  d={d_val}: rate={row['rate']:.3f}  ci95=[{row['ci95'][0]:.3f},{row['ci95'][1]:.3f}]  ({row['k']}/{row['n']})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 9B — PATCH THE PROBE LOOKUP + PRINT CI CHECK NEAR p=0.200\n",
        "# Run this BELOW the errored Cell 9\n",
        "# ==============================================================================\n",
        "\n",
        "import math\n",
        "\n",
        "# Cell 9 used probe_p=0.200, but p_grid is [0.14, 0.153, 0.165, 0.178, 0.19, 0.202, ...]\n",
        "# So 0.200 is not present; we should pick the nearest p in the grid (0.202).\n",
        "\n",
        "probe_p = 0.200\n",
        "\n",
        "def nearest_row(rows, p):\n",
        "    best = None\n",
        "    best_abs = float(\"inf\")\n",
        "    for rr in rows:\n",
        "        a = abs(rr[\"p\"] - p)\n",
        "        if a < best_abs:\n",
        "            best_abs = a\n",
        "            best = rr\n",
        "    return best, best_abs\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 9B CI CHECK (NEAREST GRID POINT TO p=0.200)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for d_val in distances:\n",
        "    row, delta = nearest_row(all_rows[d_val], probe_p)\n",
        "    lo, hi = row[\"ci95\"]\n",
        "    print(\n",
        "        f\"d={d_val}: nearest_p={row['p']:.3f} (|Δ|={delta:.3f})  \"\n",
        "        f\"rate={row['rate']:.3f}  ci95=[{lo:.3f},{hi:.3f}]  ({row['k']}/{row['n']})\"\n",
        "    )\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "SUP3vNbqU50F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea054a89-fd1b-4689-a9fd-768e66d8cea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 9B CI CHECK (NEAREST GRID POINT TO p=0.200)\n",
            "======================================================================\n",
            "d=5: nearest_p=0.202 (|Δ|=0.002)  rate=0.123  ci95=[0.101,0.149]  (86/700)\n",
            "d=7: nearest_p=0.202 (|Δ|=0.002)  rate=0.144  ci95=[0.120,0.172]  (101/700)\n",
            "d=9: nearest_p=0.202 (|Δ|=0.002)  rate=0.161  ci95=[0.136,0.191]  (113/700)\n",
            "d=11: nearest_p=0.202 (|Δ|=0.002)  rate=0.137  ci95=[0.114,0.165]  (96/700)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 10 — CPS BINDING: THRESHOLD-FINDING AS AN OPTIMIZATION PROBLEM\n",
        "# Run this BELOW Cell 9B\n",
        "# ==============================================================================\n",
        "\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Goal: treat \"find p* where logical failure ~= target_rate\" as a CPS optimization.\n",
        "# Candidate gamma = {\"p_star\": ..., \"d\": ..., \"target\": ...}\n",
        "# Objective: minimize |failure_rate(d, p_star) - target|\n",
        "# Constraint: p_star in [0.12, 0.26], d in allowed set\n",
        "#\n",
        "# This keeps forward options open:\n",
        "# - same CPS form works for decoder params, lattice coupling beta, scheduling slack, etc.\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Settings\n",
        "# ----------------------------------------------------------------------\n",
        "allowed_distances = [5, 7, 9, 11]\n",
        "target_rate = 0.10\n",
        "trials_per_eval = 600  # adjust up later for tighter accuracy\n",
        "\n",
        "# Cache results to avoid re-simulating identical (d,p) pairs\n",
        "_eval_cache = {}\n",
        "\n",
        "def eval_failure_rate(d_val, p_star, trials=trials_per_eval):\n",
        "    # round p to mill resolution to stabilize cache keys\n",
        "    p_key = round(float(p_star), 3)\n",
        "    key = (d_val, p_key, trials)\n",
        "    if key in _eval_cache:\n",
        "        return _eval_cache[key]\n",
        "    lattice = build_planar_lattice(d_val)\n",
        "    globals()[\"left_boundary\"] = [n for n in lattice.nodes() if n % d_val == 0]\n",
        "    globals()[\"right_boundary\"] = [n for n in lattice.nodes() if n % d_val == d_val-1]\n",
        "    k, n = logical_failure_count(lattice, p_key, trials=trials)\n",
        "    rate = k / n\n",
        "    _eval_cache[key] = {\"rate\": rate, \"k\": k, \"n\": n, \"p\": p_key, \"d\": d_val}\n",
        "    return _eval_cache[key]\n",
        "\n",
        "def cps_fitness(gamma):\n",
        "    d_val = gamma[\"d\"]\n",
        "    p_star = gamma[\"p_star\"]\n",
        "\n",
        "    # constraints\n",
        "    if d_val not in allowed_distances:\n",
        "        return float(\"-inf\"), {\"feasible\": False}\n",
        "    if not (0.12 <= p_star <= 0.26):\n",
        "        return float(\"-inf\"), {\"feasible\": False}\n",
        "\n",
        "    res = eval_failure_rate(d_val, p_star, trials=trials_per_eval)\n",
        "    err = abs(res[\"rate\"] - target_rate)\n",
        "\n",
        "    # Fitness: negative error (closer to target is better)\n",
        "    fitness = -err\n",
        "    metrics = {\n",
        "        \"feasible\": True,\n",
        "        \"abs_error\": err,\n",
        "        \"rate\": res[\"rate\"],\n",
        "        \"k\": res[\"k\"],\n",
        "        \"n\": res[\"n\"],\n",
        "        \"p\": res[\"p\"],\n",
        "        \"d\": res[\"d\"]\n",
        "    }\n",
        "    return fitness, metrics\n",
        "\n",
        "def mutate(gamma):\n",
        "    g = dict(gamma)\n",
        "    # small random drift in p_star; occasional jump in d\n",
        "    g[\"p_star\"] += random.uniform(-0.01, 0.01)\n",
        "    if random.random() < 0.15:\n",
        "        g[\"d\"] = random.choice(allowed_distances)\n",
        "    return g\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Evolutionary search\n",
        "# ----------------------------------------------------------------------\n",
        "pop_size = 24\n",
        "generations = 18\n",
        "retain = 8\n",
        "\n",
        "population = [{\"d\": random.choice(allowed_distances),\n",
        "               \"p_star\": random.uniform(0.14, 0.24)} for _ in range(pop_size)]\n",
        "\n",
        "evo_log = []\n",
        "\n",
        "for gen in range(generations):\n",
        "    scored = []\n",
        "    for gamma in population:\n",
        "        fit, metrics = cps_fitness(gamma)\n",
        "        scored.append((fit, gamma, metrics))\n",
        "\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "    survivors = scored[:retain]\n",
        "\n",
        "    best_fit, best_gamma, best_metrics = survivors[0]\n",
        "\n",
        "    evo_log.append({\n",
        "        \"generation\": gen,\n",
        "        \"best_fit\": best_fit,\n",
        "        \"best_gamma\": best_gamma,\n",
        "        \"best_metrics\": best_metrics\n",
        "    })\n",
        "\n",
        "    # rebuild population via mutation\n",
        "    new_pop = [s[1] for s in survivors]\n",
        "    while len(new_pop) < pop_size:\n",
        "        parent = random.choice(new_pop)\n",
        "        new_pop.append(mutate(parent))\n",
        "    population = new_pop\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Save run\n",
        "# ----------------------------------------------------------------------\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"cps_threshold_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"evolution_log.json\"), \"w\") as f:\n",
        "    json.dump(evo_log, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 10 CPS THRESHOLD OPTIMIZATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Run ID:\", run_id)\n",
        "print(\"Best gamma:\", evo_log[-1][\"best_gamma\"])\n",
        "print(\"Best metrics:\", evo_log[-1][\"best_metrics\"])\n",
        "print(\"Log saved to:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "lMy37ZekU67M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 10R — CPS THRESHOLD OPTIMIZATION (PROGRESS + SELF-CHECK REPORT)\n",
        "# Run this BELOW your current Cell 10 (do NOT re-run the old Cell 10)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "allowed_distances = [5, 7, 9, 11]\n",
        "target_rate = 0.10\n",
        "\n",
        "pop_size = 24\n",
        "generations = 18\n",
        "retain = 8\n",
        "\n",
        "# Trials per evaluation (keep moderate for CPU; increase later if needed)\n",
        "trials_per_eval = 600\n",
        "\n",
        "# Cache (avoid re-simulating repeated keys)\n",
        "_eval_cache = {}\n",
        "\n",
        "# Output paths\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "os.makedirs(ENGINE_DIR, exist_ok=True)\n",
        "\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"cps_threshold_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# LIGHTWEIGHT PROGRESS BAR\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def progress_line(i, n, prefix=\"\", width=28, extra=\"\"):\n",
        "    if n <= 0:\n",
        "        return\n",
        "    frac = min(1.0, max(0.0, i / n))\n",
        "    fill = int(round(frac * width))\n",
        "    bar = \"█\" * fill + \"░\" * (width - fill)\n",
        "    pct = int(round(frac * 100))\n",
        "    print(f\"{prefix}[{bar}] {pct:>3}% ({i}/{n}) {extra}\", end=\"\\r\")\n",
        "\n",
        "def progress_done(prefix=\"\"):\n",
        "    print(\" \" * 120, end=\"\\r\")\n",
        "    print(f\"{prefix}DONE\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# SELF-CHECK REPORT UTILITIES\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "self_check = {\n",
        "    \"run_id\": run_id,\n",
        "    \"timestamp_utc\": datetime.utcnow().isoformat(),  # ok for report; warning acceptable\n",
        "    \"checks\": [],\n",
        "    \"status\": \"UNKNOWN\"\n",
        "}\n",
        "\n",
        "def add_check(name, ok, detail):\n",
        "    self_check[\"checks\"].append({\"name\": name, \"ok\": bool(ok), \"detail\": str(detail)})\n",
        "\n",
        "def finalize_self_check():\n",
        "    self_check[\"status\"] = \"PASS\" if all(c[\"ok\"] for c in self_check[\"checks\"]) else \"FAIL\"\n",
        "    path = os.path.join(run_dir, \"self_check_report.json\")\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(self_check, f, indent=2)\n",
        "    return path\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# PRE-FLIGHT: Verify dependencies from prior cells exist\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "required_symbols = [\n",
        "    \"build_planar_lattice\",\n",
        "    \"inject_edge_noise\",\n",
        "    \"compute_vertex_syndrome\",\n",
        "    \"apply_matching\",\n",
        "    \"detect_logical_error\",\n",
        "    \"logical_failure_count\",\n",
        "]\n",
        "missing = [s for s in required_symbols if s not in globals()]\n",
        "add_check(\"required_symbols_present\", len(missing) == 0, f\"missing={missing}\")\n",
        "\n",
        "add_check(\"allowed_distances_nonempty\", len(allowed_distances) > 0, f\"{allowed_distances}\")\n",
        "add_check(\"retain_le_pop\", retain <= pop_size, f\"retain={retain}, pop_size={pop_size}\")\n",
        "add_check(\"trials_per_eval_positive\", trials_per_eval > 0, f\"{trials_per_eval}\")\n",
        "\n",
        "if missing:\n",
        "    # Hard stop with clear output; still write self-check.\n",
        "    path = finalize_self_check()\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CELL 10R ABORTED — MISSING REQUIRED SYMBOLS FROM PREVIOUS CELLS\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Missing:\", missing)\n",
        "    print(\"Self-check report:\", path)\n",
        "    print(\"=\"*70)\n",
        "    raise SystemExit\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# EVALUATION WITH CACHING\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def eval_failure_rate(d_val, p_star, trials=trials_per_eval):\n",
        "    p_key = round(float(p_star), 3)\n",
        "    key = (int(d_val), p_key, int(trials))\n",
        "    if key in _eval_cache:\n",
        "        return _eval_cache[key]\n",
        "\n",
        "    lattice = build_planar_lattice(d_val)\n",
        "    globals()[\"left_boundary\"] = [n for n in lattice.nodes() if n % d_val == 0]\n",
        "    globals()[\"right_boundary\"] = [n for n in lattice.nodes() if n % d_val == d_val - 1]\n",
        "\n",
        "    k, n = logical_failure_count(lattice, p_key, trials=trials)\n",
        "    rate = k / n\n",
        "\n",
        "    _eval_cache[key] = {\"rate\": rate, \"k\": k, \"n\": n, \"p\": p_key, \"d\": d_val}\n",
        "    return _eval_cache[key]\n",
        "\n",
        "def cps_fitness(gamma):\n",
        "    d_val = gamma[\"d\"]\n",
        "    p_star = gamma[\"p_star\"]\n",
        "\n",
        "    # constraints\n",
        "    if d_val not in allowed_distances:\n",
        "        return float(\"-inf\"), {\"feasible\": False, \"reason\": \"bad_d\"}\n",
        "    if not (0.12 <= p_star <= 0.26):\n",
        "        return float(\"-inf\"), {\"feasible\": False, \"reason\": \"p_out_of_range\"}\n",
        "\n",
        "    res = eval_failure_rate(d_val, p_star, trials=trials_per_eval)\n",
        "    err = abs(res[\"rate\"] - target_rate)\n",
        "    fitness = -err\n",
        "\n",
        "    metrics = {\n",
        "        \"feasible\": True,\n",
        "        \"abs_error\": err,\n",
        "        \"rate\": res[\"rate\"],\n",
        "        \"k\": res[\"k\"],\n",
        "        \"n\": res[\"n\"],\n",
        "        \"p\": res[\"p\"],\n",
        "        \"d\": res[\"d\"],\n",
        "    }\n",
        "    return fitness, metrics\n",
        "\n",
        "def mutate(gamma):\n",
        "    g = dict(gamma)\n",
        "    g[\"p_star\"] += random.uniform(-0.01, 0.01)\n",
        "    # clamp to keep search in-bounds\n",
        "    g[\"p_star\"] = max(0.12, min(0.26, g[\"p_star\"]))\n",
        "    if random.random() < 0.15:\n",
        "        g[\"d\"] = random.choice(allowed_distances)\n",
        "    return g\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# PROGRESS-AWARE EVOLUTION LOOP\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "total_evals = generations * pop_size\n",
        "eval_count = 0\n",
        "\n",
        "population = [{\"d\": random.choice(allowed_distances),\n",
        "               \"p_star\": random.uniform(0.14, 0.24)}\n",
        "              for _ in range(pop_size)]\n",
        "\n",
        "evo_log = []\n",
        "t_start = time.time()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 10R CPS THRESHOLD OPTIMIZATION — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"run_id={run_id} | generations={generations} | pop_size={pop_size} | trials/eval={trials_per_eval}\")\n",
        "print(f\"target_rate={target_rate} | d in {allowed_distances} | p in [0.12,0.26]\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for gen in range(generations):\n",
        "    scored = []\n",
        "\n",
        "    gen_start = time.time()\n",
        "    for idx, gamma in enumerate(population):\n",
        "        fit, metrics = cps_fitness(gamma)\n",
        "        scored.append((fit, gamma, metrics))\n",
        "\n",
        "        eval_count += 1\n",
        "        # progress line\n",
        "        extra = f\"gen={gen+1}/{generations}  best_fit={max(x[0] for x in scored):.6f}\"\n",
        "        progress_line(eval_count, total_evals, prefix=\"EVAL \", extra=extra)\n",
        "\n",
        "    # sort and select\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "    survivors = scored[:retain]\n",
        "    best_fit, best_gamma, best_metrics = survivors[0]\n",
        "\n",
        "    evo_log.append({\n",
        "        \"generation\": gen,\n",
        "        \"best_fit\": best_fit,\n",
        "        \"best_gamma\": best_gamma,\n",
        "        \"best_metrics\": best_metrics,\n",
        "        \"cache_size\": len(_eval_cache),\n",
        "        \"gen_seconds\": round(time.time() - gen_start, 3),\n",
        "    })\n",
        "\n",
        "    # print a stable per-generation line (not overwriting)\n",
        "    print(\" \" * 120, end=\"\\r\")\n",
        "    print(f\"GEN {gen+1:02d}/{generations} | best_fit={best_fit:.6f} | best={best_metrics} | cache={len(_eval_cache)}\")\n",
        "\n",
        "    # rebuild population via mutation\n",
        "    new_pop = [s[1] for s in survivors]\n",
        "    while len(new_pop) < pop_size:\n",
        "        parent = random.choice(new_pop)\n",
        "        new_pop.append(mutate(parent))\n",
        "    population = new_pop\n",
        "\n",
        "progress_done(prefix=\"EVAL \")\n",
        "\n",
        "elapsed = time.time() - t_start\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# SAVE LOGS + SELF-CHECK\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "log_path = os.path.join(run_dir, \"evolution_log.json\")\n",
        "with open(log_path, \"w\") as f:\n",
        "    json.dump(evo_log, f, indent=2)\n",
        "\n",
        "# Self-check assertions (post)\n",
        "add_check(\"evo_log_length\", len(evo_log) == generations, f\"{len(evo_log)} vs {generations}\")\n",
        "add_check(\"cache_nonempty\", len(_eval_cache) > 0, f\"cache_size={len(_eval_cache)}\")\n",
        "add_check(\"best_metrics_feasible\", bool(evo_log[-1][\"best_metrics\"].get(\"feasible\", False)), str(evo_log[-1][\"best_metrics\"]))\n",
        "add_check(\"elapsed_positive\", elapsed > 0, f\"{elapsed:.3f}s\")\n",
        "\n",
        "self_check_path = finalize_self_check()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# PRINT SUMMARY\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 10R COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Run ID:\", run_id)\n",
        "print(\"Elapsed seconds:\", round(elapsed, 3))\n",
        "print(\"Log saved to:\", run_dir)\n",
        "print(\"Evolution log:\", log_path)\n",
        "print(\"Self-check:\", self_check_path)\n",
        "print(\"Best final (gen last):\", evo_log[-1][\"best_metrics\"])\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "-HRV1MqDU8Bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea24fcd-cadb-4333-b102-b42b88295439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4151999633.py:62: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp_utc\": datetime.utcnow().isoformat(),  # ok for report; warning acceptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 10R CPS THRESHOLD OPTIMIZATION — RUNNING\n",
            "======================================================================\n",
            "run_id=20260215T021035 | generations=18 | pop_size=24 | trials/eval=600\n",
            "target_rate=0.1 | d in [5, 7, 9, 11] | p in [0.12,0.26]\n",
            "----------------------------------------------------------------------\n",
            "GEN 01/18 | best_fit=-0.008333 | best={'feasible': True, 'abs_error': 0.008333333333333345, 'rate': 0.09166666666666666, 'k': 55, 'n': 600, 'p': 0.182, 'd': 5} | cache=24\n",
            "GEN 02/18 | best_fit=-0.003333 | best={'feasible': True, 'abs_error': 0.003333333333333341, 'rate': 0.09666666666666666, 'k': 58, 'n': 600, 'p': 0.185, 'd': 9} | cache=37\n",
            "GEN 03/18 | best_fit=-0.003333 | best={'feasible': True, 'abs_error': 0.003333333333333341, 'rate': 0.09666666666666666, 'k': 58, 'n': 600, 'p': 0.185, 'd': 9} | cache=47\n",
            "GEN 04/18 | best_fit=-0.001667 | best={'feasible': True, 'abs_error': 0.0016666666666666774, 'rate': 0.09833333333333333, 'k': 59, 'n': 600, 'p': 0.194, 'd': 9} | cache=58\n",
            "GEN 05/18 | best_fit=-0.001667 | best={'feasible': True, 'abs_error': 0.0016666666666666774, 'rate': 0.09833333333333333, 'k': 59, 'n': 600, 'p': 0.194, 'd': 9} | cache=65\n",
            "GEN 06/18 | best_fit=-0.001667 | best={'feasible': True, 'abs_error': 0.0016666666666666774, 'rate': 0.09833333333333333, 'k': 59, 'n': 600, 'p': 0.194, 'd': 9} | cache=73\n",
            "GEN 07/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=80\n",
            "GEN 08/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=89\n",
            "GEN 09/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=96\n",
            "GEN 10/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=99\n",
            "GEN 11/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=103\n",
            "GEN 12/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=108\n",
            "GEN 13/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=112\n",
            "GEN 14/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=113\n",
            "GEN 15/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=115\n",
            "GEN 16/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=116\n",
            "GEN 17/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=116\n",
            "GEN 18/18 | best_fit=-0.000000 | best={'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5} | cache=122\n",
            "EVAL DONE\n",
            "======================================================================\n",
            "CELL 10R COMPLETE\n",
            "======================================================================\n",
            "Run ID: 20260215T021035\n",
            "Elapsed seconds: 418.737\n",
            "Log saved to: /content/shadowmap_cps_bootstrap/engine_runs/cps_threshold_run_20260215T021035\n",
            "Evolution log: /content/shadowmap_cps_bootstrap/engine_runs/cps_threshold_run_20260215T021035/evolution_log.json\n",
            "Self-check: /content/shadowmap_cps_bootstrap/engine_runs/cps_threshold_run_20260215T021035/self_check_report.json\n",
            "Best final (gen last): {'feasible': True, 'abs_error': 0.0, 'rate': 0.1, 'k': 60, 'n': 600, 'p': 0.198, 'd': 5}\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 11 — FIXED-d THRESHOLD CURVES + MONOTONICITY CHECK + SUMMARY REPORT\n",
        "# Run this BELOW Cell 10R\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# We saw CPS \"solve\" the iso-rate objective by picking d=5 and a favorable p\n",
        "# that hits exactly 60/600 = 0.1. That's not a threshold estimate.\n",
        "#\n",
        "# Next step (most logical): hold d fixed and compute a clean curve vs p, then\n",
        "# compare curves across d. Also verify monotonicity (failure should rise with p).\n",
        "#\n",
        "# This preserves forward options:\n",
        "# - later swap decoder, add boundaries, add distributed links\n",
        "# - same reporting structure persists\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"threshold_fixed_d_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Wilson CI (reuse if not in scope)\n",
        "# ----------------------------------------------------------------------\n",
        "def wilson_ci(k, n, z=1.96):\n",
        "    if n == 0:\n",
        "        return (0.0, 0.0)\n",
        "    phat = k / n\n",
        "    denom = 1.0 + (z*z)/n\n",
        "    center = (phat + (z*z)/(2*n)) / denom\n",
        "    half = (z * math.sqrt((phat*(1-phat) + (z*z)/(4*n)) / n)) / denom\n",
        "    lo = max(0.0, center - half)\n",
        "    hi = min(1.0, center + half)\n",
        "    return (lo, hi)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Settings\n",
        "# ----------------------------------------------------------------------\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = np.round(np.linspace(0.12, 0.26, 15), 3)  # wider range\n",
        "trials = 800  # balanced for CPU; increase later\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Run\n",
        "# ----------------------------------------------------------------------\n",
        "results = {}\n",
        "monotonicity = {}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 11 FIXED-d THRESHOLD CURVES — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "for d_val in distances:\n",
        "    lattice = build_planar_lattice(d_val)\n",
        "    globals()[\"left_boundary\"] = [n for n in lattice.nodes() if n % d_val == 0]\n",
        "    globals()[\"right_boundary\"] = [n for n in lattice.nodes() if n % d_val == d_val - 1]\n",
        "\n",
        "    results[d_val] = []\n",
        "    for i, p in enumerate(p_grid, start=1):\n",
        "        k, n = logical_failure_count(lattice, float(p), trials=trials)\n",
        "        rate = k / n\n",
        "        lo, hi = wilson_ci(k, n)\n",
        "        results[d_val].append({\"p\": float(p), \"rate\": rate, \"k\": k, \"n\": n, \"ci95\": (lo, hi)})\n",
        "\n",
        "        # simple progress line per distance\n",
        "        print(f\"d={d_val}  p={p:.3f}  rate={rate:.3f}  ({k}/{n})\")\n",
        "\n",
        "    # monotonicity check (non-decreasing within tolerance)\n",
        "    rates = [r[\"rate\"] for r in results[d_val]]\n",
        "    nondecreasing = all(rates[i] <= rates[i+1] + 1e-12 for i in range(len(rates)-1))\n",
        "    monotonicity[d_val] = {\"nondecreasing\": nondecreasing, \"rates\": rates}\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Save\n",
        "# ----------------------------------------------------------------------\n",
        "with open(os.path.join(run_dir, \"fixed_d_curves.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "with open(os.path.join(run_dir, \"monotonicity_check.json\"), \"w\") as f:\n",
        "    json.dump(monotonicity, f, indent=2)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Print compact summary table at a few probe p's\n",
        "# ----------------------------------------------------------------------\n",
        "probe_ps = [0.16, 0.18, 0.20, 0.22, 0.24]\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 11 SUMMARY (RATE @ PROBE p)\")\n",
        "print(\"=\"*70)\n",
        "print(\"probe_ps:\", probe_ps)\n",
        "print(\"-\"*70)\n",
        "\n",
        "def lookup(d_val, p):\n",
        "    # nearest neighbor in stored grid\n",
        "    rows = results[d_val]\n",
        "    best = min(rows, key=lambda r: abs(r[\"p\"] - p))\n",
        "    return best\n",
        "\n",
        "header = \"d \\\\ p | \" + \" | \".join([f\"{p:>5.2f}\" for p in probe_ps])\n",
        "print(header)\n",
        "print(\"-\"*len(header))\n",
        "for d_val in distances:\n",
        "    parts = []\n",
        "    for p in probe_ps:\n",
        "        r = lookup(d_val, p)\n",
        "        parts.append(f\"{r['rate']:.3f}\")\n",
        "    print(f\"{d_val:>4} | \" + \" | \".join([f\"{x:>5}\" for x in parts]))\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"Monotonicity (nondecreasing rates vs p):\", {d: monotonicity[d][\"nondecreasing\"] for d in distances})\n",
        "print(\"Saved to:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "1SZTmpa9U7eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5238d4f0-45d0-416a-edf6-974333da4e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 11 FIXED-d THRESHOLD CURVES — RUNNING\n",
            "======================================================================\n",
            "distances: [5, 7, 9, 11]\n",
            "p_grid: [np.float64(0.12), np.float64(0.13), np.float64(0.14), np.float64(0.15), np.float64(0.16), np.float64(0.17), np.float64(0.18), np.float64(0.19), np.float64(0.2), np.float64(0.21), np.float64(0.22), np.float64(0.23), np.float64(0.24), np.float64(0.25), np.float64(0.26)]\n",
            "trials per point: 800\n",
            "----------------------------------------------------------------------\n",
            "d=5  p=0.120  rate=0.019  (15/800)\n",
            "d=5  p=0.130  rate=0.021  (17/800)\n",
            "d=5  p=0.140  rate=0.021  (17/800)\n",
            "d=5  p=0.150  rate=0.043  (34/800)\n",
            "d=5  p=0.160  rate=0.050  (40/800)\n",
            "d=5  p=0.170  rate=0.056  (45/800)\n",
            "d=5  p=0.180  rate=0.092  (74/800)\n",
            "d=5  p=0.190  rate=0.114  (91/800)\n",
            "d=5  p=0.200  rate=0.134  (107/800)\n",
            "d=5  p=0.210  rate=0.155  (124/800)\n",
            "d=5  p=0.220  rate=0.145  (116/800)\n",
            "d=5  p=0.230  rate=0.185  (148/800)\n",
            "d=5  p=0.240  rate=0.219  (175/800)\n",
            "d=5  p=0.250  rate=0.264  (211/800)\n",
            "d=5  p=0.260  rate=0.240  (192/800)\n",
            "d=7  p=0.120  rate=0.016  (13/800)\n",
            "d=7  p=0.130  rate=0.019  (15/800)\n",
            "d=7  p=0.140  rate=0.021  (17/800)\n",
            "d=7  p=0.150  rate=0.031  (25/800)\n",
            "d=7  p=0.160  rate=0.040  (32/800)\n",
            "d=7  p=0.170  rate=0.061  (49/800)\n",
            "d=7  p=0.180  rate=0.069  (55/800)\n",
            "d=7  p=0.190  rate=0.116  (93/800)\n",
            "d=7  p=0.200  rate=0.110  (88/800)\n",
            "d=7  p=0.210  rate=0.159  (127/800)\n",
            "d=7  p=0.220  rate=0.188  (150/800)\n",
            "d=7  p=0.230  rate=0.168  (134/800)\n",
            "d=7  p=0.240  rate=0.229  (183/800)\n",
            "d=7  p=0.250  rate=0.241  (193/800)\n",
            "d=7  p=0.260  rate=0.297  (238/800)\n",
            "d=9  p=0.120  rate=0.007  (6/800)\n",
            "d=9  p=0.130  rate=0.016  (13/800)\n",
            "d=9  p=0.140  rate=0.018  (14/800)\n",
            "d=9  p=0.150  rate=0.026  (21/800)\n",
            "d=9  p=0.160  rate=0.046  (37/800)\n",
            "d=9  p=0.170  rate=0.054  (43/800)\n",
            "d=9  p=0.180  rate=0.092  (74/800)\n",
            "d=9  p=0.190  rate=0.099  (79/800)\n",
            "d=9  p=0.200  rate=0.117  (94/800)\n",
            "d=9  p=0.210  rate=0.169  (135/800)\n",
            "d=9  p=0.220  rate=0.191  (153/800)\n",
            "d=9  p=0.230  rate=0.204  (163/800)\n",
            "d=9  p=0.240  rate=0.259  (207/800)\n",
            "d=9  p=0.250  rate=0.275  (220/800)\n",
            "d=9  p=0.260  rate=0.324  (259/800)\n",
            "d=11  p=0.120  rate=0.003  (2/800)\n",
            "d=11  p=0.130  rate=0.009  (7/800)\n",
            "d=11  p=0.140  rate=0.011  (9/800)\n",
            "d=11  p=0.150  rate=0.026  (21/800)\n",
            "d=11  p=0.160  rate=0.041  (33/800)\n",
            "d=11  p=0.170  rate=0.069  (55/800)\n",
            "d=11  p=0.180  rate=0.079  (63/800)\n",
            "d=11  p=0.190  rate=0.110  (88/800)\n",
            "d=11  p=0.200  rate=0.113  (90/800)\n",
            "d=11  p=0.210  rate=0.184  (147/800)\n",
            "d=11  p=0.220  rate=0.229  (183/800)\n",
            "d=11  p=0.230  rate=0.260  (208/800)\n",
            "d=11  p=0.240  rate=0.279  (223/800)\n",
            "d=11  p=0.250  rate=0.280  (224/800)\n",
            "d=11  p=0.260  rate=0.375  (300/800)\n",
            "======================================================================\n",
            "CELL 11 SUMMARY (RATE @ PROBE p)\n",
            "======================================================================\n",
            "probe_ps: [0.16, 0.18, 0.2, 0.22, 0.24]\n",
            "----------------------------------------------------------------------\n",
            "d \\ p |  0.16 |  0.18 |  0.20 |  0.22 |  0.24\n",
            "---------------------------------------------\n",
            "   5 | 0.050 | 0.092 | 0.134 | 0.145 | 0.219\n",
            "   7 | 0.040 | 0.069 | 0.110 | 0.188 | 0.229\n",
            "   9 | 0.046 | 0.092 | 0.117 | 0.191 | 0.259\n",
            "  11 | 0.041 | 0.079 | 0.113 | 0.229 | 0.279\n",
            "----------------------------------------------------------------------\n",
            "Monotonicity (nondecreasing rates vs p): {5: False, 7: False, 9: True, 11: True}\n",
            "Saved to: /content/shadowmap_cps_bootstrap/engine_runs/threshold_fixed_d_run_20260215T022004\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 12 — MONOTONICITY REPAIR (ISOTONIC / PAV) + p* @ TARGET (INTERP) + REPORT\n",
        "# Run this BELOW Cell 11\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "# Uses 'results' from Cell 11:\n",
        "# results[d] = list of dicts with keys: p, rate, k, n, ci95\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# PAV isotonic regression (nondecreasing) on rates, weighted by n\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def isotonic_pav(x, y, w):\n",
        "    # x assumed increasing; y are rates; w are weights\n",
        "    n = len(y)\n",
        "    blocks = []\n",
        "    for i in range(n):\n",
        "        blocks.append([i, i, y[i], w[i]])  # start, end, value, weight\n",
        "        # merge while violating monotonicity\n",
        "        while len(blocks) >= 2 and blocks[-2][2] > blocks[-1][2] + 1e-15:\n",
        "            b2 = blocks.pop()\n",
        "            b1 = blocks.pop()\n",
        "            W = b1[3] + b2[3]\n",
        "            V = (b1[2]*b1[3] + b2[2]*b2[3]) / W\n",
        "            blocks.append([b1[0], b2[1], V, W])\n",
        "    # expand to full vector\n",
        "    yhat = [0.0]*n\n",
        "    for b in blocks:\n",
        "        for i in range(b[0], b[1]+1):\n",
        "            yhat[i] = b[2]\n",
        "    return yhat\n",
        "\n",
        "def estimate_p_at_target(p_list, y_list, target):\n",
        "    # linear interpolation on monotone curve; returns None if out of range\n",
        "    for i in range(len(y_list)-1):\n",
        "        y1, y2 = y_list[i], y_list[i+1]\n",
        "        p1, p2 = p_list[i], p_list[i+1]\n",
        "        if y1 == target:\n",
        "            return p1\n",
        "        if (y1 - target) * (y2 - target) < 0:\n",
        "            t = (target - y1) / (y2 - y1)\n",
        "            return p1 + t*(p2 - p1)\n",
        "    return None\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Apply monotone repair + compute p* for multiple targets\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "targets = [0.05, 0.10, 0.15]\n",
        "distances = sorted(results.keys())\n",
        "\n",
        "smoothed = {}\n",
        "pstars = {t: {} for t in targets}\n",
        "\n",
        "for d_val in distances:\n",
        "    rows = sorted(results[d_val], key=lambda r: r[\"p\"])\n",
        "    p_list = [float(r[\"p\"]) for r in rows]\n",
        "    y_list = [float(r[\"rate\"]) for r in rows]\n",
        "    w_list = [int(r[\"n\"]) for r in rows]\n",
        "\n",
        "    yhat = isotonic_pav(p_list, y_list, w_list)\n",
        "\n",
        "    smoothed[d_val] = []\n",
        "    for p, y, yh, r in zip(p_list, y_list, yhat, rows):\n",
        "        smoothed[d_val].append({\n",
        "            \"p\": p,\n",
        "            \"rate_raw\": y,\n",
        "            \"rate_iso\": yh,\n",
        "            \"k\": int(r[\"k\"]),\n",
        "            \"n\": int(r[\"n\"]),\n",
        "            \"ci95_raw\": r[\"ci95\"],\n",
        "        })\n",
        "\n",
        "    for t in targets:\n",
        "        pstars[t][d_val] = estimate_p_at_target(p_list, yhat, t)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Save artifacts\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"monotone_repair_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"smoothed_curves.json\"), \"w\") as f:\n",
        "    json.dump(smoothed, f, indent=2)\n",
        "\n",
        "with open(os.path.join(run_dir, \"pstars.json\"), \"w\") as f:\n",
        "    json.dump(pstars, f, indent=2)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Self-check: monotonicity after repair\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "mono_after = {}\n",
        "for d_val in distances:\n",
        "    yhat = [r[\"rate_iso\"] for r in smoothed[d_val]]\n",
        "    mono_after[d_val] = all(yhat[i] <= yhat[i+1] + 1e-15 for i in range(len(yhat)-1))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Print summary\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 12 MONOTONICITY REPAIR (ISOTONIC/PAV) COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Saved to:\", run_dir)\n",
        "print(\"Monotonicity after repair:\", mono_after)\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Table: raw vs isotonic at probe p's\n",
        "probe_ps = [0.16, 0.18, 0.20, 0.22, 0.24]\n",
        "print(\"RAW vs ISO @ probe p (nearest grid point):\")\n",
        "print(\"probe_ps:\", probe_ps)\n",
        "hdr = \"d \\\\ p | \" + \" | \".join([f\"{p:>5.2f}\" for p in probe_ps])\n",
        "print(hdr)\n",
        "print(\"-\"*len(hdr))\n",
        "\n",
        "def nearest(sm_rows, p):\n",
        "    return min(sm_rows, key=lambda r: abs(r[\"p\"] - p))\n",
        "\n",
        "for d_val in distances:\n",
        "    parts = []\n",
        "    for p in probe_ps:\n",
        "        r = nearest(smoothed[d_val], p)\n",
        "        parts.append(f\"{r['rate_raw']:.3f}->{r['rate_iso']:.3f}\")\n",
        "    print(f\"{d_val:>4} | \" + \" | \".join([f\"{x:>11}\" for x in parts]))\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"Estimated p* where iso-rate hits targets (linear interp):\")\n",
        "for t in targets:\n",
        "    print(f\" target={t:.2f}: \" + \", \".join([f\"d={d}:{pstars[t][d]}\" for d in distances]))\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "PpAraOENU874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616f1613-0e95-45bd-9076-c95d9517291b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 12 MONOTONICITY REPAIR (ISOTONIC/PAV) COMPLETE\n",
            "======================================================================\n",
            "Saved to: /content/shadowmap_cps_bootstrap/engine_runs/monotone_repair_run_20260215T023029\n",
            "Monotonicity after repair: {5: True, 7: True, 9: True, 11: True}\n",
            "----------------------------------------------------------------------\n",
            "RAW vs ISO @ probe p (nearest grid point):\n",
            "probe_ps: [0.16, 0.18, 0.2, 0.22, 0.24]\n",
            "d \\ p |  0.16 |  0.18 |  0.20 |  0.22 |  0.24\n",
            "---------------------------------------------\n",
            "   5 | 0.050->0.050 | 0.092->0.092 | 0.134->0.134 | 0.145->0.150 | 0.219->0.219\n",
            "   7 | 0.040->0.040 | 0.069->0.069 | 0.110->0.113 | 0.188->0.177 | 0.229->0.229\n",
            "   9 | 0.046->0.046 | 0.092->0.092 | 0.117->0.117 | 0.191->0.191 | 0.259->0.259\n",
            "  11 | 0.041->0.041 | 0.079->0.079 | 0.113->0.113 | 0.229->0.229 | 0.279->0.279\n",
            "----------------------------------------------------------------------\n",
            "Estimated p* where iso-rate hits targets (linear interp):\n",
            " target=0.05: d=5:0.16, d=7:0.16470588235294117, d=9:0.165, d=11:0.16318181818181818\n",
            " target=0.10: d=5:0.18352941176470589, d=7:0.18704225352112677, d=9:0.19066666666666668, d=11:0.1868\n",
            " target=0.15: d=5:0.21, d=7:0.2080821917808219, d=9:0.20634146341463414, d=11:0.20526315789473684\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 13 — CROSSING-POINT ESTIMATE (FINITE-SIZE SCALING STYLE)\n",
        "# Run this BELOW Cell 12\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# We will use the isotonic-smoothed curves from Cell 12's in-memory object:\n",
        "#   smoothed[d] = list of dicts with fields: p, rate_raw, rate_iso, ...\n",
        "# If smoothed isn't in scope (should be), we can load it from the saved JSON.\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Ensure 'smoothed' exists (fallback load)\n",
        "# ----------------------------------------------------------------------\n",
        "if \"smoothed\" not in globals():\n",
        "    # Try to load from the last run path printed in Cell 12 (hardcoded fallback).\n",
        "    # If you ran Cell 12 under a different run_id, update the path below.\n",
        "    path = \"/content/shadowmap_cps_bootstrap/engine_runs/monotone_repair_run_20260215T023029/smoothed_curves.json\"\n",
        "    with open(path, \"r\") as f:\n",
        "        smoothed = json.load(f)\n",
        "    # keys might be strings after JSON load\n",
        "    smoothed = {int(k): v for k, v in smoothed.items()}\n",
        "\n",
        "distances = sorted(smoothed.keys())\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Interpolate isotonic curves to a common fine p-grid\n",
        "# ----------------------------------------------------------------------\n",
        "p_min = max(min(r[\"p\"] for r in smoothed[d]) for d in distances)\n",
        "p_max = min(max(r[\"p\"] for r in smoothed[d]) for d in distances)\n",
        "p_fine = np.round(np.linspace(p_min, p_max, 121), 4)  # ~0.0012 resolution\n",
        "\n",
        "def interp_curve(d_val, p_query):\n",
        "    rows = sorted(smoothed[d_val], key=lambda r: r[\"p\"])\n",
        "    ps = [float(r[\"p\"]) for r in rows]\n",
        "    ys = [float(r[\"rate_iso\"]) for r in rows]\n",
        "    # clamp endpoints\n",
        "    if p_query <= ps[0]:\n",
        "        return ys[0]\n",
        "    if p_query >= ps[-1]:\n",
        "        return ys[-1]\n",
        "    # find interval\n",
        "    for i in range(len(ps)-1):\n",
        "        if ps[i] <= p_query <= ps[i+1]:\n",
        "            p1, p2 = ps[i], ps[i+1]\n",
        "            y1, y2 = ys[i], ys[i+1]\n",
        "            if p2 == p1:\n",
        "                return y1\n",
        "            t = (p_query - p1) / (p2 - p1)\n",
        "            return y1 + t*(y2 - y1)\n",
        "    return ys[-1]\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Crossing estimator:\n",
        "# Choose p where curves are most \"tightly clustered\" across d.\n",
        "# We use standard deviation across distances at each p.\n",
        "# ----------------------------------------------------------------------\n",
        "dispersion = []\n",
        "matrix = {}  # p -> {d: rate}\n",
        "\n",
        "for p in p_fine:\n",
        "    vals = []\n",
        "    matrix[p] = {}\n",
        "    for d in distances:\n",
        "        y = interp_curve(d, float(p))\n",
        "        matrix[p][d] = y\n",
        "        vals.append(y)\n",
        "    mean = sum(vals)/len(vals)\n",
        "    sd = math.sqrt(sum((v-mean)**2 for v in vals)/len(vals))\n",
        "    dispersion.append((sd, float(p), mean, vals))\n",
        "\n",
        "dispersion.sort(key=lambda x: x[0])\n",
        "best_sd, best_p, best_mean, best_vals = dispersion[0]\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Report + save\n",
        "# ----------------------------------------------------------------------\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"crossing_estimate_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "artifact = {\n",
        "    \"distances\": distances,\n",
        "    \"p_range\": [float(p_min), float(p_max)],\n",
        "    \"p_fine_len\": len(p_fine),\n",
        "    \"best_p\": best_p,\n",
        "    \"best_sd\": best_sd,\n",
        "    \"best_mean_rate\": best_mean,\n",
        "    \"rates_at_best_p\": {d: matrix[best_p][d] for d in distances},\n",
        "}\n",
        "\n",
        "with open(os.path.join(run_dir, \"crossing_estimate.json\"), \"w\") as f:\n",
        "    json.dump(artifact, f, indent=2)\n",
        "\n",
        "# Print a small neighborhood around best_p\n",
        "def neighborhood(center_p, window=0.01, step=0.002):\n",
        "    ps = [round(center_p + k*step, 4) for k in range(int(-window/step), int(window/step)+1)]\n",
        "    ps = [p for p in ps if p_min <= p <= p_max]\n",
        "    rows = []\n",
        "    for p in ps:\n",
        "        vals = [interp_curve(d, p) for d in distances]\n",
        "        mean = sum(vals)/len(vals)\n",
        "        sd = math.sqrt(sum((v-mean)**2 for v in vals)/len(vals))\n",
        "        rows.append((sd, p, mean, vals))\n",
        "    rows.sort(key=lambda x: x[0])\n",
        "    return rows[:10]\n",
        "\n",
        "top10 = neighborhood(best_p, window=0.012, step=0.002)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 13 CROSSING ESTIMATE (MIN DISPERSION ACROSS DISTANCES)\")\n",
        "print(\"=\"*70)\n",
        "print(\"Distances:\", distances)\n",
        "print(f\"Common p-range: [{p_min:.3f}, {p_max:.3f}]  | fine grid size={len(p_fine)}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Best p_cross ~= {best_p:.4f}\")\n",
        "print(f\"Dispersion (sd across d) = {best_sd:.6f}\")\n",
        "print(f\"Mean rate at p_cross     = {best_mean:.4f}\")\n",
        "print(\"Rates at p_cross:\", {d: round(matrix[best_p][d], 4) for d in distances})\n",
        "print(\"-\"*70)\n",
        "print(\"Top nearby candidates (lowest sd):\")\n",
        "for sd, p, mean, vals in top10:\n",
        "    rates = {distances[i]: round(vals[i], 4) for i in range(len(distances))}\n",
        "    print(f\"  p={p:.4f}  sd={sd:.6f}  mean={mean:.4f}  rates={rates}\")\n",
        "print(\"-\"*70)\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AovrOXIZJUPT",
        "outputId": "9c46fac5-f59f-4e29-a3bc-508f985f702c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 13 CROSSING ESTIMATE (MIN DISPERSION ACROSS DISTANCES)\n",
            "======================================================================\n",
            "Distances: [5, 7, 9, 11]\n",
            "Common p-range: [0.120, 0.260]  | fine grid size=121\n",
            "----------------------------------------------------------------------\n",
            "Best p_cross ~= 0.1643\n",
            "Dispersion (sd across d) = 0.001797\n",
            "Mean rate at p_cross     = 0.0511\n",
            "Rates at p_cross: {5: 0.0527, 7: 0.0491, 9: 0.0495, 11: 0.0531}\n",
            "----------------------------------------------------------------------\n",
            "Top nearby candidates (lowest sd):\n",
            "  p=0.1643  sd=0.001797  mean=0.0511  rates={5: 0.0527, 7: 0.0491, 9: 0.0495, 11: 0.0531}\n",
            "  p=0.1623  sd=0.002328  mean=0.0480  rates={5: 0.0514, 7: 0.0449, 9: 0.048, 11: 0.0476}\n",
            "  p=0.1663  sd=0.002751  mean=0.0542  rates={5: 0.0539, 7: 0.0534, 9: 0.051, 11: 0.0586}\n",
            "  p=0.1743  sd=0.003292  mean=0.0699  rates={5: 0.0718, 7: 0.0645, 9: 0.0704, 11: 0.0731}\n",
            "  p=0.1723  sd=0.003389  mean=0.0653  rates={5: 0.0646, 7: 0.063, 9: 0.0627, 11: 0.0711}\n",
            "  p=0.1603  sd=0.003761  mean=0.0448  rates={5: 0.0502, 7: 0.0406, 9: 0.0465, 11: 0.0421}\n",
            "  p=0.1583  sd=0.004149  mean=0.0422  rates={5: 0.0487, 7: 0.0385, 9: 0.0428, 11: 0.0387}\n",
            "  p=0.1683  sd=0.004294  mean=0.0573  rates={5: 0.0552, 7: 0.0576, 9: 0.0525, 11: 0.0641}\n",
            "  p=0.1563  sd=0.004527  mean=0.0396  rates={5: 0.0472, 7: 0.0368, 9: 0.0388, 11: 0.0357}\n",
            "  p=0.1543  sd=0.005079  mean=0.0371  rates={5: 0.0457, 7: 0.035, 9: 0.0348, 11: 0.0327}\n",
            "----------------------------------------------------------------------\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/crossing_estimate_run_20260215T023335\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 14 — VALIDATE CROSSING: LARGER d + TWO LOGICAL OPERATORS (LR and TB)\n",
        "# Run this BELOW Cell 13\n",
        "# ==============================================================================\n",
        "\n",
        "import os, json, math, random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import networkx as nx\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"crossing_validate_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Two logical operators on the same lattice:\n",
        "# - LR: left boundary to right boundary path\n",
        "# - TB: top boundary to bottom boundary path\n",
        "# ----------------------------------------------------------------------\n",
        "def boundaries_for(d_val):\n",
        "    # nodes are relabeled 0..d^2-1 in row-major order by build_planar_lattice\n",
        "    left = [n for n in range(d_val*d_val) if n % d_val == 0]\n",
        "    right = [n for n in range(d_val*d_val) if n % d_val == d_val-1]\n",
        "    top = [n for n in range(d_val)]                         # row 0\n",
        "    bottom = [n for n in range(d_val*(d_val-1), d_val*d_val)]  # row d-1\n",
        "    return left, right, top, bottom\n",
        "\n",
        "def detect_logical_error_lr(G, total_error_edges, left_boundary, right_boundary):\n",
        "    eg = nx.Graph()\n",
        "    eg.add_nodes_from(G.nodes())\n",
        "    eg.add_edges_from(total_error_edges)\n",
        "    for l in left_boundary:\n",
        "        for r in right_boundary:\n",
        "            if nx.has_path(eg, l, r):\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "def detect_logical_error_tb(G, total_error_edges, top_boundary, bottom_boundary):\n",
        "    eg = nx.Graph()\n",
        "    eg.add_nodes_from(G.nodes())\n",
        "    eg.add_edges_from(total_error_edges)\n",
        "    for t in top_boundary:\n",
        "        for b in bottom_boundary:\n",
        "            if nx.has_path(eg, t, b):\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "def logical_failure_count_two_ops(d_val, p_error, trials=1200):\n",
        "    G = build_planar_lattice(d_val)\n",
        "    left, right, top, bottom = boundaries_for(d_val)\n",
        "\n",
        "    fail_lr = 0\n",
        "    fail_tb = 0\n",
        "\n",
        "    for t in range(trials):\n",
        "        seed = hash((d_val, float(p_error), t)) % (2**32)\n",
        "        edge_errors = inject_edge_noise(G, float(p_error), seed)\n",
        "        syndrome = compute_vertex_syndrome(G, edge_errors)\n",
        "        defects = [n for n, v in syndrome.items() if v == 1]\n",
        "        correction = apply_matching(G, defects)\n",
        "\n",
        "        if correction is None:\n",
        "            # treat as failure for both (uncorrectable)\n",
        "            fail_lr += 1\n",
        "            fail_tb += 1\n",
        "            continue\n",
        "\n",
        "        total_error_edges = [e for e, err in edge_errors.items() if err] + correction\n",
        "        fail_lr += detect_logical_error_lr(G, total_error_edges, left, right)\n",
        "        fail_tb += detect_logical_error_tb(G, total_error_edges, top, bottom)\n",
        "\n",
        "    return {\"k_lr\": fail_lr, \"k_tb\": fail_tb, \"n\": trials,\n",
        "            \"rate_lr\": fail_lr/trials, \"rate_tb\": fail_tb/trials}\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Distances and p-grid centered around your estimated crossing ~0.164\n",
        "# ----------------------------------------------------------------------\n",
        "distances = [5, 7, 9, 11, 13, 15]\n",
        "p_grid = np.round(np.linspace(0.145, 0.185, 9), 4)  # tight window\n",
        "trials = 1200\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 14 CROSSING VALIDATION — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "results = {}\n",
        "for d_val in distances:\n",
        "    results[d_val] = {}\n",
        "    for p in p_grid:\n",
        "        out = logical_failure_count_two_ops(d_val, p, trials=trials)\n",
        "        results[d_val][float(p)] = out\n",
        "        print(f\"d={d_val:>2} p={p:.4f}  LR={out['rate_lr']:.4f}  TB={out['rate_tb']:.4f}  (n={trials})\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Crossing estimator: find p where dispersion across d is minimized, separately for LR and TB\n",
        "# ----------------------------------------------------------------------\n",
        "def dispersion_across_d(results, p, key):\n",
        "    vals = [results[d][p][key] for d in results.keys()]\n",
        "    mean = sum(vals)/len(vals)\n",
        "    sd = math.sqrt(sum((v-mean)**2 for v in vals)/len(vals))\n",
        "    return sd, mean, vals\n",
        "\n",
        "p_list = [float(p) for p in p_grid]\n",
        "disp_lr = []\n",
        "disp_tb = []\n",
        "for p in p_list:\n",
        "    sd, mean, vals = dispersion_across_d(results, p, \"rate_lr\")\n",
        "    disp_lr.append((sd, p, mean, vals))\n",
        "    sd, mean, vals = dispersion_across_d(results, p, \"rate_tb\")\n",
        "    disp_tb.append((sd, p, mean, vals))\n",
        "\n",
        "disp_lr.sort(key=lambda x: x[0])\n",
        "disp_tb.sort(key=lambda x: x[0])\n",
        "\n",
        "best_lr = disp_lr[0]\n",
        "best_tb = disp_tb[0]\n",
        "\n",
        "artifact = {\n",
        "    \"distances\": distances,\n",
        "    \"p_grid\": p_list,\n",
        "    \"trials\": trials,\n",
        "    \"best_lr\": {\"sd\": best_lr[0], \"p\": best_lr[1], \"mean\": best_lr[2],\n",
        "                \"rates_by_d\": {d: results[d][best_lr[1]][\"rate_lr\"] for d in distances}},\n",
        "    \"best_tb\": {\"sd\": best_tb[0], \"p\": best_tb[1], \"mean\": best_tb[2],\n",
        "                \"rates_by_d\": {d: results[d][best_tb[1]][\"rate_tb\"] for d in distances}},\n",
        "}\n",
        "\n",
        "with open(os.path.join(run_dir, \"crossing_validate.json\"), \"w\") as f:\n",
        "    json.dump(artifact, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 14 SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(\"Best LR dispersion:\", {\"p\": best_lr[1], \"sd\": best_lr[0], \"mean\": best_lr[2],\n",
        "                            \"rates_by_d\": artifact[\"best_lr\"][\"rates_by_d\"]})\n",
        "print(\"Best TB dispersion:\", {\"p\": best_tb[1], \"sd\": best_tb[0], \"mean\": best_tb[2],\n",
        "                            \"rates_by_d\": artifact[\"best_tb\"][\"rates_by_d\"]})\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neU7FQOaKQYi",
        "outputId": "14656369-cb37-4bd2-9d5d-e5c99f81f993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 14 CROSSING VALIDATION — RUNNING\n",
            "======================================================================\n",
            "distances: [5, 7, 9, 11, 13, 15]\n",
            "p_grid: [np.float64(0.145), np.float64(0.15), np.float64(0.155), np.float64(0.16), np.float64(0.165), np.float64(0.17), np.float64(0.175), np.float64(0.18), np.float64(0.185)]\n",
            "trials per point: 1200\n",
            "----------------------------------------------------------------------\n",
            "d= 5 p=0.1450  LR=0.0367  TB=0.0333  (n=1200)\n",
            "d= 5 p=0.1500  LR=0.0392  TB=0.0392  (n=1200)\n",
            "d= 5 p=0.1550  LR=0.0575  TB=0.0608  (n=1200)\n",
            "d= 5 p=0.1600  LR=0.0542  TB=0.0483  (n=1200)\n",
            "d= 5 p=0.1650  LR=0.0717  TB=0.0692  (n=1200)\n",
            "d= 5 p=0.1700  LR=0.0758  TB=0.0775  (n=1200)\n",
            "d= 5 p=0.1750  LR=0.0792  TB=0.0992  (n=1200)\n",
            "d= 5 p=0.1800  LR=0.0983  TB=0.0925  (n=1200)\n",
            "d= 5 p=0.1850  LR=0.0975  TB=0.1017  (n=1200)\n",
            "d= 7 p=0.1450  LR=0.0292  TB=0.0275  (n=1200)\n",
            "d= 7 p=0.1500  LR=0.0325  TB=0.0317  (n=1200)\n",
            "d= 7 p=0.1550  LR=0.0283  TB=0.0425  (n=1200)\n",
            "d= 7 p=0.1600  LR=0.0467  TB=0.0542  (n=1200)\n",
            "d= 7 p=0.1650  LR=0.0525  TB=0.0575  (n=1200)\n",
            "d= 7 p=0.1700  LR=0.0600  TB=0.0608  (n=1200)\n",
            "d= 7 p=0.1750  LR=0.0817  TB=0.0742  (n=1200)\n",
            "d= 7 p=0.1800  LR=0.0758  TB=0.0892  (n=1200)\n",
            "d= 7 p=0.1850  LR=0.0975  TB=0.0992  (n=1200)\n",
            "d= 9 p=0.1450  LR=0.0225  TB=0.0283  (n=1200)\n",
            "d= 9 p=0.1500  LR=0.0258  TB=0.0433  (n=1200)\n",
            "d= 9 p=0.1550  LR=0.0442  TB=0.0408  (n=1200)\n",
            "d= 9 p=0.1600  LR=0.0500  TB=0.0483  (n=1200)\n",
            "d= 9 p=0.1650  LR=0.0567  TB=0.0683  (n=1200)\n",
            "d= 9 p=0.1700  LR=0.0592  TB=0.0750  (n=1200)\n",
            "d= 9 p=0.1750  LR=0.0725  TB=0.0683  (n=1200)\n",
            "d= 9 p=0.1800  LR=0.0758  TB=0.0817  (n=1200)\n",
            "d= 9 p=0.1850  LR=0.0808  TB=0.0967  (n=1200)\n",
            "d=11 p=0.1450  LR=0.0175  TB=0.0183  (n=1200)\n",
            "d=11 p=0.1500  LR=0.0333  TB=0.0300  (n=1200)\n",
            "d=11 p=0.1550  LR=0.0350  TB=0.0500  (n=1200)\n",
            "d=11 p=0.1600  LR=0.0450  TB=0.0383  (n=1200)\n",
            "d=11 p=0.1650  LR=0.0592  TB=0.0542  (n=1200)\n",
            "d=11 p=0.1700  LR=0.0642  TB=0.0708  (n=1200)\n",
            "d=11 p=0.1750  LR=0.0742  TB=0.0817  (n=1200)\n",
            "d=11 p=0.1800  LR=0.0783  TB=0.1042  (n=1200)\n",
            "d=11 p=0.1850  LR=0.1000  TB=0.1050  (n=1200)\n",
            "d=13 p=0.1450  LR=0.0242  TB=0.0325  (n=1200)\n",
            "d=13 p=0.1500  LR=0.0400  TB=0.0300  (n=1200)\n",
            "d=13 p=0.1550  LR=0.0450  TB=0.0458  (n=1200)\n",
            "d=13 p=0.1600  LR=0.0483  TB=0.0592  (n=1200)\n",
            "d=13 p=0.1650  LR=0.0533  TB=0.0717  (n=1200)\n",
            "d=13 p=0.1700  LR=0.0750  TB=0.0825  (n=1200)\n",
            "d=13 p=0.1750  LR=0.0800  TB=0.0958  (n=1200)\n",
            "d=13 p=0.1800  LR=0.0925  TB=0.0933  (n=1200)\n",
            "d=13 p=0.1850  LR=0.0875  TB=0.1192  (n=1200)\n",
            "d=15 p=0.1450  LR=0.0200  TB=0.0333  (n=1200)\n",
            "d=15 p=0.1500  LR=0.0350  TB=0.0358  (n=1200)\n",
            "d=15 p=0.1550  LR=0.0408  TB=0.0442  (n=1200)\n",
            "d=15 p=0.1600  LR=0.0400  TB=0.0492  (n=1200)\n",
            "d=15 p=0.1650  LR=0.0625  TB=0.0767  (n=1200)\n",
            "d=15 p=0.1700  LR=0.0708  TB=0.0808  (n=1200)\n",
            "d=15 p=0.1750  LR=0.0800  TB=0.0967  (n=1200)\n",
            "d=15 p=0.1800  LR=0.1100  TB=0.1033  (n=1200)\n",
            "d=15 p=0.1850  LR=0.1308  TB=0.1333  (n=1200)\n",
            "======================================================================\n",
            "CELL 14 SUMMARY\n",
            "======================================================================\n",
            "Best LR dispersion: {'p': 0.175, 'sd': 0.0033592740617910625, 'mean': 0.07791666666666668, 'rates_by_d': {5: 0.07916666666666666, 7: 0.08166666666666667, 9: 0.0725, 11: 0.07416666666666667, 13: 0.08, 15: 0.08}}\n",
            "Best TB dispersion: {'p': 0.15, 'sd': 0.004976798018658034, 'mean': 0.035, 'rates_by_d': {5: 0.03916666666666667, 7: 0.03166666666666667, 9: 0.043333333333333335, 11: 0.03, 13: 0.03, 15: 0.035833333333333335}}\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/crossing_validate_run_20260215T023751\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 15 — UPGRADE MODEL: EDGE-DATA + PLAQUETTE CHECKS (Z-SYNDROME) + MWPM\n",
        "# Run this BELOW Cell 14\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Build square lattice with data on edges and plaquette checks on faces.\n",
        "# We'll use a d x d grid of vertices; plaquettes are (d-1)x(d-1) faces.\n",
        "# Data qubits live on edges between adjacent vertices.\n",
        "# A Z-check (plaquette) is the parity of its 4 boundary edges.\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def build_edge_plaquette_model(d):\n",
        "    # vertex grid\n",
        "    V = [(i, j) for i in range(d) for j in range(d)]\n",
        "    vid = {v: idx for idx, v in enumerate(V)}\n",
        "\n",
        "    # edges (undirected) between adjacent vertices\n",
        "    edges = []\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if i + 1 < d:\n",
        "                edges.append(((i, j), (i+1, j)))\n",
        "            if j + 1 < d:\n",
        "                edges.append(((i, j), (i, j+1)))\n",
        "\n",
        "    # normalize edge key\n",
        "    def ekey(a, b):\n",
        "        return tuple(sorted((a, b)))\n",
        "\n",
        "    E = [ekey(a, b) for a, b in edges]\n",
        "\n",
        "    # plaquettes (faces)\n",
        "    P = []\n",
        "    for i in range(d-1):\n",
        "        for j in range(d-1):\n",
        "            # face corners\n",
        "            v00 = (i, j)\n",
        "            v10 = (i+1, j)\n",
        "            v01 = (i, j+1)\n",
        "            v11 = (i+1, j+1)\n",
        "            # boundary edges (4)\n",
        "            pe = [\n",
        "                ekey(v00, v10),  # left vertical\n",
        "                ekey(v10, v11),  # top horizontal\n",
        "                ekey(v01, v11),  # right vertical\n",
        "                ekey(v00, v01),  # bottom horizontal\n",
        "            ]\n",
        "            P.append(((i, j), pe))\n",
        "\n",
        "    # Build adjacency graph over plaquettes: neighbors share an edge\n",
        "    PG = nx.Graph()\n",
        "    p_ids = [p[0] for p in P]\n",
        "    PG.add_nodes_from(p_ids)\n",
        "\n",
        "    # map edge->plaquettes that contain it\n",
        "    edge_to_faces = {}\n",
        "    for pid, pe in P:\n",
        "        for e in pe:\n",
        "            edge_to_faces.setdefault(e, []).append(pid)\n",
        "\n",
        "    # faces adjacent if they share any edge\n",
        "    for e, faces in edge_to_faces.items():\n",
        "        if len(faces) == 2:\n",
        "            a, b = faces\n",
        "            PG.add_edge(a, b, weight=1.0)\n",
        "\n",
        "    return {\n",
        "        \"d\": d,\n",
        "        \"V\": V,\n",
        "        \"E\": E,\n",
        "        \"P\": P,\n",
        "        \"PG\": PG,                # plaquette adjacency for matching\n",
        "        \"edge_to_faces\": edge_to_faces\n",
        "    }\n",
        "\n",
        "def inject_edge_noise_edges(E, p, seed):\n",
        "    rng = random.Random(seed)\n",
        "    return {e: (rng.random() < p) for e in E}\n",
        "\n",
        "def plaquette_syndrome(model, edge_errors):\n",
        "    # syndrome on each plaquette = XOR of its 4 edges\n",
        "    syn = {}\n",
        "    for pid, pe in model[\"P\"]:\n",
        "        s = 0\n",
        "        for e in pe:\n",
        "            if edge_errors.get(e, False):\n",
        "                s ^= 1\n",
        "        syn[pid] = s\n",
        "    return syn\n",
        "\n",
        "def mwpm_on_plaquettes(PG, defects):\n",
        "    if len(defects) % 2 != 0:\n",
        "        return None, float(\"inf\")\n",
        "    # all-pairs shortest path lengths on PG\n",
        "    spl = dict(nx.all_pairs_shortest_path_length(PG))\n",
        "    K = nx.Graph()\n",
        "    K.add_nodes_from(defects)\n",
        "    for i in range(len(defects)):\n",
        "        for j in range(i+1, len(defects)):\n",
        "            a, b = defects[i], defects[j]\n",
        "            dist = spl.get(a, {}).get(b, 10**6)\n",
        "            K.add_edge(a, b, weight=dist)\n",
        "    matching = nx.algorithms.matching.min_weight_matching(K, weight=\"weight\")\n",
        "    total = 0\n",
        "    pairs = []\n",
        "    for a, b in matching:\n",
        "        total += K[a][b][\"weight\"]\n",
        "        pairs.append((a, b))\n",
        "    return pairs, total\n",
        "\n",
        "# Correction reconstruction here will be \"graph-theoretic\" (not full path -> edges),\n",
        "# but we can still define a logical error proxy:\n",
        "# logical Z operator across left-right corresponds to an odd number of flipped edges\n",
        "# crossing a chosen cut. We'll use the vertical cut between columns c and c+1.\n",
        "\n",
        "def logical_error_proxy_lr_edgecut(model, edge_errors, cut_col=None):\n",
        "    d = model[\"d\"]\n",
        "    if cut_col is None:\n",
        "        cut_col = (d-1)//2  # middle cut\n",
        "\n",
        "    # count flipped horizontal edges that cross between (i,cut_col) and (i,cut_col+1)\n",
        "    # These are edges crossing the LR cut.\n",
        "    flips = 0\n",
        "    for i in range(d):\n",
        "        a = (i, cut_col)\n",
        "        b = (i, cut_col+1)\n",
        "        e = tuple(sorted((a, b)))\n",
        "        if e in edge_errors and edge_errors[e]:\n",
        "            flips ^= 1  # parity\n",
        "    return flips  # 1 = logical error, 0 = no logical error\n",
        "\n",
        "def trial_lr_failure(model, p, t, trials_seed):\n",
        "    seed = hash((model[\"d\"], float(p), t, trials_seed)) % (2**32)\n",
        "    edge_errors = inject_edge_noise_edges(model[\"E\"], float(p), seed)\n",
        "    syn = plaquette_syndrome(model, edge_errors)\n",
        "    defects = [pid for pid, v in syn.items() if v == 1]\n",
        "\n",
        "    # MWPM feasibility; if odd defects, treat as failure\n",
        "    pairs, _ = mwpm_on_plaquettes(model[\"PG\"], defects)\n",
        "    if pairs is None:\n",
        "        return 1\n",
        "\n",
        "    # NOTE: a full correction would flip an inferred chain; we are not reconstructing it yet.\n",
        "    # For a first \"fidelity upgrade\" step, we measure the *raw* logical error proxy parity.\n",
        "    # Next step (Cell 16) will reconstruct correction chains on the dual graph.\n",
        "    return logical_error_proxy_lr_edgecut(model, edge_errors)\n",
        "\n",
        "def estimate_lr_rate(model, p, trials=2000, trials_seed=1):\n",
        "    k = 0\n",
        "    for t in range(trials):\n",
        "        k += trial_lr_failure(model, p, t, trials_seed)\n",
        "    return k, trials, k/trials\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Run a compact sweep near the interesting region\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = np.round(np.linspace(0.145, 0.195, 11), 4)\n",
        "trials = 1500\n",
        "trials_seed = 7  # fixed seed to make runs comparable\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 15 EDGE+PLAQUETTE MODEL — LR LOGICAL PROXY SWEEP\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "out = {}\n",
        "for d in distances:\n",
        "    model = build_edge_plaquette_model(d)\n",
        "    out[d] = {}\n",
        "    for p in p_grid:\n",
        "        k, n, rate = estimate_lr_rate(model, p, trials=trials, trials_seed=trials_seed)\n",
        "        out[d][float(p)] = {\"k\": k, \"n\": n, \"rate\": rate}\n",
        "        print(f\"d={d:>2} p={p:.4f}  rate={rate:.4f}  ({k}/{n})\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RAW DICT:\", out)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "PFhcpwxySw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 16 — FINITE-SIZE SCALING: p*(d) DRIFT EXTRAPOLATION\n",
        "# Run BELOW previous cells\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Use fixed_d_curves.json from earlier run\n",
        "fixed_path = \"/content/shadowmap_cps_bootstrap/engine_runs/threshold_fixed_d_run_20260215T022004/fixed_d_curves.json\"\n",
        "\n",
        "with open(fixed_path, \"r\") as f:\n",
        "    fixed_curves = json.load(f)\n",
        "\n",
        "# Convert keys to int\n",
        "fixed_curves = {int(k): v for k, v in fixed_curves.items()}\n",
        "\n",
        "target_rate = 0.10\n",
        "\n",
        "def interpolate_p_star(curve, target):\n",
        "    curve = sorted(curve, key=lambda r: r[\"p\"])\n",
        "    for i in range(len(curve)-1):\n",
        "        r1 = curve[i][\"rate\"]\n",
        "        r2 = curve[i+1][\"rate\"]\n",
        "        p1 = curve[i][\"p\"]\n",
        "        p2 = curve[i+1][\"p\"]\n",
        "        if (r1-target)*(r2-target) < 0:\n",
        "            t = (target - r1)/(r2 - r1)\n",
        "            return p1 + t*(p2 - p1)\n",
        "    return None\n",
        "\n",
        "pstars = {}\n",
        "for d, curve in fixed_curves.items():\n",
        "    pstar = interpolate_p_star(curve, target_rate)\n",
        "    pstars[d] = pstar\n",
        "\n",
        "# Remove None values\n",
        "pstars = {d:p for d,p in pstars.items() if p is not None}\n",
        "\n",
        "ds = np.array(sorted(pstars.keys()))\n",
        "ps = np.array([pstars[d] for d in ds])\n",
        "\n",
        "# Fit p*(d) = p_c + a*(1/d)\n",
        "x = 1.0/ds\n",
        "A = np.vstack([np.ones_like(x), x]).T\n",
        "coeffs, *_ = np.linalg.lstsq(A, ps, rcond=None)\n",
        "p_c_est, a_est = coeffs\n",
        "\n",
        "# Save artifact\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"finite_size_fit_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "artifact = {\n",
        "    \"target_rate\": target_rate,\n",
        "    \"pstars\": pstars,\n",
        "    \"fit_model\": \"p*(d) = p_c + a*(1/d)\",\n",
        "    \"p_c_estimate\": float(p_c_est),\n",
        "    \"a_estimate\": float(a_est),\n",
        "}\n",
        "\n",
        "with open(os.path.join(run_dir, \"finite_size_fit.json\"), \"w\") as f:\n",
        "    json.dump(artifact, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 16 FINITE-SIZE FIT COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"p*(d):\", pstars)\n",
        "print(\"Estimated infinite-distance threshold p_c ≈\", round(p_c_est, 5))\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZOVX4X-VuPB",
        "outputId": "0bfa90af-53e0-4c21-f7f5-15fbc895b272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 16 FINITE-SIZE FIT COMPLETE\n",
            "======================================================================\n",
            "p*(d): {5: 0.18352941176470589, 7: 0.18657894736842107, 9: 0.19066666666666668, 11: 0.1868}\n",
            "Estimated infinite-distance threshold p_c ≈ 0.19305\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/finite_size_fit_run_20260215T033211\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 17 — ROBUSTNESS CHECK: p_c FROM MULTIPLE ISO-FAILURE TARGETS\n",
        "# Uses existing smoothed_curves.json from Cell 12 (no new Monte Carlo)\n",
        "# ==============================================================================\n",
        "\n",
        "import os, json, numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "\n",
        "# Path from your Cell 12 run (update if needed)\n",
        "smoothed_path = \"/content/shadowmap_cps_bootstrap/engine_runs/monotone_repair_run_20260215T023029/smoothed_curves.json\"\n",
        "\n",
        "with open(smoothed_path, \"r\") as f:\n",
        "    smoothed = json.load(f)\n",
        "smoothed = {int(k): v for k, v in smoothed.items()}\n",
        "\n",
        "targets = [0.05, 0.10, 0.15]\n",
        "distances = sorted(smoothed.keys())\n",
        "\n",
        "def interp_p_star(rows, target):\n",
        "    rows = sorted(rows, key=lambda r: r[\"p\"])\n",
        "    ps = [float(r[\"p\"]) for r in rows]\n",
        "    ys = [float(r[\"rate_iso\"]) for r in rows]\n",
        "    for i in range(len(ps)-1):\n",
        "        y1, y2 = ys[i], ys[i+1]\n",
        "        p1, p2 = ps[i], ps[i+1]\n",
        "        if (y1-target) == 0:\n",
        "            return p1\n",
        "        if (y1-target)*(y2-target) < 0:\n",
        "            t = (target - y1) / (y2 - y1)\n",
        "            return p1 + t*(p2 - p1)\n",
        "    return None\n",
        "\n",
        "def fit_pc_from_pstars(pstars):\n",
        "    ds = np.array(sorted(pstars.keys()), dtype=float)\n",
        "    ps = np.array([pstars[int(d)] for d in ds], dtype=float)\n",
        "    x = 1.0/ds\n",
        "    A = np.vstack([np.ones_like(x), x]).T\n",
        "    coeffs, *_ = np.linalg.lstsq(A, ps, rcond=None)\n",
        "    p_c, a = coeffs\n",
        "    return float(p_c), float(a)\n",
        "\n",
        "summary = {}\n",
        "for t in targets:\n",
        "    pstars = {}\n",
        "    for d in distances:\n",
        "        pstar = interp_p_star(smoothed[d], t)\n",
        "        if pstar is not None:\n",
        "            pstars[d] = pstar\n",
        "    p_c, a = fit_pc_from_pstars(pstars)\n",
        "    summary[t] = {\"pstars\": pstars, \"p_c\": p_c, \"a\": a}\n",
        "\n",
        "# Save artifact\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"pc_robustness_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "out_path = os.path.join(run_dir, \"pc_robustness.json\")\n",
        "with open(out_path, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# Print concise report\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 17 p_c ROBUSTNESS (MULTI-TARGET, FROM ISOTONIC CURVES)\")\n",
        "print(\"=\"*70)\n",
        "for t in targets:\n",
        "    print(f\"target={t:.2f}  p_c={summary[t]['p_c']:.5f}  a={summary[t]['a']:.5f}  p*(d)={summary[t]['pstars']}\")\n",
        "pcs = [summary[t][\"p_c\"] for t in targets]\n",
        "print(\"-\"*70)\n",
        "print(f\"p_c spread across targets: min={min(pcs):.5f}, max={max(pcs):.5f}, range={(max(pcs)-min(pcs)):.5f}\")\n",
        "print(\"Saved:\", out_path)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3TnLvRqVu5l",
        "outputId": "13a95228-e993-4339-ecb3-99111803b093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 17 p_c ROBUSTNESS (MULTI-TARGET, FROM ISOTONIC CURVES)\n",
            "======================================================================\n",
            "target=0.05  p_c=0.16800  a=-0.03509  p*(d)={5: 0.16, 7: 0.16470588235294117, 9: 0.165, 11: 0.16318181818181818}\n",
            "target=0.10  p_c=0.19310  a=-0.04475  p*(d)={5: 0.18352941176470589, 7: 0.18704225352112677, 9: 0.19066666666666668, 11: 0.1868}\n",
            "target=0.15  p_c=0.20153  a=0.04323  p*(d)={5: 0.21, 7: 0.2080821917808219, 9: 0.20634146341463414, 11: 0.20526315789473684}\n",
            "----------------------------------------------------------------------\n",
            "p_c spread across targets: min=0.16800, max=0.20153, range=0.03353\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/pc_robustness_run_20260215T033626/pc_robustness.json\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 18 — EDGE+PLAQUETTE MODEL WITH ACTUAL CORRECTION APPLICATION (DUAL PATHS)\n",
        "# Run BELOW your previous cells\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import os, json, math, time\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"edge_plaquette_decode_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Progress helper\n",
        "# -----------------------------\n",
        "def pbar(i, n, prefix=\"\"):\n",
        "    width = 28\n",
        "    frac = 0 if n == 0 else min(1.0, max(0.0, i/n))\n",
        "    fill = int(round(frac * width))\n",
        "    bar = \"█\"*fill + \"░\"*(width-fill)\n",
        "    print(f\"{prefix}[{bar}] {int(frac*100):>3}% ({i}/{n})\", end=\"\\r\")\n",
        "\n",
        "def pbar_done(prefix=\"\"):\n",
        "    print(\" \"*120, end=\"\\r\")\n",
        "    print(prefix + \"DONE\")\n",
        "\n",
        "# -----------------------------\n",
        "# Build edge-data / plaquette-check model\n",
        "# -----------------------------\n",
        "def build_edge_plaquette_model(d):\n",
        "    V = [(i, j) for i in range(d) for j in range(d)]\n",
        "\n",
        "    def ekey(a, b):\n",
        "        return tuple(sorted((a, b)))\n",
        "\n",
        "    # All grid edges\n",
        "    E = []\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if i + 1 < d:\n",
        "                E.append(ekey((i, j), (i+1, j)))\n",
        "            if j + 1 < d:\n",
        "                E.append(ekey((i, j), (i, j+1)))\n",
        "\n",
        "    # Plaquettes and their boundary edges\n",
        "    P = []\n",
        "    for i in range(d-1):\n",
        "        for j in range(d-1):\n",
        "            v00 = (i, j)\n",
        "            v10 = (i+1, j)\n",
        "            v01 = (i, j+1)\n",
        "            v11 = (i+1, j+1)\n",
        "            pe = [\n",
        "                ekey(v00, v10),\n",
        "                ekey(v10, v11),\n",
        "                ekey(v01, v11),\n",
        "                ekey(v00, v01),\n",
        "            ]\n",
        "            P.append(((i, j), pe))\n",
        "\n",
        "    # Edge -> faces that contain it\n",
        "    edge_to_faces = {}\n",
        "    for pid, pe in P:\n",
        "        for e in pe:\n",
        "            edge_to_faces.setdefault(e, []).append(pid)\n",
        "\n",
        "    # Dual graph over plaquettes: neighbors share an edge\n",
        "    PG = nx.Graph()\n",
        "    p_ids = [p[0] for p in P]\n",
        "    PG.add_nodes_from(p_ids)\n",
        "    for e, faces in edge_to_faces.items():\n",
        "        if len(faces) == 2:\n",
        "            a, b = faces\n",
        "            PG.add_edge(a, b, weight=1.0)\n",
        "\n",
        "    return {\"d\": d, \"V\": V, \"E\": E, \"P\": P, \"PG\": PG, \"edge_to_faces\": edge_to_faces}\n",
        "\n",
        "def inject_edge_noise(E, p, seed):\n",
        "    rng = random.Random(seed)\n",
        "    return {e: (rng.random() < p) for e in E}\n",
        "\n",
        "def plaquette_syndrome(model, edge_errors):\n",
        "    syn = {}\n",
        "    for pid, pe in model[\"P\"]:\n",
        "        s = 0\n",
        "        for e in pe:\n",
        "            if edge_errors.get(e, False):\n",
        "                s ^= 1\n",
        "        syn[pid] = s\n",
        "    return syn\n",
        "\n",
        "# -----------------------------\n",
        "# MWPM on defects in dual graph\n",
        "# -----------------------------\n",
        "def mwpm_pairs(PG, defects):\n",
        "    if len(defects) % 2 != 0:\n",
        "        return None\n",
        "\n",
        "    spl = dict(nx.all_pairs_shortest_path_length(PG))\n",
        "    K = nx.Graph()\n",
        "    K.add_nodes_from(defects)\n",
        "    for i in range(len(defects)):\n",
        "        for j in range(i+1, len(defects)):\n",
        "            a, b = defects[i], defects[j]\n",
        "            dist = spl.get(a, {}).get(b, 10**6)\n",
        "            K.add_edge(a, b, weight=dist)\n",
        "\n",
        "    matching = nx.algorithms.matching.min_weight_matching(K, weight=\"weight\")\n",
        "    return list(matching)\n",
        "\n",
        "# -----------------------------\n",
        "# Turn a dual-graph path into primal edges to flip as \"correction\"\n",
        "# Each step between adjacent plaquettes crosses one shared primal edge.\n",
        "# We flip that shared edge.\n",
        "# -----------------------------\n",
        "def shared_edge_between_faces(model, f1, f2):\n",
        "    # find the unique primal edge shared by these two faces (they must be adjacent)\n",
        "    # brute force via edge_to_faces map (fast enough for our sizes)\n",
        "    for e, faces in model[\"edge_to_faces\"].items():\n",
        "        if len(faces) == 2:\n",
        "            if (faces[0] == f1 and faces[1] == f2) or (faces[0] == f2 and faces[1] == f1):\n",
        "                return e\n",
        "    return None\n",
        "\n",
        "def correction_edges_from_pair(model, f1, f2):\n",
        "    PG = model[\"PG\"]\n",
        "    path = nx.shortest_path(PG, f1, f2)\n",
        "    flips = []\n",
        "    for i in range(len(path)-1):\n",
        "        e = shared_edge_between_faces(model, path[i], path[i+1])\n",
        "        if e is None:\n",
        "            # should not happen if PG constructed correctly\n",
        "            continue\n",
        "        flips.append(e)\n",
        "    return flips\n",
        "\n",
        "def apply_correction(model, edge_errors, pairs):\n",
        "    # Copy errors; toggle along correction edges\n",
        "    corrected = dict(edge_errors)\n",
        "    for (a, b) in pairs:\n",
        "        flips = correction_edges_from_pair(model, a, b)\n",
        "        for e in flips:\n",
        "            corrected[e] = not corrected.get(e, False)\n",
        "    return corrected\n",
        "\n",
        "# -----------------------------\n",
        "# Logical Z operator: parity of flips across a vertical cut (LR)\n",
        "# Use cut between columns c and c+1\n",
        "# -----------------------------\n",
        "def logical_parity_lr(model, edge_errors, cut_col=None):\n",
        "    d = model[\"d\"]\n",
        "    if cut_col is None:\n",
        "        cut_col = (d-1)//2\n",
        "\n",
        "    parity = 0\n",
        "    for i in range(d):\n",
        "        a = (i, cut_col)\n",
        "        b = (i, cut_col+1)\n",
        "        e = tuple(sorted((a, b)))\n",
        "        if edge_errors.get(e, False):\n",
        "            parity ^= 1\n",
        "    return parity  # 1 => logical error\n",
        "\n",
        "# -----------------------------\n",
        "# Monte Carlo estimate with actual correction\n",
        "# -----------------------------\n",
        "def lr_failure_rate_after_decode(d, p, trials=1000, seed_tag=1):\n",
        "    model = build_edge_plaquette_model(d)\n",
        "    fail = 0\n",
        "    uncorrectable = 0\n",
        "\n",
        "    for t in range(trials):\n",
        "        seed = hash((d, float(p), t, seed_tag)) % (2**32)\n",
        "        edge_err = inject_edge_noise(model[\"E\"], float(p), seed)\n",
        "        syn = plaquette_syndrome(model, edge_err)\n",
        "        defects = [pid for pid, v in syn.items() if v == 1]\n",
        "\n",
        "        pairs = mwpm_pairs(model[\"PG\"], defects)\n",
        "        if pairs is None:\n",
        "            uncorrectable += 1\n",
        "            fail += 1\n",
        "            continue\n",
        "\n",
        "        corrected = apply_correction(model, edge_err, pairs)\n",
        "        fail += logical_parity_lr(model, corrected)\n",
        "\n",
        "    return {\"rate\": fail/trials, \"uncorrectable\": uncorrectable/trials, \"k\": fail, \"n\": trials}\n",
        "\n",
        "# -----------------------------\n",
        "# Run a compact sweep (CPU-friendly) centered near your earlier bands\n",
        "# -----------------------------\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = np.round(np.linspace(0.15, 0.21, 7), 4)  # coarse; we refine next\n",
        "trials = 1200\n",
        "seed_tag = 42\n",
        "\n",
        "total_jobs = len(distances) * len(p_grid)\n",
        "job = 0\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 18 EDGE+PLAQUETTE + CORRECTION (LR) — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "results = {}\n",
        "t0 = time.time()\n",
        "\n",
        "for d in distances:\n",
        "    results[d] = {}\n",
        "    for p in p_grid:\n",
        "        job += 1\n",
        "        pbar(job, total_jobs, prefix=\"SIM \")\n",
        "        out = lr_failure_rate_after_decode(d, p, trials=trials, seed_tag=seed_tag)\n",
        "        results[d][float(p)] = out\n",
        "\n",
        "pbar_done(prefix=\"SIM \")\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "# Save\n",
        "with open(os.path.join(run_dir, \"edge_plaquette_decode_lr.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 18 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Elapsed seconds:\", round(elapsed, 3))\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"Results (d -> p -> {rate, uncorrectable, k, n}):\")\n",
        "print(results)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p4co4ahVwD8",
        "outputId": "8fc0e0bc-40fc-465c-c8c1-cdac3213d9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 18 EDGE+PLAQUETTE + CORRECTION (LR) — RUNNING\n",
            "======================================================================\n",
            "distances: [5, 7, 9, 11]\n",
            "p_grid: [np.float64(0.15), np.float64(0.16), np.float64(0.17), np.float64(0.18), np.float64(0.19), np.float64(0.2), np.float64(0.21)]\n",
            "trials per point: 1200\n",
            "----------------------------------------------------------------------\n",
            "SIM DONE\n",
            "======================================================================\n",
            "CELL 18 COMPLETE\n",
            "======================================================================\n",
            "Elapsed seconds: 113.811\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/edge_plaquette_decode_run_20260215T034244\n",
            "Results (d -> p -> {rate, uncorrectable, k, n}):\n",
            "{5: {0.15: {'rate': 0.6841666666666667, 'uncorrectable': 0.5066666666666667, 'k': 821, 'n': 1200}, 0.16: {'rate': 0.6883333333333334, 'uncorrectable': 0.48, 'k': 826, 'n': 1200}, 0.17: {'rate': 0.7025, 'uncorrectable': 0.5141666666666667, 'k': 843, 'n': 1200}, 0.18: {'rate': 0.6983333333333334, 'uncorrectable': 0.4925, 'k': 838, 'n': 1200}, 0.19: {'rate': 0.7258333333333333, 'uncorrectable': 0.505, 'k': 871, 'n': 1200}, 0.2: {'rate': 0.7016666666666667, 'uncorrectable': 0.4825, 'k': 842, 'n': 1200}, 0.21: {'rate': 0.7308333333333333, 'uncorrectable': 0.5066666666666667, 'k': 877, 'n': 1200}}, 7: {0.15: {'rate': 0.7308333333333333, 'uncorrectable': 0.5183333333333333, 'k': 877, 'n': 1200}, 0.16: {'rate': 0.6925, 'uncorrectable': 0.47583333333333333, 'k': 831, 'n': 1200}, 0.17: {'rate': 0.7175, 'uncorrectable': 0.48, 'k': 861, 'n': 1200}, 0.18: {'rate': 0.7258333333333333, 'uncorrectable': 0.5058333333333334, 'k': 871, 'n': 1200}, 0.19: {'rate': 0.7416666666666667, 'uncorrectable': 0.49833333333333335, 'k': 890, 'n': 1200}, 0.2: {'rate': 0.7458333333333333, 'uncorrectable': 0.5158333333333334, 'k': 895, 'n': 1200}, 0.21: {'rate': 0.7408333333333333, 'uncorrectable': 0.49416666666666664, 'k': 889, 'n': 1200}}, 9: {0.15: {'rate': 0.7341666666666666, 'uncorrectable': 0.5125, 'k': 881, 'n': 1200}, 0.16: {'rate': 0.7416666666666667, 'uncorrectable': 0.5133333333333333, 'k': 890, 'n': 1200}, 0.17: {'rate': 0.7558333333333334, 'uncorrectable': 0.5208333333333334, 'k': 907, 'n': 1200}, 0.18: {'rate': 0.7291666666666666, 'uncorrectable': 0.49416666666666664, 'k': 875, 'n': 1200}, 0.19: {'rate': 0.735, 'uncorrectable': 0.4816666666666667, 'k': 882, 'n': 1200}, 0.2: {'rate': 0.7416666666666667, 'uncorrectable': 0.49, 'k': 890, 'n': 1200}, 0.21: {'rate': 0.74, 'uncorrectable': 0.52, 'k': 888, 'n': 1200}}, 11: {0.15: {'rate': 0.7475, 'uncorrectable': 0.5133333333333333, 'k': 897, 'n': 1200}, 0.16: {'rate': 0.7341666666666666, 'uncorrectable': 0.5091666666666667, 'k': 881, 'n': 1200}, 0.17: {'rate': 0.7425, 'uncorrectable': 0.49416666666666664, 'k': 891, 'n': 1200}, 0.18: {'rate': 0.7325, 'uncorrectable': 0.5091666666666667, 'k': 879, 'n': 1200}, 0.19: {'rate': 0.7658333333333334, 'uncorrectable': 0.5133333333333333, 'k': 919, 'n': 1200}, 0.2: {'rate': 0.7516666666666667, 'uncorrectable': 0.5091666666666667, 'k': 902, 'n': 1200}, 0.21: {'rate': 0.7575, 'uncorrectable': 0.5133333333333333, 'k': 909, 'n': 1200}}}\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Your Cell 18 numbers diagnose the core issue: ~50% “uncorrectable” is exactly what you get when you require an even number of defects, but your planar code has open boundaries, so it’s normal to have an odd number of plaquette defects. In a real surface-code decoder, those “extra” defects are matched to a boundary (virtual node), not declared uncorrectable.\n",
        "#So the next step is to upgrade MWPM to boundary-aware matching.\n",
        "#Run this next cell under Cell 18.\n",
        "# ==============================================================================\n",
        "# CELL 19 — FIX DECODER: BOUNDARY-AWARE MWPM (ELIMINATE 50% \"UNCORRECTABLE\")\n",
        "# Run BELOW Cell 18\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import os, json, time\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"edge_plaquette_decode_boundary_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Progress helper\n",
        "# -----------------------------\n",
        "def pbar(i, n, prefix=\"\"):\n",
        "    width = 28\n",
        "    frac = 0 if n == 0 else min(1.0, max(0.0, i/n))\n",
        "    fill = int(round(frac * width))\n",
        "    bar = \"█\"*fill + \"░\"*(width-fill)\n",
        "    print(f\"{prefix}[{bar}] {int(frac*100):>3}% ({i}/{n})\", end=\"\\r\")\n",
        "\n",
        "def pbar_done(prefix=\"\"):\n",
        "    print(\" \"*120, end=\"\\r\")\n",
        "    print(prefix + \"DONE\")\n",
        "\n",
        "# -----------------------------\n",
        "# Edge-data / plaquette-check model (same as Cell 18)\n",
        "# -----------------------------\n",
        "def build_edge_plaquette_model(d):\n",
        "    V = [(i, j) for i in range(d) for j in range(d)]\n",
        "\n",
        "    def ekey(a, b):\n",
        "        return tuple(sorted((a, b)))\n",
        "\n",
        "    # All grid edges\n",
        "    E = []\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            if i + 1 < d:\n",
        "                E.append(ekey((i, j), (i+1, j)))\n",
        "            if j + 1 < d:\n",
        "                E.append(ekey((i, j), (i, j+1)))\n",
        "\n",
        "    # Plaquettes and their boundary edges\n",
        "    P = []\n",
        "    for i in range(d-1):\n",
        "        for j in range(d-1):\n",
        "            v00 = (i, j)\n",
        "            v10 = (i+1, j)\n",
        "            v01 = (i, j+1)\n",
        "            v11 = (i+1, j+1)\n",
        "            pe = [\n",
        "                ekey(v00, v10),\n",
        "                ekey(v10, v11),\n",
        "                ekey(v01, v11),\n",
        "                ekey(v00, v01),\n",
        "            ]\n",
        "            P.append(((i, j), pe))\n",
        "\n",
        "    # Edge -> faces that contain it\n",
        "    edge_to_faces = {}\n",
        "    for pid, pe in P:\n",
        "        for e in pe:\n",
        "            edge_to_faces.setdefault(e, []).append(pid)\n",
        "\n",
        "    # Dual graph over plaquettes: neighbors share an edge\n",
        "    PG = nx.Graph()\n",
        "    p_ids = [p[0] for p in P]\n",
        "    PG.add_nodes_from(p_ids)\n",
        "    for e, faces in edge_to_faces.items():\n",
        "        if len(faces) == 2:\n",
        "            a, b = faces\n",
        "            PG.add_edge(a, b, weight=1.0)\n",
        "\n",
        "    return {\"d\": d, \"V\": V, \"E\": E, \"P\": P, \"PG\": PG, \"edge_to_faces\": edge_to_faces}\n",
        "\n",
        "def inject_edge_noise(E, p, seed):\n",
        "    rng = random.Random(seed)\n",
        "    return {e: (rng.random() < p) for e in E}\n",
        "\n",
        "def plaquette_syndrome(model, edge_errors):\n",
        "    syn = {}\n",
        "    for pid, pe in model[\"P\"]:\n",
        "        s = 0\n",
        "        for e in pe:\n",
        "            if edge_errors.get(e, False):\n",
        "                s ^= 1\n",
        "        syn[pid] = s\n",
        "    return syn\n",
        "\n",
        "# -----------------------------\n",
        "# Boundary-aware MWPM on plaquette defects\n",
        "# Idea: allow pairing defects to a virtual boundary node with cost = dist to nearest boundary face.\n",
        "# This eliminates the \"odd defects => uncorrectable\" failure mode.\n",
        "# -----------------------------\n",
        "def dist_to_boundary(face, d):\n",
        "    # face is (i,j) with i,j in [0..d-2]\n",
        "    i, j = face\n",
        "    maxf = d - 2\n",
        "    return min(i, j, maxf - i, maxf - j)\n",
        "\n",
        "def mwpm_pairs_with_boundary(model, defects):\n",
        "    PG = model[\"PG\"]\n",
        "    if len(defects) == 0:\n",
        "        return []  # nothing to do\n",
        "\n",
        "    # Precompute shortest path lengths between all plaquettes (unweighted)\n",
        "    spl = dict(nx.all_pairs_shortest_path_length(PG))\n",
        "\n",
        "    # Build complete graph over defects (+ optional boundary node B)\n",
        "    K = nx.Graph()\n",
        "    for f in defects:\n",
        "        K.add_node((\"D\", f))\n",
        "\n",
        "    # defect-defect edges\n",
        "    for i in range(len(defects)):\n",
        "        for j in range(i+1, len(defects)):\n",
        "            a, b = defects[i], defects[j]\n",
        "            dist = spl.get(a, {}).get(b, 10**6)\n",
        "            K.add_edge((\"D\", a), (\"D\", b), weight=dist)\n",
        "\n",
        "    # If odd number of defects, add a single boundary node B to absorb one defect\n",
        "    # (NetworkX min_weight_matching with maxcardinality=True will then match all defect nodes.)\n",
        "    if len(defects) % 2 == 1:\n",
        "        K.add_node((\"B\", \"boundary\"))\n",
        "        for f in defects:\n",
        "            # cost to boundary ~ distance to nearest boundary face\n",
        "            K.add_edge((\"D\", f), (\"B\", \"boundary\"), weight=dist_to_boundary(f, model[\"d\"]))\n",
        "\n",
        "    matching = nx.algorithms.matching.min_weight_matching(K, weight=\"weight\", maxcardinality=True)\n",
        "\n",
        "    # Convert matching into pairs of faces or (face, boundary)\n",
        "    pairs = []\n",
        "    for u, v in matching:\n",
        "        if u[0] == \"D\" and v[0] == \"D\":\n",
        "            pairs.append((u[1], v[1]))\n",
        "        elif u[0] == \"D\" and v[0] == \"B\":\n",
        "            pairs.append((u[1], None))  # defect matched to boundary\n",
        "        elif u[0] == \"B\" and v[0] == \"D\":\n",
        "            pairs.append((v[1], None))\n",
        "        # ignore boundary-boundary (shouldn't happen with single B)\n",
        "\n",
        "    return pairs\n",
        "\n",
        "# -----------------------------\n",
        "# Turn a dual-graph path into primal edges to flip as correction\n",
        "# If paired to boundary, route to nearest boundary face via shortest path on PG to a boundary face.\n",
        "# -----------------------------\n",
        "def shared_edge_between_faces(model, f1, f2):\n",
        "    for e, faces in model[\"edge_to_faces\"].items():\n",
        "        if len(faces) == 2:\n",
        "            if (faces[0] == f1 and faces[1] == f2) or (faces[0] == f2 and faces[1] == f1):\n",
        "                return e\n",
        "    return None\n",
        "\n",
        "def correction_edges_between_faces(model, f1, f2):\n",
        "    PG = model[\"PG\"]\n",
        "    path = nx.shortest_path(PG, f1, f2)\n",
        "    flips = []\n",
        "    for i in range(len(path)-1):\n",
        "        e = shared_edge_between_faces(model, path[i], path[i+1])\n",
        "        if e is not None:\n",
        "            flips.append(e)\n",
        "    return flips\n",
        "\n",
        "def nearest_boundary_face(face, d):\n",
        "    # pick one of the closest boundary faces (ties arbitrary)\n",
        "    i, j = face\n",
        "    maxf = d - 2\n",
        "    candidates = []\n",
        "    # boundary faces have i==0 or j==0 or i==maxf or j==maxf\n",
        "    for ii in [0, maxf]:\n",
        "        candidates.append((ii, j))\n",
        "    for jj in [0, maxf]:\n",
        "        candidates.append((i, jj))\n",
        "    # choose candidate with min Manhattan distance\n",
        "    best = min(candidates, key=lambda f: abs(f[0]-i) + abs(f[1]-j))\n",
        "    return best\n",
        "\n",
        "def apply_correction_with_boundary(model, edge_errors, pairs):\n",
        "    corrected = dict(edge_errors)\n",
        "    for a, b in pairs:\n",
        "        if b is None:\n",
        "            # route defect to nearest boundary face\n",
        "            bface = nearest_boundary_face(a, model[\"d\"])\n",
        "            flips = correction_edges_between_faces(model, a, bface)\n",
        "        else:\n",
        "            flips = correction_edges_between_faces(model, a, b)\n",
        "        for e in flips:\n",
        "            corrected[e] = not corrected.get(e, False)\n",
        "    return corrected\n",
        "\n",
        "# -----------------------------\n",
        "# Logical parity (LR) after correction: vertical cut parity\n",
        "# -----------------------------\n",
        "def logical_parity_lr(model, edge_errors, cut_col=None):\n",
        "    d = model[\"d\"]\n",
        "    if cut_col is None:\n",
        "        cut_col = (d-1)//2\n",
        "    parity = 0\n",
        "    for i in range(d):\n",
        "        a = (i, cut_col)\n",
        "        b = (i, cut_col+1)\n",
        "        e = tuple(sorted((a, b)))\n",
        "        if edge_errors.get(e, False):\n",
        "            parity ^= 1\n",
        "    return parity\n",
        "\n",
        "# -----------------------------\n",
        "# Monte Carlo rate with boundary-aware decode\n",
        "# -----------------------------\n",
        "def lr_rate_after_boundary_decode(d, p, trials=800, seed_tag=1):\n",
        "    model = build_edge_plaquette_model(d)\n",
        "    fail = 0\n",
        "    boundary_matched = 0\n",
        "\n",
        "    for t in range(trials):\n",
        "        seed = hash((d, float(p), t, seed_tag)) % (2**32)\n",
        "        edge_err = inject_edge_noise(model[\"E\"], float(p), seed)\n",
        "        syn = plaquette_syndrome(model, edge_err)\n",
        "        defects = [pid for pid, v in syn.items() if v == 1]\n",
        "\n",
        "        pairs = mwpm_pairs_with_boundary(model, defects)\n",
        "        if any(b is None for (_, b) in pairs):\n",
        "            boundary_matched += 1\n",
        "\n",
        "        corrected = apply_correction_with_boundary(model, edge_err, pairs)\n",
        "        fail += logical_parity_lr(model, corrected)\n",
        "\n",
        "    return {\n",
        "        \"rate\": fail/trials,\n",
        "        \"boundary_used_frac\": boundary_matched/trials,\n",
        "        \"k\": fail,\n",
        "        \"n\": trials\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Run sweep (same p-grid as Cell 18 for comparability)\n",
        "# -----------------------------\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = np.round(np.linspace(0.15, 0.21, 7), 4)\n",
        "trials = 1200\n",
        "seed_tag = 42\n",
        "\n",
        "total_jobs = len(distances) * len(p_grid)\n",
        "job = 0\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 19 BOUNDARY-AWARE DECODE (LR) — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "results = {}\n",
        "t0 = time.time()\n",
        "\n",
        "for d in distances:\n",
        "    results[d] = {}\n",
        "    for p in p_grid:\n",
        "        job += 1\n",
        "        pbar(job, total_jobs, prefix=\"SIM \")\n",
        "        out = lr_rate_after_boundary_decode(d, p, trials=trials, seed_tag=seed_tag)\n",
        "        results[d][float(p)] = out\n",
        "\n",
        "pbar_done(prefix=\"SIM \")\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "with open(os.path.join(run_dir, \"edge_plaquette_decode_lr_boundary.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 19 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Elapsed seconds:\", round(elapsed, 3))\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"Results (d -> p -> {rate, boundary_used_frac, k, n}):\")\n",
        "print(results)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IKxtY3v0VwyJ",
        "outputId": "78bc6064-97dc-41f0-99d0-addcc91361c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 19 BOUNDARY-AWARE DECODE (LR) — RUNNING\n",
            "======================================================================\n",
            "distances: [5, 7, 9, 11]\n",
            "p_grid: [np.float64(0.15), np.float64(0.16), np.float64(0.17), np.float64(0.18), np.float64(0.19), np.float64(0.2), np.float64(0.21)]\n",
            "trials per point: 1200\n",
            "----------------------------------------------------------------------\n",
            "SIM [█░░░░░░░░░░░░░░░░░░░░░░░░░░░]   3% (1/28)\r"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "min_weight_matching() got an unexpected keyword argument 'maxcardinality'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4173999082.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mjob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SIM \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_rate_after_boundary_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_tag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4173999082.py\u001b[0m in \u001b[0;36mlr_rate_after_boundary_decode\u001b[0;34m(d, p, trials, seed_tag)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mdefects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmwpm_pairs_with_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mboundary_matched\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4173999082.py\u001b[0m in \u001b[0;36mmwpm_pairs_with_boundary\u001b[0;34m(model, defects)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"boundary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_to_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mmatching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxcardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Convert matching into pairs of faces or (face, boundary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36margmap_min_weight_matching_26\u001b[0;34m(G, weight, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/utils/backends.py\u001b[0m in \u001b[0;36m_call_if_no_backends_installed\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0;34m\"other backends.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             )\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;31m# Dispatch to backends based on inputs, `backend=` arg, or configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: min_weight_matching() got an unexpected keyword argument 'maxcardinality'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 19B — PATCH: NetworkX min_weight_matching() (no maxcardinality kwarg)\n",
        "# Then re-run the boundary-aware sweep\n",
        "# Run BELOW the errored Cell 19\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import os, json, time\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# Progress helper (reuse)\n",
        "# -----------------------------\n",
        "def pbar(i, n, prefix=\"\"):\n",
        "    width = 28\n",
        "    frac = 0 if n == 0 else min(1.0, max(0.0, i/n))\n",
        "    fill = int(round(frac * width))\n",
        "    bar = \"█\"*fill + \"░\"*(width-fill)\n",
        "    print(f\"{prefix}[{bar}] {int(frac*100):>3}% ({i}/{n})\", end=\"\\r\")\n",
        "\n",
        "def pbar_done(prefix=\"\"):\n",
        "    print(\" \"*120, end=\"\\r\")\n",
        "    print(prefix + \"DONE\")\n",
        "\n",
        "# -----------------------------\n",
        "# Patch ONLY this function: remove unsupported kwarg\n",
        "# -----------------------------\n",
        "def mwpm_pairs_with_boundary(model, defects):\n",
        "    PG = model[\"PG\"]\n",
        "    if len(defects) == 0:\n",
        "        return []\n",
        "\n",
        "    # Precompute shortest path lengths between all plaquettes (unweighted)\n",
        "    spl = dict(nx.all_pairs_shortest_path_length(PG))\n",
        "\n",
        "    K = nx.Graph()\n",
        "    for f in defects:\n",
        "        K.add_node((\"D\", f))\n",
        "\n",
        "    # defect-defect edges\n",
        "    for i in range(len(defects)):\n",
        "        for j in range(i+1, len(defects)):\n",
        "            a, b = defects[i], defects[j]\n",
        "            dist = spl.get(a, {}).get(b, 10**6)\n",
        "            K.add_edge((\"D\", a), (\"D\", b), weight=dist)\n",
        "\n",
        "    # If odd, add one boundary node to make node count even\n",
        "    if len(defects) % 2 == 1:\n",
        "        K.add_node((\"B\", \"boundary\"))\n",
        "        for f in defects:\n",
        "            K.add_edge((\"D\", f), (\"B\", \"boundary\"), weight=dist_to_boundary(f, model[\"d\"]))\n",
        "\n",
        "    # NetworkX in your env: no maxcardinality kwarg; default returns a max-cardinality MW matching\n",
        "    matching = nx.algorithms.matching.min_weight_matching(K, weight=\"weight\")\n",
        "\n",
        "    pairs = []\n",
        "    for u, v in matching:\n",
        "        if u[0] == \"D\" and v[0] == \"D\":\n",
        "            pairs.append((u[1], v[1]))\n",
        "        elif u[0] == \"D\" and v[0] == \"B\":\n",
        "            pairs.append((u[1], None))\n",
        "        elif u[0] == \"B\" and v[0] == \"D\":\n",
        "            pairs.append((v[1], None))\n",
        "    return pairs\n",
        "\n",
        "print(\"Patched mwpm_pairs_with_boundary(): removed unsupported maxcardinality kwarg.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Re-run the sweep (same settings as Cell 19)\n",
        "# -----------------------------\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"edge_plaquette_decode_boundary_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = np.round(np.linspace(0.15, 0.21, 7), 4)\n",
        "trials = 1200\n",
        "seed_tag = 42\n",
        "\n",
        "total_jobs = len(distances) * len(p_grid)\n",
        "job = 0\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 19B BOUNDARY-AWARE DECODE (LR) — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"run_id:\", run_id)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "results = {}\n",
        "t0 = time.time()\n",
        "\n",
        "for d in distances:\n",
        "    results[d] = {}\n",
        "    for p in p_grid:\n",
        "        job += 1\n",
        "        pbar(job, total_jobs, prefix=\"SIM \")\n",
        "        out = lr_rate_after_boundary_decode(d, p, trials=trials, seed_tag=seed_tag)\n",
        "        results[d][float(p)] = out\n",
        "\n",
        "pbar_done(prefix=\"SIM \")\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "with open(os.path.join(run_dir, \"edge_plaquette_decode_lr_boundary.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 19B COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Elapsed seconds:\", round(elapsed, 3))\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"Results (d -> p -> {rate, boundary_used_frac, k, n}):\")\n",
        "print(results)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqm0xevtVxcS",
        "outputId": "9d3d9742-bf92-4950-f07d-ceddba891f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched mwpm_pairs_with_boundary(): removed unsupported maxcardinality kwarg.\n",
            "======================================================================\n",
            "CELL 19B BOUNDARY-AWARE DECODE (LR) — RUNNING\n",
            "======================================================================\n",
            "run_id: 20260215T040106\n",
            "distances: [5, 7, 9, 11]\n",
            "p_grid: [np.float64(0.15), np.float64(0.16), np.float64(0.17), np.float64(0.18), np.float64(0.19), np.float64(0.2), np.float64(0.21)]\n",
            "trials per point: 1200\n",
            "----------------------------------------------------------------------\n",
            "SIM DONE\n",
            "======================================================================\n",
            "CELL 19B COMPLETE\n",
            "======================================================================\n",
            "Elapsed seconds: 268.795\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/edge_plaquette_decode_boundary_run_20260215T040106\n",
            "Results (d -> p -> {rate, boundary_used_frac, k, n}):\n",
            "{5: {0.15: {'rate': 0.36583333333333334, 'boundary_used_frac': 0.5066666666666667, 'k': 439, 'n': 1200}, 0.16: {'rate': 0.39666666666666667, 'boundary_used_frac': 0.48, 'k': 476, 'n': 1200}, 0.17: {'rate': 0.3933333333333333, 'boundary_used_frac': 0.5141666666666667, 'k': 472, 'n': 1200}, 0.18: {'rate': 0.4091666666666667, 'boundary_used_frac': 0.4925, 'k': 491, 'n': 1200}, 0.19: {'rate': 0.43083333333333335, 'boundary_used_frac': 0.505, 'k': 517, 'n': 1200}, 0.2: {'rate': 0.44083333333333335, 'boundary_used_frac': 0.4825, 'k': 529, 'n': 1200}, 0.21: {'rate': 0.44083333333333335, 'boundary_used_frac': 0.5066666666666667, 'k': 529, 'n': 1200}}, 7: {0.15: {'rate': 0.435, 'boundary_used_frac': 0.5183333333333333, 'k': 522, 'n': 1200}, 0.16: {'rate': 0.4191666666666667, 'boundary_used_frac': 0.47583333333333333, 'k': 503, 'n': 1200}, 0.17: {'rate': 0.4625, 'boundary_used_frac': 0.48, 'k': 555, 'n': 1200}, 0.18: {'rate': 0.45416666666666666, 'boundary_used_frac': 0.5058333333333334, 'k': 545, 'n': 1200}, 0.19: {'rate': 0.4925, 'boundary_used_frac': 0.49833333333333335, 'k': 591, 'n': 1200}, 0.2: {'rate': 0.47833333333333333, 'boundary_used_frac': 0.5158333333333334, 'k': 574, 'n': 1200}, 0.21: {'rate': 0.4725, 'boundary_used_frac': 0.49416666666666664, 'k': 567, 'n': 1200}}, 9: {0.15: {'rate': 0.465, 'boundary_used_frac': 0.5125, 'k': 558, 'n': 1200}, 0.16: {'rate': 0.455, 'boundary_used_frac': 0.5133333333333333, 'k': 546, 'n': 1200}, 0.17: {'rate': 0.4866666666666667, 'boundary_used_frac': 0.5208333333333334, 'k': 584, 'n': 1200}, 0.18: {'rate': 0.4575, 'boundary_used_frac': 0.49416666666666664, 'k': 549, 'n': 1200}, 0.19: {'rate': 0.48, 'boundary_used_frac': 0.4816666666666667, 'k': 576, 'n': 1200}, 0.2: {'rate': 0.5025, 'boundary_used_frac': 0.49, 'k': 603, 'n': 1200}, 0.21: {'rate': 0.48083333333333333, 'boundary_used_frac': 0.52, 'k': 577, 'n': 1200}}, 11: {0.15: {'rate': 0.4825, 'boundary_used_frac': 0.5133333333333333, 'k': 579, 'n': 1200}, 0.16: {'rate': 0.4691666666666667, 'boundary_used_frac': 0.5091666666666667, 'k': 563, 'n': 1200}, 0.17: {'rate': 0.48583333333333334, 'boundary_used_frac': 0.49416666666666664, 'k': 583, 'n': 1200}, 0.18: {'rate': 0.47, 'boundary_used_frac': 0.5091666666666667, 'k': 564, 'n': 1200}, 0.19: {'rate': 0.505, 'boundary_used_frac': 0.5133333333333333, 'k': 606, 'n': 1200}, 0.2: {'rate': 0.495, 'boundary_used_frac': 0.5091666666666667, 'k': 594, 'n': 1200}, 0.21: {'rate': 0.5075, 'boundary_used_frac': 0.5133333333333333, 'k': 609, 'n': 1200}}}\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 20 — DIAGNOSTIC: FAILURE RATE CONDITIONED ON BOUNDARY USE\n",
        "# Run BELOW Cell 19B\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import os, json, time\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"boundary_conditioned_diag_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "# We reuse functions from Cell 19/19B in-memory:\n",
        "# - build_edge_plaquette_model\n",
        "# - inject_edge_noise\n",
        "# - plaquette_syndrome\n",
        "# - mwpm_pairs_with_boundary (patched)\n",
        "# - apply_correction_with_boundary\n",
        "# - logical_parity_lr\n",
        "\n",
        "def lr_rate_conditioned(d, p, trials=1200, seed_tag=99):\n",
        "    model = build_edge_plaquette_model(d)\n",
        "    fail_b0 = 0\n",
        "    n_b0 = 0\n",
        "    fail_b1 = 0\n",
        "    n_b1 = 0\n",
        "\n",
        "    for t in range(trials):\n",
        "        seed = hash((d, float(p), t, seed_tag)) % (2**32)\n",
        "        edge_err = inject_edge_noise(model[\"E\"], float(p), seed)\n",
        "        syn = plaquette_syndrome(model, edge_err)\n",
        "        defects = [pid for pid, v in syn.items() if v == 1]\n",
        "\n",
        "        pairs = mwpm_pairs_with_boundary(model, defects)\n",
        "        used_boundary = any(b is None for (_, b) in pairs)\n",
        "\n",
        "        corrected = apply_correction_with_boundary(model, edge_err, pairs)\n",
        "        is_fail = logical_parity_lr(model, corrected)\n",
        "\n",
        "        if used_boundary:\n",
        "            n_b1 += 1\n",
        "            fail_b1 += is_fail\n",
        "        else:\n",
        "            n_b0 += 1\n",
        "            fail_b0 += is_fail\n",
        "\n",
        "    out = {\n",
        "        \"p\": float(p),\n",
        "        \"d\": int(d),\n",
        "        \"trials\": int(trials),\n",
        "        \"boundary_used_frac\": (n_b1 / trials),\n",
        "        \"rate_all\": ((fail_b0 + fail_b1) / trials),\n",
        "        \"rate_no_boundary\": (fail_b0 / n_b0) if n_b0 else None,\n",
        "        \"n_no_boundary\": int(n_b0),\n",
        "        \"rate_with_boundary\": (fail_b1 / n_b1) if n_b1 else None,\n",
        "        \"n_with_boundary\": int(n_b1),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = [0.15, 0.17, 0.19, 0.21]\n",
        "trials = 2000\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 20 CONDITIONED DIAGNOSTIC — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", p_grid)\n",
        "print(\"trials:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "results = {}\n",
        "t0 = time.time()\n",
        "for d in distances:\n",
        "    results[d] = {}\n",
        "    for p in p_grid:\n",
        "        out = lr_rate_conditioned(d, p, trials=trials, seed_tag=123)\n",
        "        results[d][p] = out\n",
        "        print(\n",
        "            f\"d={d:>2} p={p:.2f}  all={out['rate_all']:.3f}  \"\n",
        "            f\"b0={out['rate_no_boundary']:.3f} (n={out['n_no_boundary']})  \"\n",
        "            f\"b1={out['rate_with_boundary']:.3f} (n={out['n_with_boundary']})  \"\n",
        "            f\"bfrac={out['boundary_used_frac']:.3f}\"\n",
        "        )\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "with open(os.path.join(run_dir, \"conditioned_diag.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(\"Elapsed seconds:\", round(elapsed, 3))\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c43AOwXTVx9j",
        "outputId": "296a1330-ea47-4e8c-8445-30cbe6836452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 20 CONDITIONED DIAGNOSTIC — RUNNING\n",
            "======================================================================\n",
            "distances: [5, 7, 9, 11]\n",
            "p_grid: [0.15, 0.17, 0.19, 0.21]\n",
            "trials: 2000\n",
            "----------------------------------------------------------------------\n",
            "d= 5 p=0.15  all=0.382  b0=0.378 (n=1014)  b1=0.385 (n=986)  bfrac=0.493\n",
            "d= 5 p=0.17  all=0.402  b0=0.405 (n=1024)  b1=0.398 (n=976)  bfrac=0.488\n",
            "d= 5 p=0.19  all=0.445  b0=0.456 (n=1014)  b1=0.434 (n=986)  bfrac=0.493\n",
            "d= 5 p=0.21  all=0.455  b0=0.457 (n=976)  b1=0.453 (n=1024)  bfrac=0.512\n",
            "d= 7 p=0.15  all=0.399  b0=0.397 (n=1021)  b1=0.400 (n=979)  bfrac=0.489\n",
            "d= 7 p=0.17  all=0.466  b0=0.465 (n=986)  b1=0.467 (n=1014)  bfrac=0.507\n",
            "d= 7 p=0.19  all=0.475  b0=0.484 (n=974)  b1=0.467 (n=1026)  bfrac=0.513\n",
            "d= 7 p=0.21  all=0.472  b0=0.464 (n=981)  b1=0.481 (n=1019)  bfrac=0.509\n",
            "d= 9 p=0.15  all=0.441  b0=0.436 (n=995)  b1=0.445 (n=1005)  bfrac=0.502\n",
            "d= 9 p=0.17  all=0.475  b0=0.491 (n=995)  b1=0.460 (n=1005)  bfrac=0.502\n",
            "d= 9 p=0.19  all=0.495  b0=0.489 (n=999)  b1=0.500 (n=1001)  bfrac=0.500\n",
            "d= 9 p=0.21  all=0.490  b0=0.484 (n=924)  b1=0.495 (n=1076)  bfrac=0.538\n",
            "d=11 p=0.15  all=0.481  b0=0.502 (n=1009)  b1=0.460 (n=991)  bfrac=0.495\n",
            "d=11 p=0.17  all=0.500  b0=0.504 (n=954)  b1=0.496 (n=1046)  bfrac=0.523\n",
            "d=11 p=0.19  all=0.491  b0=0.484 (n=1014)  b1=0.499 (n=986)  bfrac=0.493\n",
            "d=11 p=0.21  all=0.498  b0=0.506 (n=972)  b1=0.491 (n=1028)  bfrac=0.514\n",
            "----------------------------------------------------------------------\n",
            "Elapsed seconds: 255.434\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/boundary_conditioned_diag_run_20260215T041131\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CELL 21 — FIX LOGICAL ERROR METRIC: USE RESIDUAL-CHAIN CONNECTIVITY (HOMOLOGY PROXY)\n",
        "# Run BELOW Cell 20\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import os, json, time\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"logical_metric_fix_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "# Reuse functions from Cell 19B in memory:\n",
        "# - build_edge_plaquette_model\n",
        "# - inject_edge_noise\n",
        "# - plaquette_syndrome\n",
        "# - mwpm_pairs_with_boundary (patched)\n",
        "# - apply_correction_with_boundary\n",
        "\n",
        "def vertex_boundaries_for_primal(d):\n",
        "    # vertices are (i,j), i=row, j=col, with i,j in 0..d-1\n",
        "    top = [(0, j) for j in range(d)]\n",
        "    bottom = [(d-1, j) for j in range(d)]\n",
        "    left = [(i, 0) for i in range(d)]\n",
        "    right = [(i, d-1) for i in range(d)]\n",
        "    return left, right, top, bottom\n",
        "\n",
        "def residual_error_edges(edge_err, corrected_err):\n",
        "    # residual = error XOR correction (edges where boolean differs)\n",
        "    res = []\n",
        "    for e in edge_err.keys():\n",
        "        if edge_err.get(e, False) ^ corrected_err.get(e, False):\n",
        "            res.append(e)\n",
        "    return res\n",
        "\n",
        "def has_spanning_path_primal(model, residual_edges, side=\"LR\"):\n",
        "    # Build primal graph of residual edges on vertex nodes\n",
        "    d = model[\"d\"]\n",
        "    left, right, top, bottom = vertex_boundaries_for_primal(d)\n",
        "\n",
        "    Gres = nx.Graph()\n",
        "    Gres.add_nodes_from(model[\"V\"])\n",
        "    # edges are between vertex tuples\n",
        "    Gres.add_edges_from(residual_edges)\n",
        "\n",
        "    if side == \"LR\":\n",
        "        # LR logical operator: connect LEFT boundary to RIGHT boundary\n",
        "        sources = left\n",
        "        targets = right\n",
        "    else:\n",
        "        # TB logical operator: connect TOP boundary to BOTTOM boundary\n",
        "        sources = top\n",
        "        targets = bottom\n",
        "\n",
        "    for s in sources:\n",
        "        # quick prune: if s isolated in residual graph, skip\n",
        "        if Gres.degree(s) == 0:\n",
        "            continue\n",
        "        for t in targets:\n",
        "            if Gres.degree(t) == 0:\n",
        "                continue\n",
        "            if nx.has_path(Gres, s, t):\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "def decode_and_score(model, p, t, seed_tag=7, side=\"LR\"):\n",
        "    seed = hash((model[\"d\"], float(p), t, seed_tag, side)) % (2**32)\n",
        "    edge_err = inject_edge_noise(model[\"E\"], float(p), seed)\n",
        "    syn = plaquette_syndrome(model, edge_err)\n",
        "    defects = [pid for pid, v in syn.items() if v == 1]\n",
        "\n",
        "    pairs = mwpm_pairs_with_boundary(model, defects)\n",
        "    corrected = apply_correction_with_boundary(model, edge_err, pairs)\n",
        "\n",
        "    # Residual chain after correction\n",
        "    res_edges = residual_error_edges(edge_err, corrected)\n",
        "\n",
        "    # Logical error = residual chain spans chosen boundaries\n",
        "    return has_spanning_path_primal(model, res_edges, side=side), any(b is None for (_, b) in pairs)\n",
        "\n",
        "def sweep_rates(distances, p_grid, trials=1200, side=\"LR\"):\n",
        "    out = {}\n",
        "    total = len(distances) * len(p_grid)\n",
        "    done = 0\n",
        "\n",
        "    def pbar(i, n, prefix=\"\"):\n",
        "        width = 28\n",
        "        frac = 0 if n == 0 else min(1.0, max(0.0, i/n))\n",
        "        fill = int(round(frac * width))\n",
        "        bar = \"█\"*fill + \"░\"*(width-fill)\n",
        "        print(f\"{prefix}[{bar}] {int(frac*100):>3}% ({i}/{n})\", end=\"\\r\")\n",
        "\n",
        "    def pbar_done(prefix=\"\"):\n",
        "        print(\" \"*120, end=\"\\r\")\n",
        "        print(prefix + \"DONE\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    for d in distances:\n",
        "        model = build_edge_plaquette_model(d)\n",
        "        out[d] = {}\n",
        "        for p in p_grid:\n",
        "            fail = 0\n",
        "            b_used = 0\n",
        "            for t in range(trials):\n",
        "                f, used_b = decode_and_score(model, p, t, seed_tag=42, side=side)\n",
        "                fail += f\n",
        "                b_used += 1 if used_b else 0\n",
        "            done += 1\n",
        "            pbar(done, total, prefix=f\"{side} \")\n",
        "            out[d][float(p)] = {\n",
        "                \"rate\": fail / trials,\n",
        "                \"boundary_used_frac\": b_used / trials,\n",
        "                \"k\": fail,\n",
        "                \"n\": trials\n",
        "            }\n",
        "    pbar_done(prefix=f\"{side} \")\n",
        "    elapsed = time.time() - t0\n",
        "    return out, elapsed\n",
        "\n",
        "# Settings\n",
        "distances = [5, 7, 9, 11]\n",
        "p_grid = np.round(np.linspace(0.05, 0.20, 7), 4)  # include lower p to see distance help\n",
        "trials = 1200\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 21 LOGICAL METRIC FIX — RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Run LR and TB to check anisotropy immediately\n",
        "lr_res, lr_time = sweep_rates(distances, p_grid, trials=trials, side=\"LR\")\n",
        "tb_res, tb_time = sweep_rates(distances, p_grid, trials=trials, side=\"TB\")\n",
        "\n",
        "artifact = {\n",
        "    \"distances\": distances,\n",
        "    \"p_grid\": [float(p) for p in p_grid],\n",
        "    \"trials\": trials,\n",
        "    \"LR\": lr_res,\n",
        "    \"TB\": tb_res,\n",
        "    \"elapsed_seconds\": {\"LR\": lr_time, \"TB\": tb_time}\n",
        "}\n",
        "\n",
        "with open(os.path.join(run_dir, \"logical_metric_fixed_results.json\"), \"w\") as f:\n",
        "    json.dump(artifact, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 21 COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Saved:\", run_dir)\n",
        "print(\"LR rates:\", lr_res)\n",
        "print(\"TB rates:\", tb_res)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "q2CWUVgfVymg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92bd6e92-e753-4b09-fb5e-59918ffbdf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 21 LOGICAL METRIC FIX — RUNNING\n",
            "======================================================================\n",
            "distances: [5, 7, 9, 11]\n",
            "p_grid: [np.float64(0.05), np.float64(0.075), np.float64(0.1), np.float64(0.125), np.float64(0.15), np.float64(0.175), np.float64(0.2)]\n",
            "trials per point: 1200\n",
            "----------------------------------------------------------------------\n",
            "LR DONE\n",
            "TB DONE\n",
            "======================================================================\n",
            "CELL 21 COMPLETE\n",
            "======================================================================\n",
            "Saved: /content/shadowmap_cps_bootstrap/engine_runs/logical_metric_fix_run_20260215T043616\n",
            "LR rates: {5: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.42083333333333334, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.4775, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.4658333333333333, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0016666666666666668, 'boundary_used_frac': 0.495, 'k': 2, 'n': 1200}, 0.15: {'rate': 0.005, 'boundary_used_frac': 0.5308333333333334, 'k': 6, 'n': 1200}, 0.175: {'rate': 0.0016666666666666668, 'boundary_used_frac': 0.5, 'k': 2, 'n': 1200}, 0.2: {'rate': 0.0025, 'boundary_used_frac': 0.5091666666666667, 'k': 3, 'n': 1200}}, 7: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.4475, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.47833333333333333, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.4691666666666667, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0, 'boundary_used_frac': 0.5075, 'k': 0, 'n': 1200}, 0.15: {'rate': 0.0, 'boundary_used_frac': 0.5208333333333334, 'k': 0, 'n': 1200}, 0.175: {'rate': 0.0, 'boundary_used_frac': 0.5083333333333333, 'k': 0, 'n': 1200}, 0.2: {'rate': 0.0, 'boundary_used_frac': 0.49916666666666665, 'k': 0, 'n': 1200}}, 9: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.455, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.49333333333333335, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.49166666666666664, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0, 'boundary_used_frac': 0.48583333333333334, 'k': 0, 'n': 1200}, 0.15: {'rate': 0.0, 'boundary_used_frac': 0.485, 'k': 0, 'n': 1200}, 0.175: {'rate': 0.0, 'boundary_used_frac': 0.4825, 'k': 0, 'n': 1200}, 0.2: {'rate': 0.0, 'boundary_used_frac': 0.49166666666666664, 'k': 0, 'n': 1200}}, 11: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.47583333333333333, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.4866666666666667, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.5058333333333334, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0, 'boundary_used_frac': 0.4975, 'k': 0, 'n': 1200}, 0.15: {'rate': 0.0, 'boundary_used_frac': 0.5083333333333333, 'k': 0, 'n': 1200}, 0.175: {'rate': 0.0, 'boundary_used_frac': 0.4925, 'k': 0, 'n': 1200}, 0.2: {'rate': 0.0, 'boundary_used_frac': 0.48583333333333334, 'k': 0, 'n': 1200}}}\n",
            "TB rates: {5: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.4091666666666667, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.43583333333333335, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.48, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0, 'boundary_used_frac': 0.4975, 'k': 0, 'n': 1200}, 0.15: {'rate': 0.0, 'boundary_used_frac': 0.5225, 'k': 0, 'n': 1200}, 0.175: {'rate': 0.0, 'boundary_used_frac': 0.5183333333333333, 'k': 0, 'n': 1200}, 0.2: {'rate': 0.0, 'boundary_used_frac': 0.4891666666666667, 'k': 0, 'n': 1200}}, 7: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.4625, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.4741666666666667, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.495, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0, 'boundary_used_frac': 0.5083333333333333, 'k': 0, 'n': 1200}, 0.15: {'rate': 0.0, 'boundary_used_frac': 0.4633333333333333, 'k': 0, 'n': 1200}, 0.175: {'rate': 0.0, 'boundary_used_frac': 0.4975, 'k': 0, 'n': 1200}, 0.2: {'rate': 0.0, 'boundary_used_frac': 0.4841666666666667, 'k': 0, 'n': 1200}}, 9: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.4725, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.48833333333333334, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.5091666666666667, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0, 'boundary_used_frac': 0.49083333333333334, 'k': 0, 'n': 1200}, 0.15: {'rate': 0.0, 'boundary_used_frac': 0.5158333333333334, 'k': 0, 'n': 1200}, 0.175: {'rate': 0.0, 'boundary_used_frac': 0.48333333333333334, 'k': 0, 'n': 1200}, 0.2: {'rate': 0.0, 'boundary_used_frac': 0.5033333333333333, 'k': 0, 'n': 1200}}, 11: {0.05: {'rate': 0.0, 'boundary_used_frac': 0.49416666666666664, 'k': 0, 'n': 1200}, 0.075: {'rate': 0.0, 'boundary_used_frac': 0.49916666666666665, 'k': 0, 'n': 1200}, 0.1: {'rate': 0.0, 'boundary_used_frac': 0.5083333333333333, 'k': 0, 'n': 1200}, 0.125: {'rate': 0.0, 'boundary_used_frac': 0.48583333333333334, 'k': 0, 'n': 1200}, 0.15: {'rate': 0.0, 'boundary_used_frac': 0.48583333333333334, 'k': 0, 'n': 1200}, 0.175: {'rate': 0.0, 'boundary_used_frac': 0.4925, 'k': 0, 'n': 1200}, 0.2: {'rate': 0.0, 'boundary_used_frac': 0.5066666666666667, 'k': 0, 'n': 1200}}}\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 22 — THE TOUCHDOWN: TRUE TOPOLOGICAL PLANAR CODE (EXACT MOD-2 HOMOLOGY)\n",
        "# Run BELOW Cell 21\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import os, json, time\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# EXACT O(1) ANALYTICAL LATTICE GEOMETRY\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def ekey(u, v):\n",
        "    if u == 'B': return (v, 'B')\n",
        "    if v == 'B': return (u, 'B')\n",
        "    return tuple(sorted((u, v)))\n",
        "\n",
        "def get_dist(u, v, d):\n",
        "    if u == 'B' and v == 'B': return 0\n",
        "    if u == 'B': u, v = v, u\n",
        "    if v == 'B':\n",
        "        return min(u[1] + 1, d - 1 - u[1])\n",
        "    return abs(u[0] - v[0]) + abs(u[1] - v[1])\n",
        "\n",
        "def get_correction_edges(u, v, d):\n",
        "    edges = []\n",
        "    if u == 'B' and v == 'B': return []\n",
        "    if u == 'B': u, v = v, u\n",
        "\n",
        "    r, c = u\n",
        "    if v == 'B':\n",
        "        if c + 1 <= d - 1 - c:  # Route Left\n",
        "            for col in range(c - 1, -1, -1):\n",
        "                edges.append(('H', r, col))\n",
        "            edges.append(('L', r))\n",
        "        else:                   # Route Right\n",
        "            for col in range(c, d - 2):\n",
        "                edges.append(('H', r, col))\n",
        "            edges.append(('R', r))\n",
        "        return edges\n",
        "\n",
        "    r2, c2 = v\n",
        "    step_r = 1 if r2 > r else -1\n",
        "    for curr_r in range(r, r2, step_r):\n",
        "        r_edge = curr_r if step_r == 1 else curr_r - 1\n",
        "        edges.append(('V', r_edge, c))\n",
        "\n",
        "    step_c = 1 if c2 > c else -1\n",
        "    for curr_c in range(c, c2, step_c):\n",
        "        c_edge = curr_c if step_c == 1 else curr_c - 1\n",
        "        edges.append(('H', r2, c_edge))\n",
        "\n",
        "    return edges\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# RIGOROUS PLANAR CODE DECODER (MOD-2 HOMOLOGY)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_true_surface_code(d, p, trials=1500, seed_tag=42):\n",
        "    rng = np.random.RandomState(hash((d, float(p), seed_tag)) % (2**32))\n",
        "\n",
        "    # All Data Qubits (Edges of the exact matching graph)\n",
        "    E = []\n",
        "    for r in range(d - 1):\n",
        "        for c in range(d - 1):\n",
        "            E.append(('V', r, c))\n",
        "    for r in range(d):\n",
        "        for c in range(d - 2):\n",
        "            E.append(('H', r, c))\n",
        "    for r in range(d):\n",
        "        E.append(('L', r))\n",
        "        E.append(('R', r))\n",
        "\n",
        "    # Logical Z Cut (A vertical slice down the left edge)\n",
        "    left_cut = {('L', r) for r in range(d)}\n",
        "\n",
        "    fail = 0\n",
        "    for t in range(trials):\n",
        "        # 1. Inject Noise\n",
        "        err_mask = rng.rand(len(E)) < p\n",
        "        error_edges = {E[i] for i in range(len(E)) if err_mask[i]}\n",
        "\n",
        "        # 2. Extract Syndrome (Parity of incident errors)\n",
        "        syndrome = {}\n",
        "        for edge in error_edges:\n",
        "            if edge[0] == 'V':\n",
        "                _, r, c = edge\n",
        "                syndrome[(r, c)] = syndrome.get((r, c), 0) ^ 1\n",
        "                syndrome[(r+1, c)] = syndrome.get((r+1, c), 0) ^ 1\n",
        "            elif edge[0] == 'H':\n",
        "                _, r, c = edge\n",
        "                syndrome[(r, c)] = syndrome.get((r, c), 0) ^ 1\n",
        "                syndrome[(r, c+1)] = syndrome.get((r, c+1), 0) ^ 1\n",
        "            elif edge[0] == 'L':\n",
        "                _, r = edge\n",
        "                syndrome[(r, 0)] = syndrome.get((r, 0), 0) ^ 1\n",
        "            elif edge[0] == 'R':\n",
        "                _, r = edge\n",
        "                syndrome[(r, d-2)] = syndrome.get((r, d-2), 0) ^ 1\n",
        "\n",
        "        defects = [node for node, s in syndrome.items() if s == 1]\n",
        "\n",
        "        # 3. MWPM with Perfect Virtual Shadows\n",
        "        correction_edges = set()\n",
        "        if defects:\n",
        "            K = nx.Graph()\n",
        "            for i, u in enumerate(defects):\n",
        "                shadow = f\"s_{i}\"\n",
        "                K.add_node(i)\n",
        "                K.add_node(shadow)\n",
        "                K.add_edge(i, shadow, weight=get_dist(u, 'B', d))\n",
        "\n",
        "                for j in range(i+1, len(defects)):\n",
        "                    v = defects[j]\n",
        "                    K.add_edge(i, j, weight=get_dist(u, v, d))\n",
        "                    K.add_edge(shadow, f\"s_{j}\", weight=0) # Shadows pair for free\n",
        "\n",
        "            matching = nx.algorithms.matching.min_weight_matching(K, weight=\"weight\")\n",
        "\n",
        "            # 4. Mod-2 Reconstruct\n",
        "            for a, b in matching:\n",
        "                if str(a).startswith(\"s_\") and str(b).startswith(\"s_\"): continue\n",
        "\n",
        "                node_a = 'B' if str(a).startswith(\"s_\") else defects[a]\n",
        "                node_b = 'B' if str(b).startswith(\"s_\") else defects[b]\n",
        "\n",
        "                for e in get_correction_edges(node_a, node_b, d):\n",
        "                    if e in correction_edges:\n",
        "                        correction_edges.remove(e) # Mod-2 Cancellation!\n",
        "                    else:\n",
        "                        correction_edges.add(e)\n",
        "\n",
        "        # 5. Mod-2 Homology Evaluation (Logical Parity)\n",
        "        residual = error_edges.symmetric_difference(correction_edges)\n",
        "        crossings = sum(1 for e in residual if e in left_cut)\n",
        "\n",
        "        if crossings % 2 == 1:\n",
        "            fail += 1\n",
        "\n",
        "    return fail / trials\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# THE PHASE TRANSITION SWEEP\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def pbar(i, n, prefix=\"\"):\n",
        "    frac = 0 if n == 0 else min(1.0, max(0.0, i/n))\n",
        "    fill = int(round(frac * 28))\n",
        "    bar = \"█\"*fill + \" \"*(28-fill)\n",
        "    print(f\"{prefix} [{bar}] {int(frac*100):>3}% ({i}/{n})\", end=\"\\r\")\n",
        "\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"true_topological_run_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "distances = [5, 7, 9, 11]\n",
        "# Standard Unrotated Surface Code threshold is ~10.3%.\n",
        "# We sweep perfectly over the threshold to see the absolute inversion.\n",
        "p_grid = [0.08, 0.09, 0.10, 0.11, 0.12]\n",
        "trials = 2500\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 22 - THE TOPOLOGICAL TOUCHDOWN RUNNING\")\n",
        "print(\"=\"*70)\n",
        "print(\"distances:\", distances)\n",
        "print(\"p_grid:\", list(p_grid))\n",
        "print(\"trials per point:\", trials)\n",
        "print(\"EXPECTED THRESHOLD: ~0.103\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "results = {}\n",
        "total_jobs = len(distances) * len(p_grid)\n",
        "job = 0\n",
        "t0 = time.time()\n",
        "\n",
        "for d in distances:\n",
        "    results[d] = {}\n",
        "    for p in p_grid:\n",
        "        job += 1\n",
        "        pbar(job, total_jobs, prefix=\"SIM \")\n",
        "        rate = run_true_surface_code(d, p, trials=trials, seed_tag=42)\n",
        "        results[d][float(p)] = rate\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "header = \"d \\\\ p | \" + \" | \".join([f\"{p:>6.3f}\" for p in p_grid])\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "for d in distances:\n",
        "    parts = [f\"{results[d][float(p)]:.4f}\" for p in p_grid]\n",
        "    print(f\"{d:>5} | \" + \" | \".join(parts))\n",
        "print(\"=\"*70)\n",
        "print(f\"Elapsed seconds: {elapsed:.3f}\")\n",
        "print(\"Saved to:\", run_dir)\n",
        "print(\"=\"*70)\n",
        "\n",
        "with open(os.path.join(run_dir, \"true_topological_results.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "uQx1EykNVzLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17b05a5-280a-49e4-8a5b-a261554bbdc8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 22 - THE TOPOLOGICAL TOUCHDOWN RUNNING\n",
            "======================================================================\n",
            "distances: [5, 7, 9, 11]\n",
            "p_grid: [0.08, 0.09, 0.1, 0.11, 0.12]\n",
            "trials per point: 2500\n",
            "EXPECTED THRESHOLD: ~0.103\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "d \\ p |  0.080 |  0.090 |  0.100 |  0.110 |  0.120\n",
            "--------------------------------------------------\n",
            "    5 | 0.0884 | 0.1156 | 0.1468 | 0.1812 | 0.2012\n",
            "    7 | 0.0764 | 0.0984 | 0.1424 | 0.1848 | 0.2192\n",
            "    9 | 0.0528 | 0.0960 | 0.1448 | 0.1856 | 0.2212\n",
            "   11 | 0.0520 | 0.0928 | 0.1292 | 0.1788 | 0.2560\n",
            "======================================================================\n",
            "Elapsed seconds: 226.989\n",
            "Saved to: /content/shadowmap_cps_bootstrap/engine_runs/true_topological_run_20260215T045456\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 23 — THE GRAND FINALE: TRUE CPS POLICY OPTIMIZATION ON FROZEN NOISE\n",
        "# Run BELOW Cell 22\n",
        "# ==============================================================================\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random, os, json, time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 23: HARDWARE-SOFTWARE CO-DESIGN (CPS OPTIMIZER)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. THE FROZEN WORLD (FIXED ENVIRONMENT WITH ANISOTROPIC DEFECTS)\n",
        "# ------------------------------------------------------------------------------\n",
        "d = 7\n",
        "p_V = 0.12  # 12% error on vertical links (DEFECTIVE)\n",
        "p_H = 0.04  # 4% error on horizontal/boundary links (GOOD)\n",
        "trials = 1000\n",
        "seed_crn = 42\n",
        "\n",
        "print(f\"Generating Common Random Numbers (CRN) for {trials} trials...\")\n",
        "print(f\"Hardware noise frozen at p_V={p_V}, p_H={p_H}\")\n",
        "\n",
        "rng = np.random.RandomState(seed_crn)\n",
        "E = []\n",
        "p_mask_probs = []\n",
        "\n",
        "for r in range(d - 1):\n",
        "    for c in range(d - 1):\n",
        "        E.append(('V', r, c)); p_mask_probs.append(p_V)\n",
        "for r in range(d):\n",
        "    for c in range(d - 2):\n",
        "        E.append(('H', r, c)); p_mask_probs.append(p_H)\n",
        "for r in range(d):\n",
        "    E.append(('L', r)); p_mask_probs.append(p_H)\n",
        "    E.append(('R', r)); p_mask_probs.append(p_H)\n",
        "\n",
        "p_mask_probs = np.array(p_mask_probs)\n",
        "left_cut = {('L', r) for r in range(d)}\n",
        "\n",
        "crn_dataset = []\n",
        "for t in range(trials):\n",
        "    err_mask = rng.rand(len(E)) < p_mask_probs\n",
        "    error_edges = {E[i] for i in range(len(E)) if err_mask[i]}\n",
        "\n",
        "    syndrome = {}\n",
        "    for edge in error_edges:\n",
        "        if edge[0] == 'V':\n",
        "            _, r, c = edge\n",
        "            syndrome[(r, c)] = syndrome.get((r, c), 0) ^ 1\n",
        "            syndrome[(r+1, c)] = syndrome.get((r+1, c), 0) ^ 1\n",
        "        elif edge[0] == 'H':\n",
        "            _, r, c = edge\n",
        "            syndrome[(r, c)] = syndrome.get((r, c), 0) ^ 1\n",
        "            syndrome[(r, c+1)] = syndrome.get((r, c+1), 0) ^ 1\n",
        "        elif edge[0] == 'L':\n",
        "            _, r = edge\n",
        "            syndrome[(r, 0)] = syndrome.get((r, 0), 0) ^ 1\n",
        "        elif edge[0] == 'R':\n",
        "            _, r = edge\n",
        "            syndrome[(r, d-2)] = syndrome.get((r, d-2), 0) ^ 1\n",
        "\n",
        "    defects = [node for node, s in syndrome.items() if s == 1]\n",
        "    crn_dataset.append((error_edges, defects))\n",
        "\n",
        "print(\"CRN Generation Complete. Environment Frozen.\\n\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. THE POLICY (PARAMETRIZED DECODER EVALUATION)\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_weighted_dist(u, v, d_val, w_V, w_H):\n",
        "    if u == 'B' and v == 'B': return 0\n",
        "    if u == 'B': u, v = v, u\n",
        "    if v == 'B':\n",
        "        return min(u[1] + 1, d_val - 1 - u[1]) * w_H\n",
        "    return abs(u[0] - v[0]) * w_V + abs(u[1] - v[1]) * w_H\n",
        "\n",
        "def get_correction_edges(u, v, d_val):\n",
        "    edges = []\n",
        "    if u == 'B' and v == 'B': return []\n",
        "    if u == 'B': u, v = v, u\n",
        "    r, c = u\n",
        "    if v == 'B':\n",
        "        if c + 1 <= d_val - 1 - c:\n",
        "            for col in range(c - 1, -1, -1): edges.append(('H', r, col))\n",
        "            edges.append(('L', r))\n",
        "        else:\n",
        "            for col in range(c, d_val - 2): edges.append(('H', r, col))\n",
        "            edges.append(('R', r))\n",
        "        return edges\n",
        "    r2, c2 = v\n",
        "    step_r = 1 if r2 > r else -1\n",
        "    for curr_r in range(r, r2, step_r):\n",
        "        edges.append(('V', curr_r if step_r == 1 else curr_r - 1, c))\n",
        "    step_c = 1 if c2 > c else -1\n",
        "    for curr_c in range(c, c2, step_c):\n",
        "        edges.append(('H', r2, curr_c if step_c == 1 else curr_c - 1))\n",
        "    return edges\n",
        "\n",
        "def evaluate_policy(theta):\n",
        "    w_V = max(0.01, theta['w_V'])\n",
        "    w_H = max(0.01, theta['w_H'])\n",
        "\n",
        "    fail = 0\n",
        "    for error_edges, defects in crn_dataset:\n",
        "        correction_edges = set()\n",
        "        if defects:\n",
        "            K = nx.Graph()\n",
        "            for i, u in enumerate(defects):\n",
        "                shadow = f\"s_{i}\"\n",
        "                K.add_node(i); K.add_node(shadow)\n",
        "                K.add_edge(i, shadow, weight=get_weighted_dist(u, 'B', d, w_V, w_H))\n",
        "                for j in range(i+1, len(defects)):\n",
        "                    v = defects[j]\n",
        "                    K.add_edge(i, j, weight=get_weighted_dist(u, v, d, w_V, w_H))\n",
        "                    K.add_edge(shadow, f\"s_{j}\", weight=0)\n",
        "\n",
        "            matching = nx.algorithms.matching.min_weight_matching(K, weight=\"weight\")\n",
        "\n",
        "            for a, b in matching:\n",
        "                if str(a).startswith(\"s_\") and str(b).startswith(\"s_\"): continue\n",
        "                node_a = 'B' if str(a).startswith(\"s_\") else defects[a]\n",
        "                node_b = 'B' if str(b).startswith(\"s_\") else defects[b]\n",
        "                for e in get_correction_edges(node_a, node_b, d):\n",
        "                    if e in correction_edges: correction_edges.remove(e)\n",
        "                    else: correction_edges.add(e)\n",
        "\n",
        "        residual = error_edges.symmetric_difference(correction_edges)\n",
        "        if sum(1 for e in residual if e in left_cut) % 2 == 1:\n",
        "            fail += 1\n",
        "\n",
        "    return fail / trials\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. THE CPS OPTIMIZER LOOP\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# BASELINE: The decoder doesn't know the hardware is defective (isotropic weights)\n",
        "print(\"Evaluating Naive Baseline Decoder (w_V=1.0, w_H=1.0)...\")\n",
        "baseline_rate = evaluate_policy({'w_V': 1.0, 'w_H': 1.0})\n",
        "print(f\" -> Baseline Logical Failure Rate: {baseline_rate:.4f}\\n\")\n",
        "\n",
        "pop_size = 12\n",
        "generations = 8\n",
        "retain = 4\n",
        "\n",
        "# Start with a population clustered around 1.0 to see it evolve\n",
        "population = [{'w_V': random.uniform(0.5, 1.5), 'w_H': random.uniform(0.5, 1.5)} for _ in range(pop_size)]\n",
        "\n",
        "print(\"Starting CPS Evolutionary Engine...\")\n",
        "t0 = time.time()\n",
        "best_history = []\n",
        "\n",
        "for gen in range(generations):\n",
        "    scored = []\n",
        "    for theta in population:\n",
        "        rate = evaluate_policy(theta)\n",
        "        scored.append((rate, theta))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0])\n",
        "    survivors = scored[:retain]\n",
        "\n",
        "    best_rate, best_theta = survivors[0]\n",
        "    best_history.append((best_rate, best_theta))\n",
        "\n",
        "    print(f\"GEN {gen+1:02d}/{generations} | Best Fail Rate: {best_rate:.4f} | Weights: w_V={best_theta['w_V']:.3f}, w_H={best_theta['w_H']:.3f}\")\n",
        "\n",
        "    new_pop = [s[1] for s in survivors]\n",
        "    while len(new_pop) < pop_size:\n",
        "        parent = random.choice(survivors)[1]\n",
        "        child = {\n",
        "            'w_V': max(0.01, parent['w_V'] + random.uniform(-0.2, 0.2)),\n",
        "            'w_H': max(0.01, parent['w_H'] + random.uniform(-0.2, 0.2))\n",
        "        }\n",
        "        new_pop.append(child)\n",
        "    population = new_pop\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "opt_theta = best_history[-1][1]\n",
        "\n",
        "theory_V = np.log((1 - p_V) / p_V)\n",
        "theory_H = np.log((1 - p_H) / p_H)\n",
        "theory_ratio = theory_V / theory_H\n",
        "ai_ratio = opt_theta['w_V'] / opt_theta['w_H']\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"CPS OPTIMIZATION COMPLETE IN {elapsed:.1f}s\")\n",
        "print(\"=\"*70)\n",
        "print(f\"NAIVE DECODER FAILURE RATE:  {baseline_rate:.4f} (Uniform Weights)\")\n",
        "print(f\"CPS OPTIMIZED FAILURE RATE:  {best_history[-1][0]:.4f} (Learned Hardware Asymmetry)\")\n",
        "if baseline_rate > 0:\n",
        "    print(f\"ERROR REDUCTION ACHIEVED:    {((baseline_rate - best_history[-1][0])/baseline_rate)*100:.1f}%\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"AI Discovered w_V / w_H ratio:           {ai_ratio:.3f}\")\n",
        "print(f\"Theoretical Entropy Ratio (Shannon limit): {theory_ratio:.3f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save artifact\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "run_dir = os.path.join(ENGINE_DIR, f\"cps_policy_optimization_{run_id}\")\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(run_dir, \"policy_opt.json\"), \"w\") as f:\n",
        "    json.dump({\n",
        "        \"baseline_rate\": baseline_rate,\n",
        "        \"opt_rate\": best_history[-1][0],\n",
        "        \"opt_theta\": opt_theta,\n",
        "        \"theory_ratio\": theory_ratio,\n",
        "        \"ai_ratio\": ai_ratio\n",
        "    }, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB776vsxptXX",
        "outputId": "921a9f90-324a-42e1-d856-b3c9fa734db1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CELL 23: HARDWARE-SOFTWARE CO-DESIGN (CPS OPTIMIZER)\n",
            "======================================================================\n",
            "Generating Common Random Numbers (CRN) for 1000 trials...\n",
            "Hardware noise frozen at p_V=0.12, p_H=0.04\n",
            "CRN Generation Complete. Environment Frozen.\n",
            "\n",
            "Evaluating Naive Baseline Decoder (w_V=1.0, w_H=1.0)...\n",
            " -> Baseline Logical Failure Rate: 0.0270\n",
            "\n",
            "Starting CPS Evolutionary Engine...\n",
            "GEN 01/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "GEN 02/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "GEN 03/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "GEN 04/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "GEN 05/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "GEN 06/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "GEN 07/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "GEN 08/8 | Best Fail Rate: 0.0180 | Weights: w_V=0.687, w_H=1.072\n",
            "\n",
            "======================================================================\n",
            "CPS OPTIMIZATION COMPLETE IN 108.1s\n",
            "======================================================================\n",
            "NAIVE DECODER FAILURE RATE:  0.0270 (Uniform Weights)\n",
            "CPS OPTIMIZED FAILURE RATE:  0.0180 (Learned Hardware Asymmetry)\n",
            "ERROR REDUCTION ACHIEVED:    33.3%\n",
            "----------------------------------------------------------------------\n",
            "AI Discovered w_V / w_H ratio:           0.641\n",
            "Theoretical Entropy Ratio (Shannon limit): 0.627\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def generate_directory_report(root_path):\n",
        "    report_lines = [f\"Directory Report for: {root_path}\\n\" + \"=\"*50 + \"\\n\"]\n",
        "\n",
        "    if not os.path.exists(root_path):\n",
        "        print(f\"Error: Path {root_path} does not exist.\")\n",
        "        return\n",
        "\n",
        "    for root, dirs, files in os.walk(root_path):\n",
        "        # Calculate depth for indentation\n",
        "        level = root.replace(root_path, '').count(os.sep)\n",
        "        indent = ' ' * 4 * level\n",
        "        sub_indent = ' ' * 4 * (level + 1)\n",
        "\n",
        "        # Add folder to report\n",
        "        folder_name = os.path.basename(root) if level > 0 else root\n",
        "        report_lines.append(f\"{indent}[Folder] {folder_name}/\")\n",
        "\n",
        "        # Add files to report\n",
        "        for f in sorted(files):\n",
        "            # Skip the manifest file if it already exists to avoid recursion in the text\n",
        "            if f == \"directory_manifest.txt\": continue\n",
        "            report_lines.append(f\"{sub_indent}- {f}\")\n",
        "\n",
        "    # Create the final string\n",
        "    full_report = \"\\n\".join(report_lines)\n",
        "\n",
        "    # 1. Print to console\n",
        "    print(full_report)\n",
        "\n",
        "    # 2. Save to file\n",
        "    manifest_path = os.path.join(root_path, \"directory_manifest.txt\")\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        f.write(full_report)\n",
        "\n",
        "    print(f\"\\n[Success] Manifest saved to: {manifest_path}\")\n",
        "\n",
        "# Execute\n",
        "target = \"/content/shadowmap_cps_bootstrap\"\n",
        "generate_directory_report(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x1ddPL0wf6g",
        "outputId": "d1f71c09-8a41-471a-82d9-cb2b6d56b0b8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory Report for: /content/shadowmap_cps_bootstrap\n",
            "==================================================\n",
            "\n",
            "[Folder] /content/shadowmap_cps_bootstrap/\n",
            "    - cell0_environment_report.json\n",
            "    [Folder] engine_runs/\n",
            "        [Folder] true_topological_run_20260215T045456/\n",
            "            - true_topological_results.json\n",
            "        [Folder] surface_mwpm_run_20260215T020317/\n",
            "            - evolution_log.json\n",
            "        [Folder] cps_policy_optimization_20260215T051016/\n",
            "            - policy_opt.json\n",
            "        [Folder] surface_mc_run_20260215T020217/\n",
            "            - evolution_log.json\n",
            "        [Folder] surface_run_20260215T020216/\n",
            "            - evolution_log.json\n",
            "        [Folder] graph_run_20260215T020315/\n",
            "            - evolution_log.json\n",
            "        [Folder] run_20260215T020315/\n",
            "            - evolution_log.json\n",
            "        [Folder] graph_run_20260215T020216/\n",
            "            - evolution_log.json\n",
            "        [Folder] run_20260215T020216/\n",
            "            - evolution_log.json\n",
            "        [Folder] threshold_fixed_d_run_20260215T022004/\n",
            "            - fixed_d_curves.json\n",
            "            - monotonicity_check.json\n",
            "        [Folder] cps_threshold_run_20260215T021035/\n",
            "            - evolution_log.json\n",
            "            - self_check_report.json\n",
            "        [Folder] surface_mc_run_20260215T020315/\n",
            "            - evolution_log.json\n",
            "        [Folder] surface_mwpm_run_20260215T020219/\n",
            "            - evolution_log.json\n",
            "        [Folder] surface_run_20260215T020315/\n",
            "            - evolution_log.json\n",
            "        [Folder] finite_size_fit_run_20260215T033211/\n",
            "            - finite_size_fit.json\n",
            "        [Folder] logical_metric_fix_run_20260215T043616/\n",
            "            - logical_metric_fixed_results.json\n",
            "        [Folder] monotone_repair_run_20260215T023025/\n",
            "            - pstars.json\n",
            "            - smoothed_curves.json\n",
            "        [Folder] monotone_repair_run_20260215T023029/\n",
            "            - pstars.json\n",
            "            - smoothed_curves.json\n",
            "        [Folder] crossing_estimate_run_20260215T023335/\n",
            "            - crossing_estimate.json\n",
            "        [Folder] monotone_repair_run_20260215T023027/\n",
            "            - pstars.json\n",
            "            - smoothed_curves.json\n",
            "        [Folder] edge_plaquette_decode_run_20260215T034244/\n",
            "            - edge_plaquette_decode_lr.json\n",
            "        [Folder] boundary_conditioned_diag_run_20260215T041131/\n",
            "            - conditioned_diag.json\n",
            "        [Folder] crossing_validate_run_20260215T023751/\n",
            "            - crossing_validate.json\n",
            "        [Folder] edge_plaquette_decode_boundary_run_20260215T040106/\n",
            "            - edge_plaquette_decode_lr_boundary.json\n",
            "        [Folder] edge_plaquette_decode_boundary_run_20260215T035200/\n",
            "        [Folder] pc_robustness_run_20260215T033626/\n",
            "            - pc_robustness.json\n",
            "\n",
            "[Success] Manifest saved to: /content/shadowmap_cps_bootstrap/directory_manifest.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "files utility within Colab to trigger a direct browser download. This will zip your folder locally on the VM and then push it to your computer.\n",
        "\n",
        "The \"Zip and Download\" Script\n",
        "Run this cell to compress the folder and start the download:"
      ],
      "metadata": {
        "id": "Gp7MClN-y2iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Configuration\n",
        "folder_to_zip = '/content/shadowmap_cps_bootstrap'\n",
        "output_filename = 'shadowmap_bootstrap_full_20260215.zip'\n",
        "\n",
        "# 2. Check if folder exists\n",
        "if os.path.exists(folder_to_zip):\n",
        "    print(f\"Compressing {folder_to_zip}...\")\n",
        "\n",
        "    # Create the zip file in /content/\n",
        "    shutil.make_archive(output_filename.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "\n",
        "    # 3. Trigger the browser download\n",
        "    print(f\"Starting download of {output_filename}...\")\n",
        "    files.download(output_filename)\n",
        "else:\n",
        "    print(f\"Error: {folder_to_zip} not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "r0MBMyb3wgFS",
        "outputId": "d9b1c2a6-2f64-4bd4-ca85-33564cab2b35"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressing /content/shadowmap_cps_bootstrap...\n",
            "Starting download of shadowmap_bootstrap_full_20260215.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_623f1625-d753-4af9-9fc8-f04e7507153f\", \"shadowmap_bootstrap_full_20260215.zip\", 26576)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 24 — VALIDATION PACK (CI + paired McNemar + holdout generalization)\n",
        "# Run BELOW Cell 23\n",
        "# ==============================================================================\n",
        "\n",
        "import os, json, glob, math\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from math import comb\n",
        "\n",
        "# -----------------------------\n",
        "# Utility: Wilson CI\n",
        "# -----------------------------\n",
        "def wilson_ci(k, n, z=1.96):\n",
        "    if n == 0:\n",
        "        return (0.0, 0.0)\n",
        "    phat = k / n\n",
        "    denom = 1.0 + (z*z)/n\n",
        "    center = (phat + (z*z)/(2*n)) / denom\n",
        "    half = (z * math.sqrt((phat*(1-phat) + (z*z)/(4*n)) / n)) / denom\n",
        "    lo = max(0.0, center - half)\n",
        "    hi = min(1.0, center + half)\n",
        "    return (lo, hi)\n",
        "\n",
        "# -----------------------------\n",
        "# Utility: exact McNemar p-value (two-sided)\n",
        "# -----------------------------\n",
        "def mcnemar_exact_p(n01, n10):\n",
        "    # Under H0, n01 ~ Bin(n01+n10, 0.5)\n",
        "    n = n01 + n10\n",
        "    if n == 0:\n",
        "        return 1.0\n",
        "    x = min(n01, n10)\n",
        "    p_le_x = sum(comb(n, i) * (0.5 ** n) for i in range(x + 1))\n",
        "    p_two_sided = min(1.0, 2.0 * p_le_x)\n",
        "    return p_two_sided\n",
        "\n",
        "# -----------------------------\n",
        "# Load latest optimized policy (robust to kernel resets)\n",
        "# -----------------------------\n",
        "BASE_DIR = \"/content/shadowmap_cps_bootstrap\"\n",
        "ENGINE_DIR = os.path.join(BASE_DIR, \"engine_runs\")\n",
        "policy_paths = sorted(glob.glob(os.path.join(ENGINE_DIR, \"cps_policy_optimization_*\", \"policy_opt.json\")))\n",
        "\n",
        "opt_theta = None\n",
        "if policy_paths:\n",
        "    latest_policy = policy_paths[-1]\n",
        "    with open(latest_policy, \"r\") as f:\n",
        "        saved = json.load(f)\n",
        "    opt_theta = saved.get(\"opt_theta\", None)\n",
        "else:\n",
        "    latest_policy = None\n",
        "\n",
        "# Defaults from your printed run (used if variables not in memory)\n",
        "d_default = 7\n",
        "p_V_default = 0.12\n",
        "p_H_default = 0.04\n",
        "trials_default = 1000\n",
        "seed_train_default = 42\n",
        "\n",
        "# Prefer in-memory values if present\n",
        "d = globals().get(\"d\", d_default)\n",
        "p_V = globals().get(\"p_V\", p_V_default)\n",
        "p_H = globals().get(\"p_H\", p_H_default)\n",
        "trials = globals().get(\"trials\", trials_default)\n",
        "seed_train = globals().get(\"seed_crn\", seed_train_default)\n",
        "\n",
        "# If opt_theta not found, fall back to the printed weights you pasted\n",
        "if opt_theta is None:\n",
        "    opt_theta = {\"w_V\": 0.687, \"w_H\": 1.072}\n",
        "\n",
        "baseline_theta = {\"w_V\": 1.0, \"w_H\": 1.0}\n",
        "theory_theta = {\n",
        "    \"w_V\": math.log((1.0 - p_V) / p_V),\n",
        "    \"w_H\": math.log((1.0 - p_H) / p_H),\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Re-implement the exact Cell 23 world + evaluation\n",
        "# -----------------------------\n",
        "def build_edge_list_and_probs(d_val, pV, pH):\n",
        "    E = []\n",
        "    probs = []\n",
        "\n",
        "    # Vertical links\n",
        "    for r in range(d_val - 1):\n",
        "        for c in range(d_val - 1):\n",
        "            E.append((\"V\", r, c)); probs.append(pV)\n",
        "\n",
        "    # Horizontal links\n",
        "    for r in range(d_val):\n",
        "        for c in range(d_val - 2):\n",
        "            E.append((\"H\", r, c)); probs.append(pH)\n",
        "\n",
        "    # Boundary links (treated like \"horizontal/boundary\" in your cell)\n",
        "    for r in range(d_val):\n",
        "        E.append((\"L\", r)); probs.append(pH)\n",
        "        E.append((\"R\", r)); probs.append(pH)\n",
        "\n",
        "    return E, np.array(probs, dtype=float)\n",
        "\n",
        "def generate_crn_dataset(d_val, pV, pH, ntrials, seed):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    E, probs = build_edge_list_and_probs(d_val, pV, pH)\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(ntrials):\n",
        "        err_mask = rng.rand(len(E)) < probs\n",
        "        error_edges = {E[i] for i in range(len(E)) if err_mask[i]}\n",
        "\n",
        "        syndrome = {}\n",
        "        for edge in error_edges:\n",
        "            tag = edge[0]\n",
        "            if tag == \"V\":\n",
        "                _, r, c = edge\n",
        "                syndrome[(r, c)] = syndrome.get((r, c), 0) ^ 1\n",
        "                syndrome[(r + 1, c)] = syndrome.get((r + 1, c), 0) ^ 1\n",
        "            elif tag == \"H\":\n",
        "                _, r, c = edge\n",
        "                syndrome[(r, c)] = syndrome.get((r, c), 0) ^ 1\n",
        "                syndrome[(r, c + 1)] = syndrome.get((r, c + 1), 0) ^ 1\n",
        "            elif tag == \"L\":\n",
        "                _, r = edge\n",
        "                syndrome[(r, 0)] = syndrome.get((r, 0), 0) ^ 1\n",
        "            elif tag == \"R\":\n",
        "                _, r = edge\n",
        "                syndrome[(r, d_val - 2)] = syndrome.get((r, d_val - 2), 0) ^ 1\n",
        "\n",
        "        defects = [node for node, s in syndrome.items() if s == 1]\n",
        "        dataset.append((error_edges, defects))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def get_weighted_dist(u, v, d_val, w_V, w_H):\n",
        "    if u == \"B\" and v == \"B\":\n",
        "        return 0.0\n",
        "    if u == \"B\":\n",
        "        u, v = v, u\n",
        "    if v == \"B\":\n",
        "        # distance to nearest boundary (counts boundary edge too)\n",
        "        return min(u[1] + 1, d_val - 1 - u[1]) * w_H\n",
        "    return abs(u[0] - v[0]) * w_V + abs(u[1] - v[1]) * w_H\n",
        "\n",
        "def get_correction_edges(u, v, d_val):\n",
        "    edges = []\n",
        "    if u == \"B\" and v == \"B\":\n",
        "        return edges\n",
        "    if u == \"B\":\n",
        "        u, v = v, u\n",
        "\n",
        "    r, c = u\n",
        "    if v == \"B\":\n",
        "        # route to nearest boundary\n",
        "        if (c + 1) <= (d_val - 1 - c):\n",
        "            for col in range(c - 1, -1, -1):\n",
        "                edges.append((\"H\", r, col))\n",
        "            edges.append((\"L\", r))\n",
        "        else:\n",
        "            for col in range(c, d_val - 2):\n",
        "                edges.append((\"H\", r, col))\n",
        "            edges.append((\"R\", r))\n",
        "        return edges\n",
        "\n",
        "    r2, c2 = v\n",
        "    # vertical steps\n",
        "    step_r = 1 if r2 > r else -1\n",
        "    for curr_r in range(r, r2, step_r):\n",
        "        edges.append((\"V\", curr_r if step_r == 1 else curr_r - 1, c))\n",
        "    # horizontal steps\n",
        "    step_c = 1 if c2 > c else -1\n",
        "    for curr_c in range(c, c2, step_c):\n",
        "        edges.append((\"H\", r2, curr_c if step_c == 1 else curr_c - 1))\n",
        "    return edges\n",
        "\n",
        "def evaluate_policy_trace(theta, dataset, d_val):\n",
        "    w_V = max(0.01, float(theta[\"w_V\"]))\n",
        "    w_H = max(0.01, float(theta[\"w_H\"]))\n",
        "    left_cut = {(\"L\", r) for r in range(d_val)}\n",
        "\n",
        "    fails = []\n",
        "    for error_edges, defects in dataset:\n",
        "        correction_edges = set()\n",
        "\n",
        "        if defects:\n",
        "            K = nx.Graph()\n",
        "            for i, u in enumerate(defects):\n",
        "                shadow = f\"s_{i}\"\n",
        "                K.add_node(i); K.add_node(shadow)\n",
        "                K.add_edge(i, shadow, weight=get_weighted_dist(u, \"B\", d_val, w_V, w_H))\n",
        "                for j in range(i + 1, len(defects)):\n",
        "                    v = defects[j]\n",
        "                    K.add_edge(i, j, weight=get_weighted_dist(u, v, d_val, w_V, w_H))\n",
        "                    K.add_edge(shadow, f\"s_{j}\", weight=0)\n",
        "\n",
        "            matching = nx.algorithms.matching.min_weight_matching(K, weight=\"weight\")\n",
        "\n",
        "            for a, b in matching:\n",
        "                if str(a).startswith(\"s_\") and str(b).startswith(\"s_\"):\n",
        "                    continue\n",
        "\n",
        "                node_a = \"B\" if str(a).startswith(\"s_\") else defects[a]\n",
        "                node_b = \"B\" if str(b).startswith(\"s_\") else defects[b]\n",
        "\n",
        "                for e in get_correction_edges(node_a, node_b, d_val):\n",
        "                    if e in correction_edges:\n",
        "                        correction_edges.remove(e)\n",
        "                    else:\n",
        "                        correction_edges.add(e)\n",
        "\n",
        "        residual = error_edges.symmetric_difference(correction_edges)\n",
        "        fail = (sum(1 for e in residual if e in left_cut) % 2) == 1\n",
        "        fails.append(1 if fail else 0)\n",
        "\n",
        "    k = sum(fails)\n",
        "    n = len(fails)\n",
        "    rate = k / n if n else 0.0\n",
        "    return {\n",
        "        \"k\": k, \"n\": n, \"rate\": rate,\n",
        "        \"ci95\": wilson_ci(k, n),\n",
        "        \"fails\": fails,\n",
        "        \"w_V\": w_V, \"w_H\": w_H, \"ratio\": (w_V / w_H),\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Build train + test datasets\n",
        "# -----------------------------\n",
        "train = generate_crn_dataset(d, p_V, p_H, trials, seed_train)\n",
        "test  = generate_crn_dataset(d, p_V, p_H, trials, seed_train + 999)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluate\n",
        "# -----------------------------\n",
        "train_base = evaluate_policy_trace(baseline_theta, train, d)\n",
        "train_opt  = evaluate_policy_trace(opt_theta, train, d)\n",
        "train_th   = evaluate_policy_trace(theory_theta, train, d)\n",
        "\n",
        "test_base  = evaluate_policy_trace(baseline_theta, test, d)\n",
        "test_opt   = evaluate_policy_trace(opt_theta, test, d)\n",
        "test_th    = evaluate_policy_trace(theory_theta, test, d)\n",
        "\n",
        "# Paired McNemar (baseline vs opt on TRAIN)\n",
        "n01 = sum(1 for b, o in zip(train_base[\"fails\"], train_opt[\"fails\"]) if b == 1 and o == 0)\n",
        "n10 = sum(1 for b, o in zip(train_base[\"fails\"], train_opt[\"fails\"]) if b == 0 and o == 1)\n",
        "p_mcnemar = mcnemar_exact_p(n01, n10)\n",
        "\n",
        "def fmt(name, r):\n",
        "    lo, hi = r[\"ci95\"]\n",
        "    return f\"{name:>8}: {r['rate']:.4f}  [{lo:.4f}, {hi:.4f}]  ({r['k']}/{r['n']})  ratio={r['ratio']:.3f}\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CELL 24 — VALIDATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Params: d={d}, p_V={p_V}, p_H={p_H}, trials={trials}\")\n",
        "if latest_policy:\n",
        "    print(\"Loaded opt_theta from:\", latest_policy)\n",
        "print(\"-\"*80)\n",
        "print(\"TRAIN (seed =\", seed_train, \")\")\n",
        "print(fmt(\"baseline\", train_base))\n",
        "print(fmt(\"opt\",      train_opt))\n",
        "print(fmt(\"theory\",   train_th))\n",
        "print(\"-\"*80)\n",
        "print(\"TEST  (seed =\", seed_train + 999, \")\")\n",
        "print(fmt(\"baseline\", test_base))\n",
        "print(fmt(\"opt\",      test_opt))\n",
        "print(fmt(\"theory\",   test_th))\n",
        "print(\"-\"*80)\n",
        "print(\"Paired McNemar (baseline vs opt, TRAIN):\")\n",
        "print(f\"  n01 (baseline fail, opt success) = {n01}\")\n",
        "print(f\"  n10 (baseline success, opt fail) = {n10}\")\n",
        "print(f\"  exact two-sided p-value          = {p_mcnemar:.6f}\")\n",
        "print(\"-\"*80)\n",
        "print(\"opt_theta:\", opt_theta)\n",
        "print(\"theory_theta:\", theory_theta)\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjyYwlJCwgQH",
        "outputId": "863074a4-9a6e-4a72-bdc6-1a7835b99053"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CELL 24 — VALIDATION RESULTS\n",
            "================================================================================\n",
            "Params: d=7, p_V=0.12, p_H=0.04, trials=1000\n",
            "Loaded opt_theta from: /content/shadowmap_cps_bootstrap/engine_runs/cps_policy_optimization_20260215T051016/policy_opt.json\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN (seed = 42 )\n",
            "baseline: 0.0270  [0.0186, 0.0390]  (27/1000)  ratio=1.000\n",
            "     opt: 0.0180  [0.0114, 0.0283]  (18/1000)  ratio=0.641\n",
            "  theory: 0.0180  [0.0114, 0.0283]  (18/1000)  ratio=0.627\n",
            "--------------------------------------------------------------------------------\n",
            "TEST  (seed = 1041 )\n",
            "baseline: 0.0350  [0.0253, 0.0483]  (35/1000)  ratio=1.000\n",
            "     opt: 0.0180  [0.0114, 0.0283]  (18/1000)  ratio=0.641\n",
            "  theory: 0.0180  [0.0114, 0.0283]  (18/1000)  ratio=0.627\n",
            "--------------------------------------------------------------------------------\n",
            "Paired McNemar (baseline vs opt, TRAIN):\n",
            "  n01 (baseline fail, opt success) = 15\n",
            "  n10 (baseline success, opt fail) = 6\n",
            "  exact two-sided p-value          = 0.078354\n",
            "--------------------------------------------------------------------------------\n",
            "opt_theta: {'w_V': 0.686639293515646, 'w_H': 1.0715459373955074}\n",
            "theory_theta: {'w_V': 1.9924301646902063, 'w_H': 3.1780538303479458}\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4R0CNMK7wgWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JC3O4ukYwgav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ul1bk_odwgfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIZLnMQAwglY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtaKINLLwgpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WzYjpuxSwgts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbYFJXHdwgy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hvf72aVTwg29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8i2xXYipwg7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqBvZ7tRwhAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYico18DwhF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IRcT_zkwhKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Zs6YCAswhOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-WuXOHjwhSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1r2G7z9twhXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mfgJoCk0whbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5laSNPUswhf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdvUKEIPwhkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zs07TQWvz6YQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04f61f0e-88f7-4f5d-ffcc-283c5bc16110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "V2 PHYSICS ENGINE: SPARSE MATRIX PYMATCHING COMPILATION\n",
            "======================================================================\n",
            "Detected 8 CPU Cores. Initializing Multiprocessing Pool...\n",
            "Completed d= 7 sweep.\n",
            "Completed d= 9 sweep.\n",
            "Completed d=11 sweep.\n",
            "Completed d=15 sweep.\n",
            "\n",
            "======================================================================\n",
            "d \\ p |      0.080 |      0.090 |      0.100 |      0.103 |      0.110 |      0.120\n",
            "-----------------------------------------------------------------------------------\n",
            "    7 | 0.0870[0.083,0.091] | 0.1168[0.112,0.121] | 0.1557[0.151,0.161] | 0.1644[0.159,0.170] | 0.1942[0.189,0.200] | 0.2382[0.232,0.244]\n",
            "    9 | 0.0733[0.070,0.077] | 0.1054[0.101,0.110] | 0.1519[0.147,0.157] | 0.1659[0.161,0.171] | 0.1985[0.193,0.204] | 0.2489[0.243,0.255]\n",
            "   11 | 0.0649[0.062,0.068] | 0.1023[0.098,0.107] | 0.1493[0.144,0.154] | 0.1609[0.156,0.166] | 0.2011[0.196,0.207] | 0.2510[0.245,0.257]\n",
            "   15 | 0.0464[0.044,0.049] | 0.0859[0.082,0.090] | 0.1435[0.139,0.148] | 0.1626[0.158,0.168] | 0.2054[0.200,0.211] | 0.2782[0.272,0.284]\n",
            "======================================================================\n",
            "Total Decodes Performed: 480,000\n",
            "Total Elapsed Time:      2.24 seconds\n",
            "Engine Speed:            214,712 decodes / sec\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type int64 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-399424787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%dT%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"v2_pymatching_sweep_{run_id}.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# V2 CELL 1 (PATCHED) — PYMATCHING V2: SPARSE MATRIX COMPILATION\n",
        "# Run this as the FIRST cell in your clean \"V2\" Notebook\n",
        "# ==============================================================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# 1. Install required packages silently if missing\n",
        "try:\n",
        "    import pymatching\n",
        "    import scipy\n",
        "except ImportError:\n",
        "    print(\"Installing pymatching and scipy...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pymatching\", \"scipy\"], check=True)\n",
        "    import pymatching\n",
        "    import scipy\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import concurrent.futures\n",
        "import multiprocessing\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"V2 PHYSICS ENGINE: SPARSE MATRIX PYMATCHING COMPILATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def build_matrices(d, p):\n",
        "    \"\"\"\n",
        "    Builds the Parity Check Matrix (H) and Logical Observable Matrix (L)\n",
        "    using exact Mod-2 Sparse Matrices. This perfectly mirrors the friend's V1 topology.\n",
        "    \"\"\"\n",
        "    H_rows, H_cols = [], []\n",
        "    L_cols = []\n",
        "    weights, probs = [], []\n",
        "    edge_idx = 0\n",
        "\n",
        "    # Logical cut: vertical cut down the middle\n",
        "    j_cut = d // 2\n",
        "    if j_cut + 1 >= d: j_cut = max(0, d // 2 - 1)\n",
        "\n",
        "    # MWPM log-odds weights: log((1-p)/p)\n",
        "    w = math.log((1-p)/p) if 0.0 < p < 0.5 else 1e-5\n",
        "\n",
        "    for r in range(d):\n",
        "        for c in range(d):\n",
        "            u = r * d + c\n",
        "\n",
        "            # 1. Horizontal edges (Data qubits)\n",
        "            if c + 1 < d:\n",
        "                v = r * d + (c + 1)\n",
        "                H_rows.extend([u, v])\n",
        "                H_cols.extend([edge_idx, edge_idx])\n",
        "                weights.append(w); probs.append(p)\n",
        "\n",
        "                # If this edge crosses the logical cut, mark it in L\n",
        "                if c == j_cut:\n",
        "                    L_cols.append(edge_idx)\n",
        "                edge_idx += 1\n",
        "\n",
        "            # 2. Vertical edges (Data qubits)\n",
        "            if r + 1 < d:\n",
        "                v = (r + 1) * d + c\n",
        "                H_rows.extend([u, v])\n",
        "                H_cols.extend([edge_idx, edge_idx])\n",
        "                weights.append(w); probs.append(p)\n",
        "                edge_idx += 1\n",
        "\n",
        "            # 3. Left boundary edges (Perfect Virtual Shadows)\n",
        "            # (Only one 1 in the column -> PyMatching natively connects it to the boundary!)\n",
        "            if c == 0:\n",
        "                H_rows.append(u)\n",
        "                H_cols.append(edge_idx)\n",
        "                weights.append(0.0); probs.append(0.0) # Cost to exit boundary is 0\n",
        "                edge_idx += 1\n",
        "\n",
        "            # 4. Right boundary edges\n",
        "            if c == d - 1:\n",
        "                H_rows.append(u)\n",
        "                H_cols.append(edge_idx)\n",
        "                weights.append(0.0); probs.append(0.0) # Cost to exit boundary is 0\n",
        "                edge_idx += 1\n",
        "\n",
        "    # Compile SciPy Sparse Matrices\n",
        "    H = sp.csc_matrix(([1]*len(H_rows), (H_rows, H_cols)), shape=(d*d, edge_idx), dtype=np.uint8)\n",
        "    L = sp.csc_matrix(([1]*len(L_cols), ([0]*len(L_cols), L_cols)), shape=(1, edge_idx), dtype=np.uint8)\n",
        "\n",
        "    return H, L, np.array(weights), np.array(probs)\n",
        "\n",
        "def wilson_ci(k, n, z=1.96):\n",
        "    if n == 0: return (0.0, 0.0)\n",
        "    phat = k / n\n",
        "    denom = 1.0 + (z*z)/n\n",
        "    center = (phat + (z*z)/(2*n)) / denom\n",
        "    half = (z * math.sqrt((phat*(1-phat) + (z*z)/(4*n)) / n)) / denom\n",
        "    return (max(0.0, center - half), min(1.0, center + half))\n",
        "\n",
        "def run_pymatching_chunk(args):\n",
        "    d, p, trials, seed = args\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Build Sparse Matrices\n",
        "    H, L, weights, probs = build_matrices(d, p)\n",
        "    num_edges = len(probs)\n",
        "\n",
        "    # Initialize the C++ Matcher directly from the Sparse Check Matrix\n",
        "    matcher = pymatching.Matching.from_check_matrix(H, weights=weights, faults_matrix=L)\n",
        "\n",
        "    # 1. Vectorized Noise Generation (Generates thousands of trials instantly)\n",
        "    noise_matrix = (rng.random((trials, num_edges)) < probs).astype(np.uint8)\n",
        "\n",
        "    # 2. Fast Mod-2 Syndrome & Observable Generation (Sparse Matrix Math)\n",
        "    H_csr = H.tocsr()\n",
        "    L_csr = L.tocsr()\n",
        "\n",
        "    syndromes = (H_csr @ noise_matrix.T).T % 2\n",
        "    actual_observables = (L_csr @ noise_matrix.T).T % 2\n",
        "\n",
        "    syndromes = np.asarray(syndromes, dtype=np.uint8)\n",
        "    actual_observables = np.asarray(actual_observables, dtype=np.uint8)\n",
        "\n",
        "    # 3. C++ Batch Decode (Takes the syndromes and natively spits out the logical flips)\n",
        "    predicted_observables = matcher.decode_batch(syndromes)\n",
        "\n",
        "    # 4. Compare Predictions to Ground Truth (Logical Failures)\n",
        "    failures = np.sum(predicted_observables[:, 0] != actual_observables[:, 0])\n",
        "\n",
        "    return failures\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    NUM_CORES = min(44, multiprocessing.cpu_count())\n",
        "    print(f\"Detected {NUM_CORES} CPU Cores. Initializing Multiprocessing Pool...\")\n",
        "\n",
        "    # Pushing distance to 15 because we finally have the horsepower.\n",
        "    distances = [7, 9, 11, 15]\n",
        "    p_grid = [0.08, 0.09, 0.10, 0.103, 0.11, 0.12]\n",
        "\n",
        "    # Running 20,000 trials per point.\n",
        "    total_trials = 20000\n",
        "\n",
        "    results = {}\n",
        "    t0 = time.time()\n",
        "\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_CORES) as executor:\n",
        "        for d in distances:\n",
        "            results[d] = {}\n",
        "            for p in p_grid:\n",
        "                chunk_size = max(1, total_trials // NUM_CORES)\n",
        "                actual_trials = chunk_size * NUM_CORES\n",
        "                tasks = [(d, p, chunk_size, hash((d, p, i, 42)) % (2**32)) for i in range(NUM_CORES)]\n",
        "\n",
        "                # Execute mapped jobs across all cores\n",
        "                failures = sum(executor.map(run_pymatching_chunk, tasks))\n",
        "\n",
        "                rate = failures / actual_trials\n",
        "                lo, hi = wilson_ci(failures, actual_trials)\n",
        "                results[d][p] = {\"rate\": rate, \"ci95\": (lo, hi), \"k\": failures, \"n\": actual_trials}\n",
        "\n",
        "            print(f\"Completed d={d:>2} sweep.\")\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    header = \"d \\\\ p | \" + \" | \".join([f\"{p:>10.3f}\" for p in p_grid])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for d in distances:\n",
        "        row = []\n",
        "        for p in p_grid:\n",
        "            r = results[d][p][\"rate\"]\n",
        "            lo, hi = results[d][p][\"ci95\"]\n",
        "            row.append(f\"{r:.4f}[{lo:.3f},{hi:.3f}]\")\n",
        "        print(f\"{d:>5} | \" + \" | \".join(row))\n",
        "\n",
        "    total_decodes = len(distances) * len(p_grid) * actual_trials\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total Decodes Performed: {total_decodes:,}\")\n",
        "    print(f\"Total Elapsed Time:      {elapsed:.2f} seconds\")\n",
        "    print(f\"Engine Speed:            {total_decodes/elapsed:,.0f} decodes / sec\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    BASE_DIR = \"/content/v2_shadowmap\"\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "    run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "    with open(os.path.join(BASE_DIR, f\"v2_pymatching_sweep_{run_id}.json\"), \"w\") as f:\n",
        "        json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V2 CELL 1B — JSON FIX\n",
        "# Run BELOW V2 CELL 1\n",
        "# ==============================================================================\n",
        "import json, os\n",
        "from datetime import datetime\n",
        "\n",
        "# Convert NumPy int64s/float64s to native Python types for JSON serialization\n",
        "clean_results = {}\n",
        "for d in results:\n",
        "    clean_results[int(d)] = {}\n",
        "    for p in results[d]:\n",
        "        clean_results[int(d)][float(p)] = {\n",
        "            \"rate\": float(results[d][p][\"rate\"]),\n",
        "            \"ci95\": (float(results[d][p][\"ci95\"][0]), float(results[d][p][\"ci95\"][1])),\n",
        "            \"k\": int(results[d][p][\"k\"]),\n",
        "            \"n\": int(results[d][p][\"n\"])\n",
        "        }\n",
        "\n",
        "BASE_DIR = \"/content/v2_shadowmap\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "with open(os.path.join(BASE_DIR, f\"v2_pymatching_sweep_{run_id}.json\"), \"w\") as f:\n",
        "    json.dump(clean_results, f, indent=2)\n",
        "print(\"JSON successfully saved! The V2 Engine is secure.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upFP4N-p0GeN",
        "outputId": "e5d6c2a9-1790-41a9-ffbe-4ae51005b691"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON successfully saved! The V2 Engine is secure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V2 CELL 2 — STATISTICAL ANNIHILATION: 100,000-TRIAL CPS OPTIMIZER\n",
        "# Run BELOW V2 Cell 1B\n",
        "# ==============================================================================\n",
        "\n",
        "import pymatching\n",
        "import scipy.sparse as sp\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "import random, os, json, time, math\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"V2 CELL 2: 100,000-TRIAL HARDWARE/SOFTWARE CO-DESIGN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. SPARSE TOPOLOGY GENERATOR\n",
        "# ------------------------------------------------------------------------------\n",
        "d = 9          # Upping distance to 9 for harder physics\n",
        "p_V = 0.12     # High Vertical Error\n",
        "p_H = 0.04     # Low Horizontal Error\n",
        "trials = 100000\n",
        "\n",
        "print(f\"Building topological matrices for d={d}...\")\n",
        "H_rows, H_cols, L_cols, probs = [], [], [], []\n",
        "edge_types = []\n",
        "edge_idx = 0\n",
        "j_cut = d // 2\n",
        "if j_cut + 1 >= d: j_cut = max(0, d // 2 - 1)\n",
        "\n",
        "for r in range(d):\n",
        "    for c in range(d):\n",
        "        u = r * d + c\n",
        "\n",
        "        # Horizontal Edges (p_H)\n",
        "        if c + 1 < d:\n",
        "            H_rows.extend([u, r * d + (c + 1)]); H_cols.extend([edge_idx, edge_idx])\n",
        "            probs.append(p_H); edge_types.append('H')\n",
        "            if c == j_cut: L_cols.append(edge_idx)\n",
        "            edge_idx += 1\n",
        "\n",
        "        # Vertical Edges (p_V)\n",
        "        if r + 1 < d:\n",
        "            H_rows.extend([u, (r + 1) * d + c]); H_cols.extend([edge_idx, edge_idx])\n",
        "            probs.append(p_V); edge_types.append('V')\n",
        "            edge_idx += 1\n",
        "\n",
        "        # Left Boundary (Perfect Virtual Shadow)\n",
        "        if c == 0:\n",
        "            H_rows.append(u); H_cols.append(edge_idx)\n",
        "            probs.append(0.0); edge_types.append('B')\n",
        "            edge_idx += 1\n",
        "\n",
        "        # Right Boundary (Perfect Virtual Shadow)\n",
        "        if c == d - 1:\n",
        "            H_rows.append(u); H_cols.append(edge_idx)\n",
        "            probs.append(0.0); edge_types.append('B')\n",
        "            edge_idx += 1\n",
        "\n",
        "H = sp.csc_matrix(([1]*len(H_rows), (H_rows, H_cols)), shape=(d*d, edge_idx), dtype=np.uint8)\n",
        "L = sp.csc_matrix(([1]*len(L_cols), ([0]*len(L_cols), L_cols)), shape=(1, edge_idx), dtype=np.uint8)\n",
        "probs = np.array(probs)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. GENERATE MASSIVE CRN DATASETS (INSTANTLY IN RAM)\n",
        "# ------------------------------------------------------------------------------\n",
        "print(f\"Generating 100,000 CRN Training Trials and 100,000 Test Trials...\")\n",
        "H_csr, L_csr = H.tocsr(), L.tocsr()\n",
        "num_edges = len(probs)\n",
        "\n",
        "# TRAIN SET\n",
        "rng_train = np.random.default_rng(42)\n",
        "noise_train = (rng_train.random((trials, num_edges)) < probs).astype(np.uint8)\n",
        "syn_train = np.asarray((H_csr @ noise_train.T).T % 2, dtype=np.uint8)\n",
        "obs_train = np.asarray((L_csr @ noise_train.T).T % 2, dtype=np.uint8)[:, 0]\n",
        "\n",
        "# TEST SET (Fresh Seed)\n",
        "rng_test = np.random.default_rng(999)\n",
        "noise_test = (rng_test.random((trials, num_edges)) < probs).astype(np.uint8)\n",
        "syn_test = np.asarray((H_csr @ noise_test.T).T % 2, dtype=np.uint8)\n",
        "obs_test = np.asarray((L_csr @ noise_test.T).T % 2, dtype=np.uint8)[:, 0]\n",
        "\n",
        "print(\"CRN Generation Complete.\\n\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. FAST POLICY EVALUATOR\n",
        "# ------------------------------------------------------------------------------\n",
        "def evaluate_weights(w_V, w_H, dataset=\"train\"):\n",
        "    w_V, w_H = max(0.001, float(w_V)), max(0.001, float(w_H))\n",
        "\n",
        "    weights = np.zeros(len(probs), dtype=float)\n",
        "    for i, t in enumerate(edge_types):\n",
        "        if t == 'V': weights[i] = w_V\n",
        "        elif t == 'H': weights[i] = w_H\n",
        "        else: weights[i] = 0.0 # Boundaries must be 0\n",
        "\n",
        "    # C++ Compiler injects sparse matrices instantly\n",
        "    matcher = pymatching.Matching.from_check_matrix(H, weights=weights, faults_matrix=L)\n",
        "\n",
        "    if dataset == \"train\":\n",
        "        pred = matcher.decode_batch(syn_train)[:, 0]\n",
        "        actual = obs_train\n",
        "    else:\n",
        "        pred = matcher.decode_batch(syn_test)[:, 0]\n",
        "        actual = obs_test\n",
        "\n",
        "    # Fast boolean array of failures\n",
        "    fails_arr = (pred != actual)\n",
        "    return int(np.sum(fails_arr)), fails_arr\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. CPS OPTIMIZATION LOOP\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"Evaluating Naive Baseline (w_V=1.0, w_H=1.0)...\")\n",
        "base_k_train, _ = evaluate_weights(1.0, 1.0, \"train\")\n",
        "print(f\" -> Baseline Train Rate: {base_k_train/trials:.5f}\\n\")\n",
        "\n",
        "pop_size = 12\n",
        "generations = 8\n",
        "retain = 4\n",
        "population = [{'w_V': random.uniform(0.5, 1.5), 'w_H': random.uniform(0.5, 1.5)} for _ in range(pop_size)]\n",
        "\n",
        "print(\"Starting CPS Evolutionary Engine...\")\n",
        "t0 = time.time()\n",
        "best_history = []\n",
        "\n",
        "for gen in range(generations):\n",
        "    scored = []\n",
        "    for theta in population:\n",
        "        fails, _ = evaluate_weights(theta['w_V'], theta['w_H'], \"train\")\n",
        "        scored.append((fails / trials, theta))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0])\n",
        "    survivors = scored[:retain]\n",
        "    best_rate, best_theta = survivors[0]\n",
        "    best_history.append((best_rate, best_theta))\n",
        "\n",
        "    print(f\"GEN {gen+1:02d}/{generations} | Train Fail Rate: {best_rate:.5f} | Weights: w_V={best_theta['w_V']:.3f}, w_H={best_theta['w_H']:.3f}\")\n",
        "\n",
        "    new_pop = [s[1] for s in survivors]\n",
        "    while len(new_pop) < pop_size:\n",
        "        parent = random.choice(survivors)[1]\n",
        "        child = {\n",
        "            'w_V': max(0.01, parent['w_V'] + random.uniform(-0.15, 0.15)),\n",
        "            'w_H': max(0.01, parent['w_H'] + random.uniform(-0.15, 0.15))\n",
        "        }\n",
        "        new_pop.append(child)\n",
        "    population = new_pop\n",
        "\n",
        "opt_theta = best_history[-1][1]\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. HOLDOUT GENERALIZATION & STATISTICAL SIGNIFICANCE (McNemar)\n",
        "# ------------------------------------------------------------------------------\n",
        "base_k_test, base_fails_test = evaluate_weights(1.0, 1.0, \"test\")\n",
        "opt_k_test, opt_fails_test = evaluate_weights(opt_theta['w_V'], opt_theta['w_H'], \"test\")\n",
        "\n",
        "# Theoretical Max-Likelihood Match\n",
        "w_V_th = math.log((1 - p_V) / p_V)\n",
        "w_H_th = math.log((1 - p_H) / p_H)\n",
        "th_k_test, th_fails_test = evaluate_weights(w_V_th, w_H_th, \"test\")\n",
        "\n",
        "# Asymptotic McNemar Test Formulation (Continuity Corrected for large N)\n",
        "n01 = int(np.sum((base_fails_test == True) & (opt_fails_test == False)))\n",
        "n10 = int(np.sum((base_fails_test == False) & (opt_fails_test == True)))\n",
        "\n",
        "if n01 + n10 == 0:\n",
        "    p_val = 1.0\n",
        "else:\n",
        "    chi2_stat = (abs(n01 - n10) - 1)**2 / (n01 + n10)\n",
        "    p_val = scipy.stats.chi2.sf(chi2_stat, 1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"CPS OPTIMIZATION COMPLETE IN {elapsed:.2f}s\")\n",
        "print(\"=\"*70)\n",
        "print(f\"TEST Baseline Failure Rate: {base_k_test/trials:.5f} ({base_k_test}/{trials})\")\n",
        "print(f\"TEST AI Optimized Rate:     {opt_k_test/trials:.5f} ({opt_k_test}/{trials})\")\n",
        "print(f\"TEST Theoretical Limit:     {th_k_test/trials:.5f} ({th_k_test}/{trials})\")\n",
        "print(\"-\" * 70)\n",
        "print(\"STATISTICAL SIGNIFICANCE (McNemar's Test on Test Set):\")\n",
        "print(f\"  Failures avoided by AI (n01): {n01:,}\")\n",
        "print(f\"  New failures caused (n10):    {n10:,}\")\n",
        "print(f\"  p-value:                      {p_val:.2e} (< 0.05 is significant)\")\n",
        "if p_val < 0.0001:\n",
        "    print(\"  -> RESULT IS STATISTICALLY IRONCLAD (p < 0.0001)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Safe JSON Save\n",
        "BASE_DIR = \"/content/v2_shadowmap\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
        "artifact = {\n",
        "    \"baseline_rate_test\": float(base_k_test / trials),\n",
        "    \"opt_rate_test\": float(opt_k_test / trials),\n",
        "    \"theory_rate_test\": float(th_k_test / trials),\n",
        "    \"mcnemar_p_value\": float(p_val),\n",
        "    \"n01\": int(n01),\n",
        "    \"n10\": int(n10),\n",
        "    \"opt_w_V\": float(opt_theta['w_V']),\n",
        "    \"opt_w_H\": float(opt_theta['w_H'])\n",
        "}\n",
        "\n",
        "with open(os.path.join(BASE_DIR, f\"v2_100k_optimizer_{run_id}.json\"), \"w\") as f:\n",
        "    json.dump(artifact, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJR8u6FO8f52",
        "outputId": "91231505-fec6-4c8f-c899-2eb539f751f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "V2 CELL 2: 100,000-TRIAL HARDWARE/SOFTWARE CO-DESIGN\n",
            "======================================================================\n",
            "Building topological matrices for d=9...\n",
            "Generating 100,000 CRN Training Trials and 100,000 Test Trials...\n",
            "CRN Generation Complete.\n",
            "\n",
            "Evaluating Naive Baseline (w_V=1.0, w_H=1.0)...\n",
            " -> Baseline Train Rate: 0.03128\n",
            "\n",
            "Starting CPS Evolutionary Engine...\n",
            "GEN 01/8 | Train Fail Rate: 0.01954 | Weights: w_V=0.855, w_H=1.151\n",
            "GEN 02/8 | Train Fail Rate: 0.01711 | Weights: w_V=0.576, w_H=0.941\n",
            "GEN 03/8 | Train Fail Rate: 0.01711 | Weights: w_V=0.576, w_H=0.941\n",
            "GEN 04/8 | Train Fail Rate: 0.01711 | Weights: w_V=0.576, w_H=0.941\n",
            "GEN 05/8 | Train Fail Rate: 0.01711 | Weights: w_V=0.576, w_H=0.941\n",
            "GEN 06/8 | Train Fail Rate: 0.01711 | Weights: w_V=0.576, w_H=0.941\n",
            "GEN 07/8 | Train Fail Rate: 0.01711 | Weights: w_V=0.576, w_H=0.941\n",
            "GEN 08/8 | Train Fail Rate: 0.01711 | Weights: w_V=0.576, w_H=0.941\n",
            "\n",
            "======================================================================\n",
            "CPS OPTIMIZATION COMPLETE IN 53.33s\n",
            "======================================================================\n",
            "TEST Baseline Failure Rate: 0.03172 (3172/100000)\n",
            "TEST AI Optimized Rate:     0.01775 (1775/100000)\n",
            "TEST Theoretical Limit:     0.01774 (1774/100000)\n",
            "----------------------------------------------------------------------\n",
            "STATISTICAL SIGNIFICANCE (McNemar's Test on Test Set):\n",
            "  Failures avoided by AI (n01): 2,042\n",
            "  New failures caused (n10):    645\n",
            "  p-value:                      9.54e-160 (< 0.05 is significant)\n",
            "  -> RESULT IS STATISTICALLY IRONCLAD (p < 0.0001)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V2 CELL 3 — THE SPATIAL MAP CHALLENGE: DISCOVERING A HARDWARE DEFECT\n",
        "# Run BELOW V2 Cell 2\n",
        "# ==============================================================================\n",
        "\n",
        "import pymatching\n",
        "import scipy.sparse as sp\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "import random, os, time, math\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"V2 CELL 3: SPATIALLY VARYING DEFECTS (THE 'DEAD ZONE')\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "d = 9\n",
        "trials = 80000\n",
        "\n",
        "# 1. The Environment: A chip with a \"Dead Zone\" at Column 4\n",
        "bad_col = 4\n",
        "p_good = 0.04\n",
        "p_bad  = 0.16\n",
        "\n",
        "print(f\"Fabricating d={d} chip with a massive defect at Column {bad_col}...\")\n",
        "print(f\" -> Healthy regions: {p_good*100}% error rate\")\n",
        "print(f\" -> Dead Zone (Col {bad_col}): {p_bad*100}% error rate\")\n",
        "\n",
        "H_rows, H_cols, L_cols, probs = [], [], [], []\n",
        "edge_types, edge_cols = [], []\n",
        "edge_idx = 0\n",
        "j_cut = d // 2\n",
        "\n",
        "for r in range(d):\n",
        "    for c in range(d):\n",
        "        u = r * d + c\n",
        "\n",
        "        # Horizontal Edges\n",
        "        if c + 1 < d:\n",
        "            H_rows.extend([u, r * d + (c + 1)]); H_cols.extend([edge_idx, edge_idx])\n",
        "            probs.append(p_good); edge_types.append('H'); edge_cols.append(c)\n",
        "            if c == j_cut: L_cols.append(edge_idx)\n",
        "            edge_idx += 1\n",
        "\n",
        "        # Vertical Edges (THIS IS WHERE THE DEFECT LIVES)\n",
        "        if r + 1 < d:\n",
        "            H_rows.extend([u, (r + 1) * d + c]); H_cols.extend([edge_idx, edge_idx])\n",
        "            p_edge = p_bad if c == bad_col else p_good\n",
        "            probs.append(p_edge); edge_types.append('V'); edge_cols.append(c)\n",
        "            edge_idx += 1\n",
        "\n",
        "        # Left/Right Boundaries\n",
        "        if c == 0 or c == d - 1:\n",
        "            H_rows.append(u); H_cols.append(edge_idx)\n",
        "            probs.append(0.0); edge_types.append('B'); edge_cols.append(c)\n",
        "            edge_idx += 1\n",
        "\n",
        "H = sp.csc_matrix(([1]*len(H_rows), (H_rows, H_cols)), shape=(d*d, edge_idx), dtype=np.uint8)\n",
        "L = sp.csc_matrix(([1]*len(L_cols), ([0]*len(L_cols), L_cols)), shape=(1, edge_idx), dtype=np.uint8)\n",
        "probs = np.array(probs)\n",
        "\n",
        "# 2. GENERATE CRN DATASETS\n",
        "print(f\"Generating {trials:,} CRN Trials for Train and Test sets...\")\n",
        "H_csr, L_csr = H.tocsr(), L.tocsr()\n",
        "num_edges = len(probs)\n",
        "\n",
        "def make_crn(seed):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    noise = (rng.random((trials, num_edges)) < probs).astype(np.uint8)\n",
        "    syn = np.asarray((H_csr @ noise.T).T % 2, dtype=np.uint8)\n",
        "    obs = np.asarray((L_csr @ noise.T).T % 2, dtype=np.uint8)[:, 0]\n",
        "    return syn, obs\n",
        "\n",
        "syn_train, obs_train = make_crn(42)\n",
        "syn_test,  obs_test  = make_crn(999)\n",
        "\n",
        "# 3. FAST SPATIAL POLICY EVALUATOR\n",
        "is_H = np.array([t == 'H' for t in edge_types])\n",
        "is_V = np.array([t == 'V' for t in edge_types])\n",
        "is_B = np.array([t == 'B' for t in edge_types])\n",
        "col_idx_arr = np.array(edge_cols)\n",
        "\n",
        "def evaluate_spatial_map(w_H, w_V_arr, dataset=\"train\"):\n",
        "    weights = np.zeros(num_edges, dtype=np.float32)\n",
        "    weights[is_H] = max(0.01, w_H)\n",
        "    weights[is_V] = np.maximum(0.01, w_V_arr[col_idx_arr[is_V]])\n",
        "    weights[is_B] = 0.0\n",
        "\n",
        "    matcher = pymatching.Matching.from_check_matrix(H, weights=weights, faults_matrix=L)\n",
        "\n",
        "    if dataset == \"train\":\n",
        "        pred = matcher.decode_batch(syn_train)[:, 0]\n",
        "        fails = (pred != obs_train)\n",
        "    else:\n",
        "        pred = matcher.decode_batch(syn_test)[:, 0]\n",
        "        fails = (pred != obs_test)\n",
        "\n",
        "    return int(np.sum(fails)), fails\n",
        "\n",
        "# 4. MULTI-DIMENSIONAL CPS OPTIMIZATION LOOP\n",
        "print(\"\\nEvaluating Naive Baseline (Uniform weights = 1.0)...\")\n",
        "base_k_train, _ = evaluate_spatial_map(1.0, np.ones(d), \"train\")\n",
        "print(f\" -> Baseline Train Rate: {base_k_train/trials:.5f}\\n\")\n",
        "\n",
        "# AI learns 10 independent parameters!\n",
        "pop_size = 20\n",
        "generations = 15\n",
        "retain = 6\n",
        "\n",
        "population = [\n",
        "    {\n",
        "        'w_H': random.uniform(0.8, 1.2),\n",
        "        'w_V': np.random.uniform(0.8, 1.2, d)\n",
        "    } for _ in range(pop_size)\n",
        "]\n",
        "\n",
        "print(\"Starting Spatial CPS Evolutionary Engine (Optimizing 10 parameters)...\")\n",
        "t0 = time.time()\n",
        "best_history = []\n",
        "\n",
        "for gen in range(generations):\n",
        "    scored = []\n",
        "    for theta in population:\n",
        "        fails, _ = evaluate_spatial_map(theta['w_H'], theta['w_V'], \"train\")\n",
        "        scored.append((fails / trials, theta))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0])\n",
        "    survivors = scored[:retain]\n",
        "    best_rate, best_theta = survivors[0]\n",
        "    best_history.append((best_rate, best_theta))\n",
        "\n",
        "    w_V_norm = best_theta['w_V'][0]\n",
        "    w_V_def  = best_theta['w_V'][bad_col]\n",
        "    print(f\"GEN {gen+1:02d}/{generations} | Fail: {best_rate:.5f} | w_V_normal: {w_V_norm:.2f} | w_V_DEFECT: {w_V_def:.2f}\")\n",
        "\n",
        "    new_pop = [s[1] for s in survivors]\n",
        "    while len(new_pop) < pop_size:\n",
        "        parent = random.choice(survivors)[1]\n",
        "        # Mutate the spatial array with Gaussian noise (explore heavily)\n",
        "        child = {\n",
        "            'w_H': max(0.01, parent['w_H'] + random.uniform(-0.15, 0.15)),\n",
        "            'w_V': np.maximum(0.01, parent['w_V'] + np.random.normal(0, 0.15, d))\n",
        "        }\n",
        "        new_pop.append(child)\n",
        "    population = new_pop\n",
        "\n",
        "opt_theta = best_history[-1][1]\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "# 5. HOLDOUT GENERALIZATION & THEORETICAL COMPARISON\n",
        "base_k_test, base_fails = evaluate_spatial_map(1.0, np.ones(d), \"test\")\n",
        "opt_k_test, opt_fails   = evaluate_spatial_map(opt_theta['w_H'], opt_theta['w_V'], \"test\")\n",
        "\n",
        "# Theoretical Max-Likelihood Map\n",
        "th_w_H = math.log((1 - p_good) / p_good)\n",
        "th_w_V = np.zeros(d)\n",
        "for c in range(d):\n",
        "    p_c = p_bad if c == bad_col else p_good\n",
        "    th_w_V[c] = math.log((1 - p_c) / p_c)\n",
        "\n",
        "th_k_test, th_fails = evaluate_spatial_map(th_w_H, th_w_V, \"test\")\n",
        "\n",
        "n01 = int(np.sum((base_fails == True) & (opt_fails == False)))\n",
        "n10 = int(np.sum((base_fails == False) & (opt_fails == True)))\n",
        "chi2_stat = (abs(n01 - n10) - 1)**2 / (n01 + n10) if (n01 + n10) > 0 else 0\n",
        "p_val = scipy.stats.chi2.sf(chi2_stat, 1) if (n01 + n10) > 0 else 1.0\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"SPATIAL OPTIMIZATION COMPLETE IN {elapsed:.2f}s\")\n",
        "print(\"=\"*70)\n",
        "print(f\"TEST Baseline Failure Rate: {base_k_test/trials:.5f} ({base_k_test}/{trials})\")\n",
        "print(f\"TEST SHADOWMAP() Optimized Rate:     {opt_k_test/trials:.5f} ({opt_k_test}/{trials})\")\n",
        "print(f\"TEST Theoretical Limit:     {th_k_test/trials:.5f} ({th_k_test}/{trials})\")\n",
        "print(\"-\" * 70)\n",
        "print(\"STATISTICAL SIGNIFICANCE (McNemar Test):\")\n",
        "print(f\"  Failures avoided by AI: {n01:,}\")\n",
        "print(f\"  New failures caused:    {n10:,}\")\n",
        "print(f\"  p-value:                {p_val:.2e}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nSHADOWMAP() DISCOVERED HARDWARE MAP (Vertical Weights by Column):\")\n",
        "print(\"Col | True Error Rate | SHADOWMAP() Weight (Scaled) | Theoretical Opt\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "scale_factor = th_w_H / opt_theta['w_H']\n",
        "scaled_ai_w_V = opt_theta['w_V'] * scale_factor\n",
        "\n",
        "for c in range(d):\n",
        "    true_p = p_bad if c == bad_col else p_good\n",
        "    tag = \"<-- DEAD ZONE DISCOVERED!\" if c == bad_col else \"\"\n",
        "    print(f\"{c:>3} | {true_p:>15.2f} | {scaled_ai_w_V[c]:^18.3f} | {th_w_V[c]:^15.3f} {tag}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7rjX3et8gB8",
        "outputId": "27cf2939-4b4f-403a-d980-aa220cd25724"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "V2 CELL 3: SPATIALLY VARYING DEFECTS (THE 'DEAD ZONE')\n",
            "======================================================================\n",
            "Fabricating d=9 chip with a massive defect at Column 4...\n",
            " -> Healthy regions: 4.0% error rate\n",
            " -> Dead Zone (Col 4): 16.0% error rate\n",
            "Generating 80,000 CRN Trials for Train and Test sets...\n",
            "\n",
            "Evaluating Naive Baseline (Uniform weights = 1.0)...\n",
            " -> Baseline Train Rate: 0.00768\n",
            "\n",
            "Starting Spatial CPS Evolutionary Engine (Optimizing 10 parameters)...\n",
            "GEN 01/15 | Fail: 0.00609 | w_V_normal: 1.10 | w_V_DEFECT: 0.81\n",
            "GEN 02/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 03/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 04/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 05/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 06/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 07/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 08/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 09/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 10/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 11/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 12/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 13/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 14/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "GEN 15/15 | Fail: 0.00556 | w_V_normal: 1.03 | w_V_DEFECT: 0.64\n",
            "\n",
            "======================================================================\n",
            "SPATIAL OPTIMIZATION COMPLETE IN 79.96s\n",
            "======================================================================\n",
            "TEST Baseline Failure Rate: 0.00806 (645/80000)\n",
            "TEST SHADOWMAP() Optimized Rate:     0.00600 (480/80000)\n",
            "TEST Theoretical Limit:     0.00635 (508/80000)\n",
            "----------------------------------------------------------------------\n",
            "STATISTICAL SIGNIFICANCE (McNemar Test):\n",
            "  Failures avoided by AI: 297\n",
            "  New failures caused:    132\n",
            "  p-value:                2.41e-15\n",
            "======================================================================\n",
            "\n",
            "SHADOWMAP() DISCOVERED HARDWARE MAP (Vertical Weights by Column):\n",
            "Col | True Error Rate | SHADOWMAP() Weight (Scaled) | Theoretical Opt\n",
            "----------------------------------------------------------------------\n",
            "  0 |            0.04 |       2.559        |      3.178      \n",
            "  1 |            0.04 |       3.039        |      3.178      \n",
            "  2 |            0.04 |       2.502        |      3.178      \n",
            "  3 |            0.04 |       3.400        |      3.178      \n",
            "  4 |            0.16 |       1.579        |      1.658      <-- DEAD ZONE DISCOVERED!\n",
            "  5 |            0.04 |       3.058        |      3.178      \n",
            "  6 |            0.04 |       2.913        |      3.178      \n",
            "  7 |            0.04 |       2.603        |      3.178      \n",
            "  8 |            0.04 |       2.365        |      3.178      \n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V2 CELL 4 — THE MASTER ARTIFACT: SHADOWMAP CALIBRATOR CLASS\n",
        "# Run this BELOW V2 Cell 3\n",
        "# ==============================================================================\n",
        "\n",
        "import pymatching\n",
        "import scipy.sparse as sp\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "import random, time, math\n",
        "\n",
        "class ShadowMapCalibrator:\n",
        "    \"\"\"\n",
        "    SHADOWMAP: Utility-Scale Hardware-Software Co-Design Engine.\n",
        "    Discovers spatial defect maps on quantum chips using evolutionary MWPM weight calibration.\n",
        "    \"\"\"\n",
        "    def __init__(self, d=9, p_healthy=0.04, dead_zones=None, trials=80000, seed=42):\n",
        "        self.d = d\n",
        "        self.p_good = p_healthy\n",
        "        self.dead_zones = dead_zones or {} # e.g., {4: 0.16, 7: 0.10}\n",
        "        self.trials = trials\n",
        "        self.seed = seed\n",
        "\n",
        "        self.H = None\n",
        "        self.L = None\n",
        "        self.probs = None\n",
        "        self.edge_types = []\n",
        "        self.edge_cols = []\n",
        "\n",
        "        self.syn_train, self.obs_train = None, None\n",
        "        self.syn_test,  self.obs_test  = None, None\n",
        "\n",
        "        self.opt_theta = None\n",
        "\n",
        "        print(f\"[*] Fabricating d={self.d} Virtual Chip...\")\n",
        "        self._fabricate_chip()\n",
        "        self._generate_crn_datasets()\n",
        "\n",
        "    def _fabricate_chip(self):\n",
        "        H_rows, H_cols, L_cols, probs = [], [], [], []\n",
        "        edge_idx = 0\n",
        "        j_cut = self.d // 2\n",
        "\n",
        "        for r in range(self.d):\n",
        "            for c in range(self.d):\n",
        "                u = r * self.d + c\n",
        "\n",
        "                # Horizontal\n",
        "                if c + 1 < self.d:\n",
        "                    H_rows.extend([u, r * self.d + (c + 1)]); H_cols.extend([edge_idx, edge_idx])\n",
        "                    probs.append(self.p_good); self.edge_types.append('H'); self.edge_cols.append(c)\n",
        "                    if c == j_cut: L_cols.append(edge_idx)\n",
        "                    edge_idx += 1\n",
        "\n",
        "                # Vertical (WHERE DEFECTS LIVE)\n",
        "                if r + 1 < self.d:\n",
        "                    H_rows.extend([u, (r + 1) * self.d + c]); H_cols.extend([edge_idx, edge_idx])\n",
        "                    p_edge = self.dead_zones.get(c, self.p_good)\n",
        "                    probs.append(p_edge); self.edge_types.append('V'); self.edge_cols.append(c)\n",
        "                    edge_idx += 1\n",
        "\n",
        "                # Boundaries\n",
        "                if c == 0 or c == self.d - 1:\n",
        "                    H_rows.append(u); H_cols.append(edge_idx)\n",
        "                    probs.append(0.0); self.edge_types.append('B'); self.edge_cols.append(c)\n",
        "                    edge_idx += 1\n",
        "\n",
        "        self.H = sp.csc_matrix(([1]*len(H_rows), (H_rows, H_cols)), shape=(self.d*self.d, edge_idx), dtype=np.uint8)\n",
        "        self.L = sp.csc_matrix(([1]*len(L_cols), ([0]*len(L_cols), L_cols)), shape=(1, edge_idx), dtype=np.uint8)\n",
        "        self.probs = np.array(probs)\n",
        "\n",
        "        self.is_H = np.array([t == 'H' for t in self.edge_types])\n",
        "        self.is_V = np.array([t == 'V' for t in self.edge_types])\n",
        "        self.is_B = np.array([t == 'B' for t in self.edge_types])\n",
        "        self.col_idx = np.array(self.edge_cols)\n",
        "\n",
        "    def _generate_crn_datasets(self):\n",
        "        print(f\"[*] Generating {self.trials:,} CRN Trials for Train/Test...\")\n",
        "        H_csr, L_csr = self.H.tocsr(), self.L.tocsr()\n",
        "        num_edges = len(self.probs)\n",
        "\n",
        "        def make_crn(s):\n",
        "            rng = np.random.default_rng(s)\n",
        "            noise = (rng.random((self.trials, num_edges)) < self.probs).astype(np.uint8)\n",
        "            syn = np.asarray((H_csr @ noise.T).T % 2, dtype=np.uint8)\n",
        "            obs = np.asarray((L_csr @ noise.T).T % 2, dtype=np.uint8)[:, 0]\n",
        "            return syn, obs\n",
        "\n",
        "        self.syn_train, self.obs_train = make_crn(self.seed)\n",
        "        self.syn_test,  self.obs_test  = make_crn(self.seed + 999)\n",
        "        print(\"[*] Hardware Environment Frozen.\")\n",
        "\n",
        "    def evaluate_policy(self, w_H, w_V_arr, dataset=\"train\"):\n",
        "        weights = np.zeros(len(self.probs), dtype=np.float32)\n",
        "        weights[self.is_H] = max(0.01, w_H)\n",
        "        weights[self.is_V] = np.maximum(0.01, w_V_arr[self.col_idx[self.is_V]])\n",
        "        weights[self.is_B] = 0.0\n",
        "\n",
        "        matcher = pymatching.Matching.from_check_matrix(self.H, weights=weights, faults_matrix=self.L)\n",
        "\n",
        "        if dataset == \"train\":\n",
        "            fails = (matcher.decode_batch(self.syn_train)[:, 0] != self.obs_train)\n",
        "        else:\n",
        "            fails = (matcher.decode_batch(self.syn_test)[:, 0] != self.obs_test)\n",
        "\n",
        "        return int(np.sum(fails)), fails\n",
        "\n",
        "    def fit(self, pop_size=20, generations=15, retain=6):\n",
        "        print(f\"\\n[*] Starting SHADOWMAP Evolutionary Optimization...\")\n",
        "        t0 = time.time()\n",
        "\n",
        "        population = [{'w_H': random.uniform(0.8, 1.2), 'w_V': np.random.uniform(0.8, 1.2, self.d)} for _ in range(pop_size)]\n",
        "\n",
        "        for gen in range(generations):\n",
        "            scored = []\n",
        "            for theta in population:\n",
        "                fails, _ = self.evaluate_policy(theta['w_H'], theta['w_V'], \"train\")\n",
        "                scored.append((fails / self.trials, theta))\n",
        "\n",
        "            scored.sort(key=lambda x: x[0])\n",
        "            survivors = scored[:retain]\n",
        "            best_rate, self.opt_theta = survivors[0]\n",
        "\n",
        "            print(f\" -> GEN {gen+1:02d}/{generations} | Lowest Train Fail Rate: {best_rate:.5f}\")\n",
        "\n",
        "            new_pop = [s[1] for s in survivors]\n",
        "            while len(new_pop) < pop_size:\n",
        "                parent = random.choice(survivors)[1]\n",
        "                new_pop.append({\n",
        "                    'w_H': max(0.01, parent['w_H'] + random.uniform(-0.15, 0.15)),\n",
        "                    'w_V': np.maximum(0.01, parent['w_V'] + np.random.normal(0, 0.15, self.d))\n",
        "                })\n",
        "            population = new_pop\n",
        "\n",
        "        self.fit_time = time.time() - t0\n",
        "        print(f\"[*] Optimization Complete in {self.fit_time:.2f}s\")\n",
        "\n",
        "    def report(self):\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"SHADOWMAP() FINAL VALIDATION REPORT\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        base_k, base_fails = self.evaluate_policy(1.0, np.ones(self.d), \"test\")\n",
        "        opt_k, opt_fails   = self.evaluate_policy(self.opt_theta['w_H'], self.opt_theta['w_V'], \"test\")\n",
        "\n",
        "        th_w_H = math.log((1 - self.p_good) / self.p_good)\n",
        "        th_w_V = np.array([math.log((1 - self.dead_zones.get(c, self.p_good)) / self.dead_zones.get(c, self.p_good)) for c in range(self.d)])\n",
        "        th_k, th_fails = self.evaluate_policy(th_w_H, th_w_V, \"test\")\n",
        "\n",
        "        n01 = int(np.sum((base_fails == True) & (opt_fails == False)))\n",
        "        n10 = int(np.sum((base_fails == False) & (opt_fails == True)))\n",
        "        chi2_stat = (abs(n01 - n10) - 1)**2 / (n01 + n10) if (n01 + n10) > 0 else 0\n",
        "        p_val = scipy.stats.chi2.sf(chi2_stat, 1) if (n01 + n10) > 0 else 1.0\n",
        "\n",
        "        print(f\"TEST Baseline Failure Rate: {base_k/self.trials:.5f}\")\n",
        "        print(f\"TEST SHADOWMAP Optimized:   {opt_k/self.trials:.5f}  <---\")\n",
        "        print(f\"TEST Theoretical Analytic:  {th_k/self.trials:.5f}\")\n",
        "        print(f\"McNemar p-value:            {p_val:.2e}\")\n",
        "\n",
        "        print(\"\\nDISCOVERED HARDWARE MAP (Vertical Weights):\")\n",
        "        print(\"Col | True Error | SHADOWMAP (Scaled) | Theoretical Opt\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "        scale_factor = th_w_H / self.opt_theta['w_H']\n",
        "        scaled_w_V = self.opt_theta['w_V'] * scale_factor\n",
        "\n",
        "        for c in range(self.d):\n",
        "            p_c = self.dead_zones.get(c, self.p_good)\n",
        "            tag = \"<-- DEFECT FOUND\" if c in self.dead_zones else \"\"\n",
        "            print(f\"{c:>3} | {p_c:>10.2f} | {scaled_w_V[c]:^18.3f} | {th_w_V[c]:^15.3f} {tag}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "# --- EXECUTE THE API ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Let's test it with TWO dead zones! Base error 3%, Col 2 is 12%, Col 8 is 20%.\n",
        "    calibrator = ShadowMapCalibrator(d=11, p_healthy=0.03, dead_zones={2: 0.12, 8: 0.20}, trials=80000)\n",
        "    calibrator.fit()\n",
        "    calibrator.report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1HIlnXz8gJJ",
        "outputId": "570872f9-67d4-49da-eb23-75310986554e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Fabricating d=11 Virtual Chip...\n",
            "[*] Generating 80,000 CRN Trials for Train/Test...\n",
            "[*] Hardware Environment Frozen.\n",
            "\n",
            "[*] Starting SHADOWMAP Evolutionary Optimization...\n",
            " -> GEN 01/15 | Lowest Train Fail Rate: 0.00081\n",
            " -> GEN 02/15 | Lowest Train Fail Rate: 0.00075\n",
            " -> GEN 03/15 | Lowest Train Fail Rate: 0.00072\n",
            " -> GEN 04/15 | Lowest Train Fail Rate: 0.00065\n",
            " -> GEN 05/15 | Lowest Train Fail Rate: 0.00065\n",
            " -> GEN 06/15 | Lowest Train Fail Rate: 0.00065\n",
            " -> GEN 07/15 | Lowest Train Fail Rate: 0.00063\n",
            " -> GEN 08/15 | Lowest Train Fail Rate: 0.00060\n",
            " -> GEN 09/15 | Lowest Train Fail Rate: 0.00060\n",
            " -> GEN 10/15 | Lowest Train Fail Rate: 0.00060\n",
            " -> GEN 11/15 | Lowest Train Fail Rate: 0.00060\n",
            " -> GEN 12/15 | Lowest Train Fail Rate: 0.00059\n",
            " -> GEN 13/15 | Lowest Train Fail Rate: 0.00059\n",
            " -> GEN 14/15 | Lowest Train Fail Rate: 0.00059\n",
            " -> GEN 15/15 | Lowest Train Fail Rate: 0.00059\n",
            "[*] Optimization Complete in 105.62s\n",
            "\n",
            "======================================================================\n",
            "SHADOWMAP() FINAL VALIDATION REPORT\n",
            "======================================================================\n",
            "TEST Baseline Failure Rate: 0.00135\n",
            "TEST SHADOWMAP Optimized:   0.00065  <---\n",
            "TEST Theoretical Analytic:  0.00065\n",
            "McNemar p-value:            1.96e-09\n",
            "\n",
            "DISCOVERED HARDWARE MAP (Vertical Weights):\n",
            "Col | True Error | SHADOWMAP (Scaled) | Theoretical Opt\n",
            "-----------------------------------------------------------------\n",
            "  0 |       0.03 |       2.531        |      3.476      \n",
            "  1 |       0.03 |       4.616        |      3.476      \n",
            "  2 |       0.12 |       1.946        |      1.992      <-- DEFECT FOUND\n",
            "  3 |       0.03 |       3.870        |      3.476      \n",
            "  4 |       0.03 |       3.595        |      3.476      \n",
            "  5 |       0.03 |       3.386        |      3.476      \n",
            "  6 |       0.03 |       2.348        |      3.476      \n",
            "  7 |       0.03 |       3.315        |      3.476      \n",
            "  8 |       0.20 |       1.507        |      1.386      <-- DEFECT FOUND\n",
            "  9 |       0.03 |       3.888        |      3.476      \n",
            " 10 |       0.03 |       4.135        |      3.476      \n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXoE5EVJ8gMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZrHxluQ18gOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdVAcn4F8gQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceHFvLDt8gTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BunJD6N8gV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RuD8ZV1F8gYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-PWj7cn8ga8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aspoDl-s8gdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHXmbPwC8ggs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kD7N0v9M8gji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "08U3APmf8glq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbCa8Rsi8goD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p316n8Xr8gqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3m3LZzzr8gtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hhW875z8gvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBjGuRDi8hBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quVbHH_S8hEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLYWLGdG8hGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hjH-TgKx8hI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fEG-Ayt_8hLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mpt4IHua8hOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_k05IYA8hQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LqyHq2kX8hTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTqz2pze8hVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUsLPIRr8hYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8fXqYoJa8ha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWS9B5di8heB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMYOiLxm8hhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}